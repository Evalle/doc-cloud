<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet
href="urn:x-daps:xslt:profiling:novdoc-profile.xsl"
type="text/xml"
title="Profiling step"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
<!ENTITY % NOVDOC.DEACTIVATE.IDREF "IGNORE">
<!ENTITY % entities SYSTEM "entity-decl.ent">
%entities;
]>
<chapter id="cha.depl.adm_conf">
  <title>Configuring the &admserv;</title>
  <para>
   After the installation of the operating system and the add-on products on
   the &admserv; has finished, you need to set up product and update
   repositories and, optionally, configure the bastion network. After the
   &admserv; host is fully configured, start the cloud installation script.
  </para>

  <sect1 id="sec.depl.inst.admserv.post.smt_repos">
   <title>Setting up the &smt; Repositories</title>
   <para>
    If you are using an &smt; server (locally or from another network), you
    need to configure it to mirror the update repositories needed for
    &cloud;. Skip this step if you are not using an &smt; server (but make
    sure the repositories listed at <xref linkend="sec.depl.req.repos"/> are
    available on the &admserv;).
   </para>
   <para>
    The &smt; server mirrors the update channels from the &ncc;. In order to
    be able to access the &sls; and &productname; repositories, make sure
    to have registered both products in &ncc;. Run the following commands as
    user &rootuser; on the &smt; server:
   </para>
<screen><?dbsuse-fo font-size="0.63em"?>for REPO in SLES11-SP3-{Pool,Updates} SUSE-Cloud-5-{Pool,Updates}; do
  smt-repos $REPO sle-11-x86_64 -e
done
smt-mirror -L /var/log/smt/smt-mirror.log</screen>
   <para>
    For the optional &hasetup; you also need to mirror the &ha; repositories:
   </para>
<screen><?dbsuse-fo font-size="0.63em"?>for REPO in SLE11-HAE-SP3-{Pool,Updates}; do
  smt-repos $REPO sle-11-x86_64 -e
done
smt-mirror -L /var/log/smt/smt-mirror_ha.log</screen>
   <para>
    The <command>smt-repos</command> command will add the list of repositories
    to the &smt; server. The <command>smt-mirror</command> command will mirror
    them and download several GB of patches. This process may last up to
    several hours. A log file is written to
    <filename>/var/log/smt/smt-mirror.log</filename>. The following table
    lists all repositories and their file system location:
   </para>
   <table>
    <title>&smt; Repositories for &cloud;</title>
    <tgroup cols="2">
     <colspec colnum="1" colname="1" colwidth="25*"/>
     <colspec colnum="2" colname="2" colwidth="75*"/>
     <thead>
      <row>
       <entry>
	<para>
	 Repository
	</para>
       </entry>
       <entry>
	<para>
	 Directory
	</para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
	<para>
	 SLES11-SP3-Pool
	</para>
       </entry>
       <entry>
	<para>
	 <filename>/srv/www/htdocs/repo/$RCE/SLES11-SP3-Pool/sle-11-x86_64</filename>
	</para>
       </entry>
      </row>
      <row>
       <entry>
	<para>
	 SLES11-SP3-Updates
	</para>
       </entry>
       <entry>
	<para>
	 <filename>/srv/www/htdocs/repo/$RCE/SLES11-SP3-Updates/sle-11-x86_64</filename>
	</para>
       </entry>
      </row>
      <row>
       <entry>
	<para>
	 SUSE-Cloud-5-Pool
	</para>
       </entry>
       <entry>
	<para>
	 <filename>/srv/www/htdocs/repo/$RCE/SUSE-Cloud-5-Pool/sle-11-x86_64</filename>
	</para>
       </entry>
      </row>
      <row>
       <entry>
	<para>
	 SUSE-Cloud-5-Updates
	</para>
       </entry>
       <entry>
	<para>
	 <filename>/srv/www/htdocs/repo/$RCE/SUSE-Cloud-5-Updates/sle-11-x86_64</filename>
	</para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>

   <table>
    <title>Additional &smt; Repositories for an &hasetup; (optional)</title>
    <tgroup cols="2">
     <colspec colnum="1" colname="1" colwidth="25*"/>
     <colspec colnum="2" colname="2" colwidth="75*"/>
     <thead>
      <row>
       <entry>
	<para>
	 Repository
	</para>
       </entry>
       <entry>
	<para>
	 Directory
	</para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
	<para>
	 SLE11-HAE-SP3-Pool
	</para>
       </entry>
       <entry>
	<para>
	 <filename>/srv/www/htdocs/repo/$RCE/SLE11-HAE-SP3-Pool/sle-11-x86_64</filename>
	</para>
       </entry>
      </row>
      <row>
       <entry>
	<para>
	 SLE11-HAE-SP3-Updates
	</para>
       </entry>
       <entry>
	<para>
	 <filename>/srv/www/htdocs/repo/$RCE/SLE11-HAE-SP3-Updates/sle-11-x86_64</filename>
	</para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </sect1>

  <sect1 id="sec.depl.inst.admserv.post.local_repos">
   <title>Setting Up Repositories for Node Deployment</title>
   <para>
    In order to deploy the &ostack; nodes and to provide update repositories
    for them, product and update repositories for &sls; and &productname;
    must be configured. See <xref linkend="sec.depl.req.repos"/> for
    background information.
   </para>
   <sect2 id="sec.depl.inst.admserv.post.local_repos.product">
    <title>Product Repositories</title>
    <para>
     The files in the product repositories for &sls; and &cloud; do not
     change, therefore they do not need to be synchronized with a remote
     source.  It is sufficient to either copy the data (from a remote host or
     the installation media) or mount the product repository from a remote
     server via <literal>NFS</literal>. Alternatively you may copy the iso
     images of DVD1 from both products to the admin node and <quote>loop
     mount</quote> them.
    </para>

    <important>
     <title>No Symbolic Links for the &sls; Repository</title>
     <para>
      Note that the &sls; product repository <emphasis>must</emphasis> be
      directly available from the local directory listed below. It is not
      possible to use a symbolic link to a directory located elsewhere, since
      this will cause booting using PXE to fail.
     </para>
    </important>

    <para>
     While the &sls; product repository must be made available locally, the
     &productname; repository may also be served via <literal>http</literal>
     from a remote host. However, copying the data (approximately
     &mediaspace;) to the &admserv; as described here, is recommended, since
     it does not require a custom network configuration for the &admserv;.
    </para>
    <para>
     The following product repositories need to be available for
     &productname; deployment. Make sure to create these directories prior to
     copying or mounting the data:
    </para>
    <table>
     <title>Local Product Repositories for &cloud;</title>
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="40*"/>
      <colspec colnum="2" colname="2" colwidth="60*"/>
      <thead>
       <row>
        <entry>
         <para>
          Repository
         </para>
        </entry>
        <entry>
         <para>
          Directory
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>
         <para>
          SLES11 SP3 Product
         </para>
        </entry>
        <entry>
         <para>
          <filename>/srv/tftpboot/suse-11.3/install</filename>
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Cloud 5 Product
         </para>
        </entry>
        <entry>
         <para>
          <filename>/srv/tftpboot/suse-11.3/repos/Cloud</filename>
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
    <sect3 id="sec.depl.inst.admserv.post.local_repos.product.media">
     <title>Copying from the Installation Media</title>
     <para>
      If copying, it is recommended to use <command>rsync</command>. If the
      installation data is located on a removable device, make sure to mount
      it first (for example, after inserting the DVD1 in the &admserv; and
      waiting for the device to become ready):
     </para>
<screen>mkdir -p /srv/tftpboot/suse-11.3/install/
mount /dev/dvd /mnt
rsync -avP /mnt/ /srv/tftpboot/suse-11.3/install/
umount /mnt</screen>
     <para>
      Also make the contents of the &productname; product repository
      available at <filename>/srv/tftpboot/suse-11.3/repos/Cloud/</filename> the same
      way:
     </para>
<screen>mkdir -p /srv/tftpboot/suse-11.3/repos/Cloud
mount /dev/dvd /mnt
rsync -avP /mnt/ /srv/tftpboot/suse-11.3/repos/Cloud/
umount /mnt</screen>
    </sect3>
    <sect3 id="sec.depl.inst.admserv.post.local_repos.product.media.remote">
     <title>Copying from a Remote Host</title>
     <para>
      If the &slsa; installation data is provided by a remote machine, log
      in to that machine and push the data to the &admserv; (which has the
      IP address
      <systemitem class="etheraddress">192.168.124.10</systemitem> in the
      following example):
     </para>
     <screen>mkdir -p /srv/tftpboot/suse-11.3/install/
rsync -avPz /data/sles11sp3/ <replaceable>192.168.124.10</replaceable>:/srv/tftpboot/suse-11.3/install/</screen>
     <para>
      Also make the contents of the &productname; product repository
      available at <filename>/srv/tftpboot/suse-11.3/repos/Cloud/</filename> the same
      way:
     </para>
     <screen>mkdir -p /srv/tftpboot/suse-11.3/repos/Cloud
rsync -avPz /data/Cloud/ <replaceable>192.168.124.10</replaceable>:/srv/tftpboot/suse-11.3/repos/Cloud/</screen>
    </sect3>
    <sect3 id="sec.depl.inst.admserv.post.local_repos.product.media.nfs">
     <title>Mounting from an NFS Share</title>
     <para>
      If the SLES installation data is provided via NFS by a remote machine,
      mount them as follows:
     </para>
     <screen>mkdir -p /srv/tftpboot/suse-11.3/install/
mount -t nfs <replaceable>nfs.&exampledomain;:/exports/SLES-11-SP3/x86_64/DVD1/</replaceable> /srv/tftpboot/suse-11.3/install
mkdir -p /srv/tftpboot/suse-11.3/repos/Cloud
mount -t nfs <replaceable>nfs.&exampledomain;:/exports/SUSE-CLOU/DVD1/</replaceable> /srv/tftpboot/suse-11.3/repos/Cloud</screen>
     <para>
      To automatically mount these directories either create entries in
      <filename>/etc/fstab</filename> or set up the automounter.
     </para>
    </sect3>
    <sect3 id="sec.depl.inst.admserv.post.local_repos.product.media.iso">
     <title>Mounting ISO Images</title>
     <para>
      The product repositories can also be made available by copying the iso
      images of DVD1 to the &admserv; and mounting them:
     </para>
     <screen>mkdir -p /srv/tftpboot/suse-11.3/install/
mount -o loop <replaceable>/local/SLES-11-SP3-DVD-x86_64-DVD1.iso</replaceable> /srv/tftpboot/suse-11.3/install
mkdir -p /srv/tftpboot/suse-11.3/repos/Cloud
mount -o loop <replaceable>/local/SUSE-CLOUD-5-DVD1.iso</replaceable> /srv/tftpboot/suse-11.3/repos/Cloud</screen>
     <para>
      To automatically mount these directories either create entries in
      <filename>/etc/fstab</filename> or set up the automounter.
     </para>
    </sect3>
   </sect2>
   <sect2 id="sec.depl.inst.admserv.post.local_repos.update">
    <title>Update Repositories</title>
    <para>
     Update repositories are already used when deploying the nodes that will
     build &cloud; in order to ensure they are initially equipped with the
     latest software versions available. If you are using a remote &smt; or
     SUSE Manager server, the update repositories from the remote server are
     used directly. In this case the repository URLs need to be added via
     the &yast; Crowbar module as explained in <xref
     linkend="sec.depl.inst.admserv.os.crowbar.repos"/>. If using
     repositories locally available, the &admserv; itself acts as the
     repository provider for all nodes. This requires to make them available
     in <filename>/srv/tftpboot/suse-11.3/repos</filename>.
    </para>
    <table>
     <title>Local Update Repositories for &cloud;</title>
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="25*"/>
      <colspec colnum="2" colname="2" colwidth="75*"/>
      <thead>
       <row>
        <entry>
         <para>
          Repository
         </para>
        </entry>
        <entry>
         <para>
          Directory
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>
         <para>
          SLES11-SP3-Pool
         </para>
        </entry>
        <entry>
         <para>
          <filename>/srv/tftpboot/suse-11.3/repos/SLES11-SP3-Pool</filename>
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          SLES11-SP3-Updates
         </para>
        </entry>
        <entry>
         <para>
          <filename>/srv/tftpboot/suse-11.3/repos/SLES11-SP3-Updates</filename>
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          SUSE-Cloud-5-Pool
         </para>
        </entry>
        <entry>
         <para>
          <filename>/srv/tftpboot/suse-11.3/repos/SUSE-Cloud-5-Pool</filename>
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          SUSE-Cloud-5-Updates
         </para>
        </entry>
        <entry>
         <para>
          <filename>/srv/tftpboot/suse-11.3/repos/SUSE-Cloud-5-Updates</filename>
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>

    <table>
     <title>
      Additional Local Update Repositories for an &hasetup; (optional)
     </title>
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="25*"/>
      <colspec colnum="2" colname="2" colwidth="75*"/>
      <thead>
       <row>
        <entry>
         <para>
          Repository
         </para>
        </entry>
        <entry>
         <para>
          Directory
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>
         <para>
          SLE11-HAE-SP3-Pool
         </para>
        </entry>
        <entry>
         <para>
          <filename>/srv/tftpboot/suse-11.3/repos/SLE11-HAE-SP3-Pool</filename>
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          SLE11-HAE-SP3-Updates
         </para>
        </entry>
        <entry>
         <para>
          <filename>/srv/tftpboot/suse-11.3/repos/SLE11-HAE-SP3-Updates</filename>
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>

    <sect3 id="sec.depl.inst.admserv.post.local_repos.update.smt">
     <title>Update Repositories Hosted on an &smt; Server</title>
     <para>
      An &smt; server makes the &suse; update repositories available within
      your network by regularly synchronizing with the &ncc;. To provide
      update repositories for &productname;, you can either use an existing
      &smt; server from a network that can be accessed from the cloud, or
      install an &smt; server on the &admserv;.
     </para>
     <variablelist>
      <varlistentry>
       <term>&smt; Server installed on a Remote Host</term>
       <listitem>
        <para>
         In order to use repositories from a remote &smt; server to deploy
         &cloud; you first need to make sure the products &sls; 11 SP3 and
         Cloud 5 are registered and the corresponding channels are mirrored
         in &smt;. Now you need to enter the repository URLs on the
         <guimenu>Repositories</guimenu> tab in the &yast; &crow; module as
         described in <xref
         linkend="sec.depl.inst.admserv.os.crowbar.repos"/>. A complete set of
         repository URLs is listed below. Note that you need to replace
         <replaceable>smt.&exampledomain;</replaceable> with the fully
         qualified host name of your &smt; server.
	</para>
	<simplelist>
	 <member>http://<replaceable>smt.&exampledomain;</replaceable>/repo/\$RCE/SLES11-SP3-Pool/sle-11-x86_64/</member>
	 <member>http://<replaceable>smt.&exampledomain;</replaceable>/repo/\$RCE/SLES11-SP3-Updates/sle-11-x86_64/</member>
	 <member>http://<replaceable>smt.&exampledomain;</replaceable>/repo/\$RCE/SUSE-Cloud-5-Pool/sle-11-x86_64/</member>
	 <member>http://<replaceable>smt.&exampledomain;</replaceable>/repo/\$RCE/SUSE-Cloud-5-Updates/sle-11-x86_64/</member>
	</simplelist>
	<para>
	 For the optional &hasetup; you also need the &ha; repositories:
	</para>
	<simplelist>
	 <member>http://<replaceable>smt.&exampledomain;</replaceable>/repo/\$RCE/SLE11-HAE-SP3-Pool/sle-11-x86_64/</member>
	 <member>http://<replaceable>smt.&exampledomain;</replaceable>/repo/\$RCE/SLE11-HAE-SP3-Updates/sle-11-x86_64/</member>
	</simplelist>

<!--
http://diddy-1.arch.suse.de/repo/SUSE/Products/SLE-SERVER/12/x86_64/product/
http://diddy-1.arch.suse.de/repo/SUSE/Updates/SLE-SERVER/12/x86_64/update/
-->
      
	<note>
	 <title>Accessing an External &smt; Server</title>
	 <para>
	  An external &smt; server needs to be accessed from the &admserv;
	  only. A network connection can either be established via a bastion
	  network (see <xref
	  linkend="sec.depl.inst.admserv.os.crowbar.mode.bastion"/> or an
	  external gateway.
	 </para>
	</note>
       </listitem>       
      </varlistentry>
      <varlistentry>
       <term>&smt; Server installed on the &admserv;</term>
       <listitem>
        <para>
	 The repositories mirrored by &smt; will automatically be linked by the
	 install script in <filename>/srv/tftpboot/suse-11.3/repos</filename> and
	 <filename>/srv/tftpboot/suse-12.0/repos</filename>.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </sect3>
    <sect3 id="sec.depl.inst.admserv.post.local_repos.update.susemgr">
     <title>Update Repositories Hosted on a &susemgr; Server</title>
     <para>
      In order to use repositories from &susemgr; to deploy &cloud; you first
      need to make sure the products &sls; 11 SP3 and &productname;
      &productnumber; are registered and the corresponding channels are
      mirrored in &susemgr;.  By default &susemgr; does not expose
      repositories for direct access.  In order to be able to access them via
      <literal>https</literal>, you need to create a
      <guimenu>Distribution</guimenu> for auto-installation for the &sls; 11
      SP3 (x86_64) Product on &susemgr;. Instructions can be found at
      <ulink
	url="https://www.suse.com/documentation/suse_manager/book_susemanager_ref/data/book_susemanager_ref.html"/>
      under the heading <guimenu>Creating Distribution for
      Autoinstallation</guimenu>. During the distribution setup you need to
      provide a <guimenu>Label</guimenu> for the distribution. This label
      will be part of the URL under which the repositories are available. It
      is recommended to choose a name consisting of characters that do not
      need to be URL-encoded, for example
      <literal>sles11-sp3-x86_64</literal>.
     </para>
     <para>
      Creating a distribution for &sls; 11 SP3 not only makes the
      installation data and Update repositories for this product available,
      but also for all registered add-on products, including &productname;
      &productnumber;.
     </para>
     <para>
      The repositories are available under the following URLs.
      <literal>manager.&exampledomain;</literal> needs to be replaced by the
      fully qualified host name of your &susemgr; server and
      <literal>sles11-sp3-x86_64</literal> needs to be replaced by the
      distribution label you specified when setting up the distribution for
      auto-installation. Note that the URLs are not browseable.
     </para>
     <variablelist>
      <varlistentry>
       <term>SLES11-SP3-Updates</term>
       <listitem>
        <para>
         <literal>http://manager.&exampledomain;/ks/dist/child/sles11-sp3-updates-x86_64/sles11-sp3-x86_64/</literal>
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>SUSE-Cloud-5-Pool</term>
       <listitem>
        <para>
         <literal>http://manager.&exampledomain;/ks/dist/child/suse-cloud-5-pool-x86_64/sles11-sp3-x86_64/</literal>
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>SUSE-Cloud-5-Updates</term>
       <listitem>
        <para>
         <literal>http://manager.&exampledomain;/ks/dist/child/suse-cloud-5-updates-x86_64/sles11-sp3-x86_64/</literal>
        </para>
       </listitem>
      </varlistentry>
     </variablelist>

     <para>
      For the optional &hasetup; you also need the &ha; repositories:
     </para>

     <variablelist>
      <varlistentry>
       <term>SLES11-SP3-HAE-Pool</term>
       <listitem>
        <para>
         <literal>http://manager.&exampledomain;/ks/dist/child/sle11-hae-sp3-pool-x86_64/sles11-sp3-x86_64/</literal>
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>SLES11-SP3-HAE-Updates</term>
       <listitem>
        <para>
         <literal>http://manager.&exampledomain;/ks/dist/child/sle11-hae-sp3-updates-x86_64/sles11-sp3-x86_64/</literal>
        </para>
       </listitem>
      </varlistentry>
     </variablelist>

     <para>
      To make the repositories available for node deployment, you need to
      enter the repository URLs on the <guimenu>Repositories</guimenu> tab in
      the &yast; &crow; module as described in <xref
      linkend="sec.depl.inst.admserv.os.crowbar.repos"/>.
     </para>
     <important>
      <title>Accessing a &susemgr; Server</title>
      <para>
       An external &susemgr; server needs to be accessed from
       <emphasis>all</emphasis> nodes in &cloud;. To be able to access it, the
       network hosting the &susemgr; server needs to be added to the network
       definitions as described in <xref
       linkend="sec.depl.inst.admserv.post.network.external"/>.
      </para>
     </important>

    </sect3>
    <sect3 id="sec.depl.inst.admserv.post.local_repos.update.remote">
     <title>Update Repositories Hosted on a Remote Host</title>
     <para>
      If the update repositories are hosted on a remote host that can be
      accessed from the &admserv; you can either mount them, for example via
      <literal>NFS</literal>, or regularly <command>rsync</command> them.
     </para>
     <para>
      To <literal>NFS</literal>-mount the repositories from a remote host,
      either use the &yast; <guimenu>NFS Client</guimenu> module or edit
      <filename>/etc/fstab</filename>. The local mount point should be
      <filename>/srv/tftpboot/suse-11.3/repos/<replaceable>&lt;REPOSITORY_NAME&gt;</replaceable></filename>.
     </para>
     <para>
      To <command>rsync</command> the repositories from a remote host,
      create a daily cron job running the following command on the
      &admserv;. This command will <emphasis>pull</emphasis> the files from
      a host named host.&exampledomain;:
     </para>
<screen><?dbsuse-fo font-size="0.63em"?>for REPO in SLES11-SP3-{Pool,Updates} SUSE-Cloud-5-{Pool,Updates}; do
  rsync -avPz host.&exampledomain;:/srv/www/htdocs/repo/\\\$RCE/$REPO/sle-11-x86_64/ \
  /srv/tftpboot/suse-11.3/repos/${REPO}/
done</screen>
     <para>
      For the optional &hasetup; you also need the &ha; repositories:
     </para>
<screen><?dbsuse-fo font-size="0.63em"?>for REPO in SLE11-HAE-SP3-{Pool,Updates}; do
  rsync -avPz host.&exampledomain;:/srv/www/htdocs/repo/\\\$RCE/$REPO/sle-11-x86_64/ \
  /srv/tftpboot/suse-11.3/repos/${REPO}/
done</screen>
     <para>
      Alternatively you may set up the cron job on the remote host and
      <emphasis>push</emphasis> the files to the &admserv; (which has the IP
      address <systemitem class="etheraddress">192.168.124.10</systemitem>
      in the following example):
     </para>
     <screen><?dbsuse-fo font-size="0.63em"?>for REPO in SLES11-SP3-{Pool,Updates} SUSE-Cloud-5-{Pool,Updates}; do
  rsync -avPz /srv/www/htdocs/repo/\\\$RCE/$REPO/sle-11-x86_64/ \
  <replaceable>192.168.124.10</replaceable>:/srv/tftpboot/suse-11.3/repos/${REPO}/
done</screen>

     <para>And for the &hasetup;:</para>

     <screen><?dbsuse-fo font-size="0.63em"?>for REPO in SLE11-HAE-SP3-{Pool,Updates}; do
  rsync -avPz /srv/www/htdocs/repo/\\\$RCE/$REPO/sle-11-x86_64/ \
  <replaceable>192.168.124.10</replaceable>:/srv/tftpboot/suse-11.3/repos/${REPO}/
done</screen>

     <note>
      <title>Mind the Trailing Slash</title>
      <para>
       The <command>rsync</command> command must be used with trailing slashes
       in the directory names as shown above. Otherwise rsync would copy the
       repositories into the wrong directory.
      </para>
     </note>
     <note>
      <title>Accessing a remote Host</title>
      <para>
       A remote machine hosting the update repositories needs to be accessed
       from the &admserv; only. A network connection can either be established
       via a bastion network (see <xref
       linkend="sec.depl.inst.admserv.os.crowbar.mode.bastion"/> or an
       external gateway.
      </para>
     </note>
    </sect3>
    <sect3 id="sec.depl.inst.admserv.post.local_repos.update.sneaker">
     <title>Update Repositories Hosted on Removable Media (<quote>Sneakernet</quote>)</title>
     <para>
      If your admin network is isolated from other networks, you need to
      manually synchronize the update repositories from removable media. To do so
      you can either use <command>rsync</command> (see above for an example)
      or <command>cp <option>-axu</option></command>. If copying from a
      &smt; server, see
      <xref linkend="sec.depl.inst.admserv.post.smt_repos"/> for a list of
      directories to copy.
     </para>
    </sect3>
   </sect2>
  </sect1>

  <sect1 id="sec.depl.inst.admserv.post.adm_repos">
   <title>Software Repository Sources on the &admserv;</title>
   <para>
    Update repositories are not only required to deploy &cloud;. The
    &admserv; itself also needs to be kept up-to-date and therefore needs to
    have a proper repository setup. In case you have registered &sls; and
    &productname; during the installation process, the &admserv; already has
    all required update repositories.
   </para>
   <para>
    These repositories are served directly from &ncc;. To avoid
    downloading the same patches twice or in case you want to cut off
    the &admserv; from the Internet, it makes sense to change this setup in
    a way that the repositories set up for &cloud; deployment are also used
    on the &admserv;. To do so, you need to disable or delete all services.
    In a second step all &sls; and &cloud; repositories need to be edited in
    order to point to the alternative sources. Editing the repository setup
    can either be done with Zypper or &yast;. Note that changing the
    repository setup on the &admserv; is optional.
   </para>
  </sect1>

  <sect1 id="sec.depl.inst.admserv.post.network">
   <title>Custom Network Configuration</title>
   <para>
    In case you need to adjust the pre-defined network setup of &cloud;
    beyond the scope of changing IP address assignments (as described in
    <xref linkend="sec.depl.inst.admserv.os.crowbar"/>), you need to
    manually modify the network &barcl; template. Refer to
    <xref linkend="app.deploy.network_json"/> for details.
   </para>
   <sect2 id="sec.depl.inst.admserv.post.network.external">
    <title>Providing Access to External Networks</title>
    <para>
     By default, external networks cannot be reached from nodes in the
     &cloud;. In order to be able to access external services such as a
     &susemgr; server an &smt; server, or a SAN, you need to make the external
     network(s) known to &cloud;. This is achieved by adding a network
     definition for each external network to
     <filename>/etc/crowbar/network.json</filename>. Refer to <xref
     linkend="app.deploy.network_json"/> for setup instructions.
    </para>
    <example>
     <title>
      Example Network Definition for the External Network 192.168.150.0/16
     </title>
     <screen>            "external" : {
               "add_bridge" : false,
               "vlan" : <replaceable>XXX</replaceable>,
               "ranges" : {
                  "host" : {
                     "start" : "192.168.150.1",
                     "end" : "192.168.150.254"
                  }
               },
               "broadcast" : "192.168.150.255",
               "netmask" : "255.255.255.0",
               "conduit" : "intf1",
               "subnet" : "192.168.150.0",
               "use_vlan" : true
            }</screen>
    </example>
    <para>
     The value <replaceable>XXX</replaceable> for the VLAN needs to be
     replaced by a value not used within the &cloud; network and not used by
     &o_netw;. By default, the following VLANs are already used:
    </para>
    <table>
     <title>VLANs used by the &cloud; Default Network Setup</title>
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="20*"/>
      <colspec colnum="2" colname="2" colwidth="80*"/>
      <thead>
       <row>
	<entry><para>VLAN ID</para></entry>
	<entry><para>Used by</para></entry>
       </row>
      </thead>
      <tbody>
       <row>
	<entry><para>100</para></entry>
	<entry><para>BMC VLAN (bmc_vlan)</para></entry>
       </row>
       <row>
	<entry><para>200</para></entry>
	<entry><para>Storage Network</para></entry>
       </row>
       <row>
	<entry><para>300</para></entry>
	<entry><para>Public Network (nova-floating, public)</para></entry>
       </row>
       <row>
	<entry><para>400</para></entry>
	<entry><para>Software-defined network (os_sdn)</para></entry>
       </row>
       <row>
	<entry><para>500</para></entry>
	<entry><para>Private Network (nova-fixed)</para></entry>
       </row>
       <row>
	<entry><para>501 - 2500</para></entry>
	<entry><para>&o_netw; (value of nova-fixed plus 2000)</para></entry>
       </row>
      </tbody>
     </tgroup>
    </table>
    
   </sect2>
  </sect1>

  <sect1 id="sec.depl.inst.admserv.post.cloud_installation">
   <title>Running the Cloud Installation Script</title>
   <para>
    Before running the cloud installation script to finish the configuration
    of the &admserv; make sure to double-check the following items.
   </para>
   <itemizedlist>
    <title>Final Check Points</title>
    <listitem>
     <para>
      Make sure the network configuration is correct. Run <menuchoice>
      <guimenu>&yast;</guimenu> <guimenu>&crow;</guimenu> </menuchoice> to
      review/change the configuration. See
      <xref
	linkend="sec.depl.inst.admserv.os.crowbar"/> for further
      instructions.
     </para>

     <important>
      <title>An &haSetup; Requires Teaming Network Mode</title>
      <para>
       In case you are planning to make &cloud; highly available upon the
       initial setup or at a later point in time, make sure to set up the
       network in teaming mode. Such a setup requires at least two network
       cards for each node. 
      </para>
     </important>
     
    </listitem>
    <listitem>
     <para>
      Make sure <command>hostname <option>-f</option></command> returns a
      fully qualified host name. See
      <xref
	linkend="sec.depl.inst.admserv.os.network"/> for further
      instructions.
     </para>
    </listitem>
    <listitem>
     <para>
      Make sure all update and product repositories are available.
      See <xref linkend="sec.depl.inst.admserv.post.local_repos"/> for
      further instructions.
     </para>
    </listitem>
    <listitem>
     <para>
      Make sure the operating system and &productname; are up-to-date and
      have the latest patches installed. Run <command>zypper patch</command>
      to install them.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Now everything is in place to finally configure the &admserv;. This is
    done by running the script <filename>install-suse-cloud</filename>. This
    command will install and configure &chef;, and use it to complete the
    installation of &crow; and all required &barcl;s. It will take several
    minutes to complete. If you are <emphasis>not</emphasis> using &susemgr;
    to provide update repositories, run the following command:
   </para>
   <screen>screen install-suse-cloud</screen>
   <para>
    In case you are using &susemgr; (as described in
    <xref
      linkend="sec.depl.inst.admserv.post.local_repos.update.susemgr"/>),
    you need to run the following command:
   </para>
   <screen>screen env REPOS_SKIP_CHECKS+=" SLES11-SP3-Pool" install-suse-cloud</screen>
   <important>
    <title>Use a Terminal Multiplexer to run the Cloud Installation Script</title>
    <para>
     Run the installation script <filename>install-suse-cloud</filename>
     inside of a terminal multiplexer like GNU Screen (provided by the
     <systemitem class="resource">screen</systemitem> package).
    </para>
    <para>
     During the run of this script the network will be reconfigured. This
     may result in interrupting the script when being run from a network
     connection (like SSH). Using <command>screen</command> will continue
     running the script in a session to which you can reconnect via
     <command>screen -r</command> if you lose the connection.
    </para>
   </important>
   <para>
    <command>install-suse-cloud</command> will produce a lot of output that
    gets written to a log file located at
    <filename>/var/log/crowbar/install.log</filename>. Check this log file
    in case something goes wrong. You can run
    <command>install-suse-cloud</command> multiple times as long as you have
    not started to deploy the &ostack; services. It is also possible to run
    <command>install-suse-cloud</command> in verbose mode with the
    <option>-v</option> switch. It will show the same output that goes to
    the log file on STDOUT, too.
   </para>
   <para>
    If the script has successfully finished, you will see a message telling
    you how to log in to the &crow; Web interface.
   </para>
   &no_network_changes;
   <figure>
    <title>&crow; Web Interface: Initial State</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="depl_node_dashboard_initial.png" width="100%"
		 format="png"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="depl_node_dashboard_initial.png" width="75%"
		 format="png"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect1>
 </chapter>


