<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet
 href="urn:x-daps:xslt:profiling:novdoc-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "IGNORE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.depl.maintenance">
 <title>&cloud; Maintenance</title>
 <para/>
 <sect1 id="sec.depl.maintenance.updates">
  <title>Keeping the Nodes Up-to-date</title>
  <para>
   Keeping the nodes in &productname; up-to-date requires an appropriate
   setup of the update repositories (see
   <xref
     linkend="sec.depl.inst.admserv.post.local_repos.update"/> and the
   deployment of either the <guimenu>Updater</guimenu> &barcl; (see
   <xref
     linkend="sec.depl.inst.nodes.post.updater"/>) or the &susemgr;
   &barcl; (see <xref linkend="sec.depl.inst.nodes.post.manager"/>.
  </para>
  <para>
   If one of those &barcl;s is deployed, patches are installed on the nodes.
   Installing patches that do not require a reboot of a node does not come
   with any service interruption. In case a patch (for example a kernel
   update) requires a reboot after the installation, services running on the
   machine that is rebooted, will not be available within &cloud;. Therefore
   it is strongly recommended to install those patches during a maintenance
   window.
  </para>
  <note>
   <title>No Maintenance Mode</title>
   <para>
    As of &productname; &productnumber; it is not possible to put &cloud;
    into <quote>Maintenance Mode</quote>.
   </para>
  </note>
  <remark condition="clarity">
   2013-10-02 - fs: The following is mainly based on assumptions...
  </remark>
  <variablelist>
   <title>Consequences when Rebooting Nodes</title>
   <varlistentry>
    <term>&admserv;</term>
    <listitem>
     <para>
      While the &admserv; is offline, it is not possible to deploy new nodes.
      However, rebooting the &admserv; has no effect on starting &vmguest;s or
      on &vmguest;s already running.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&contrnode;s</term>
    <listitem>
     <para>
      The consequences a reboot of a &contrnode; has, depends on the services
      running on that node:
     </para>
     <formalpara>
      <title>Database, &o_ident;, RabbitMQ, &o_img;, &o_comp;:</title>
      <para>
       No new &vmguest;s can be started.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_objstore;:</title>
      <para>
       No object storage data is available. If &o_img; uses &o_objstore;, it
       will not be possible to start new &vmguest;s.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_blockstore;, &ceph;:</title>
      <para>
       No block storage data is available.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_netw;:</title>
      <para>
       No new &vmguest;s can be started. On running &vmguest;s the network
       will be unavailable.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_dash;</title>
      <para>
       &o_dash; will be unavailable. Starting and managing &vmguest;s can be
       done with the command line tools.
      </para>
     </formalpara>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&compnode;s</term>
    <listitem>
     <para>
      <remark condition="clarity">
       2013-10-02 - fs: How to ensure no new instances are started on a compute
       node while evacuating it? What about Windows compute nodes? What about
       VMware?
      </remark>
      Whenever a &compnode; is rebooted, all &vmguest;s running on that
      particular node will be shut down and must be manually restarted.
      Therefore it is recommended to <quote>evacuate</quote> the node by
      migrating &vmguest;s to another node, before rebooting it. <!-- (see FIXME
      for details) -->
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.depl.maintenance.upgrade">
  <title>Upgrading from &productname; 2.0 to &productname; 3</title>

<!-- fs 2014-03-25: FIXME

Server names change when using HA and that requires to create new certificates
for Cloud. See pacemaker barclamp for more information 

-->


  <para>
   Upgrading from &productname; 2.0 to &productname; 3 is supported, with the
   following manual steps. Please note, however, that it will turn off the
   &ostack; cloud during the upgrade and it is not possible to downgrade back
   to &productname; 2.0 in case of a regression or problem. It is highly
   recommended to back up data before the upgrade.
  </para>
  <itemizedlist>
   <title>Prerequisites</title>
   <listitem>
    <para>
     all allocated nodes are turned on
    </para>
   </listitem>
   <listitem>
    <para>
     all nodes, including the &admserv;, have the latest &productname; 2.0
     updates installed. If this is not the case, refer to <xref
     linkend="sec.depl.inst.nodes.post.updater"/> for instructions.
    </para>
   </listitem>
   <listitem>
    <para>
     all instances running in &cloud; must be suspended
    </para>
   </listitem>
  </itemizedlist>
  
  <important>
   <title>
    Instance Running on HyperV Nodes will not <quote>survive</quote> an
    Upgrade
   </title>
   <para>
    As of &productname; 3, HyperV Nodes need to be re-installed after the
    upgrade procedure. This re-installation will overwrite the instance's data
    and therefore they will be lost. &kvm;, VMWare and &xen; instances are not
    affected.
   </para>
  </important>

  <procedure>
   <title> Upgrading &productname; 2.0 to 3</title>
   <step>
    <para>
     Make sure to subscribe to the SUSE Cloud 3 Pool and Update channels on
     the source that provides these channels for your &cloud; installation
     (for example an &smt; or &susemgr; server).
    </para>
    <para>
     In case the channels are provided by an &smt; server (either remote or on
     the &admserv; itself), refer to <xref
     linkend="sec.depl.inst.admserv.post.smt_repos"/> for instructions on
     subscribing to the &cloud; 3 channels.
    </para>
   </step>
   <step>
    <para>
     Update the &productname; product repository on the &admserv;. To do so,
     insert the &productname; 3 DVD and run the following commands:
    </para>
    <screen>mount /dev/dvd /mnt
rsync -avP --delete /mnt/ /srv/tftpboot/repos/Cloud/
umount /mnt</screen>
   </step>
   <step>
    <para>
    Update the &cloud; repository links on the &admserv;.
    </para>
    <para>
     If you are using external URLs (for example from an external &smt; or
     &susemgr; server) for the SUSE Cloud 2.0 Pool and Update repositories,
     you need to update the file
     <filename>/etc/crowbar/provisioner.json</filename>. Open it in an editor
     and Search for the <menuchoice> <guimenu>suse</guimenu>
     <guimenu>autoyast</guimenu> <guimenu>repos</guimenu></menuchoice>
     entry. Replace the URLs pointing to the &productname; 2.0 Pool and Update
     repositories with the ones pointing to version 3. Also change the name of
     these repositories to SUSE-Cloud-3-Pool and SUSE-Cloud-3-Updates,
     respectively.
    </para>
    <para>
     In case you are using an &smt; server installed on the &admserv;, run the
     following commands to make the repositories available for &cloud;:
    </para>
    <screen>for REPO in SUSE-Cloud-3.0-{Pool,Updates}; do
ln -s /srv/www/htdocs/repo/\$RCE/$REPO/sle-11-x86_64 /srv/tftpboot/repos/${REPO/.0/}
done</screen>
   </step>
   <step>
    <para>
     Now you need to replace the &productname; 2.0 Pool and Update
     repositories used by the &admserv; with the ones for version 3. Modify
     the repository configuration by running <menuchoice>
     <guimenu>&yast;</guimenu> <guimenu>Software</guimenu> <guimenu>Software
     Repositories</guimenu></menuchoice> on the &admserv;. 
    </para>
   </step>
   <step>
    <para>
     Refresh the repositories and install the <systemitem
     class="resource">suse-cloud-upgrade</systemitem> package by running the
     following commands on the &admserv;:
    </para>
    <screen>zypper refresh
zypper install suse-cloud-upgrade</screen>
   </step>
   <step>
    <para>
     Run <command>suse-cloud-upgrade <option>upgrade</option></command> for
     the first time. It will completely shut down the &ostack; and the &crow;
     Web interface. In case the command fails, please contact the &suse;
     support (see <xref linkend="sec.depl.trouble.support"/> for more
     information). At this point of the upgrade workflow it is still possible
     to go back to version 2.0.
    </para>
   </step>
   <step>
    <para>
     Update all packages on the &admserv; by running <command>zypper
     <option>up</option></command>. During the update process you will need to
     accept the &productname; 3 EULA. 
    </para>
   </step>
   <step>
    <para>
     Run <command>suse-cloud-upgrade <option>upgrade</option></command> for
     the second time. Now the &crow; Web interface is available again. In case
     the command fails, please contact the &suse; support (see <xref
     linkend="sec.depl.trouble.support"/> for more information). From this
     point in the installation workflow on it is no longer possible to go back
     to version 2.0.
    </para>
   </step>
   <step>
    <para>
     Go to the &crow; Web interface and open <menuchoice>
     <guimenu>Barclamps</guimenu> <guimenu>Crowbar</guimenu>
     </menuchoice>. <guimenu>Edit</guimenu> the <guimenu>Provisioner</guimenu>
     proposal and <guimenu>Apply</guimenu> it. 
    </para>
   </step>
   <step>
    <para>
     Update packages on all nodes except the &admserv; as described in <xref
     linkend="sec.depl.inst.nodes.post.updater"/>. You should use the
     following configuration:
    </para>
    <simplelist>
     <member><guimenu>Use zypper</guimenu>: update</member>
     <member><guimenu>Enable GPG checks</guimenu>: true</member>
     <member><guimenu>Automatically agree with licenses</guimenu>: true</member>
    </simplelist>
   </step>
   <step>
    <para>
     Reboot all nodes except the &admserv;. This can either be done manually
     or by running <command>suse-cloud-upgrade
     <option>reboot-nodes</option></command>. Wait until all nodes are up and
     running again before proceeding.
    </para>
   </step>
   <step>
    <para>
     Go to the &crow; Web interface and open <menuchoice>
     <guimenu>Barclamps</guimenu> <guimenu>Openstack</guimenu>
     </menuchoice>. Open the &barcl; one-by-one in the given order by clicking
     <guimenu>Edit</guimenu> and re-apply each proposal with
     <guimenu>Apply</guimenu>.
    </para>
   </step>
   <step>
    <para>
     In case you have deployed Microsoft HyperV nodes, you need to re-install
     them. Go to the &crow; Web interface and open the
     <guimenu>Dashboard</guimenu>. For each HyperV node, click on its name
     and then on <guimenu>Reinstall</guimenu>.
    </para>
   </step>
  </procedure>
  <para>
   Once the upgrade procedure has been finished, &cloud; should be up and
   running again. You may restart all suspended instances now.
  </para>
 </sect1>
 <sect1 id="sec.depl.maintenance.hasetup">
  <title>Upgrading to an &hasetup;</title>
  <para>
   When making an existing &cloud; deployment highly available by setting up
   HA clusters and moving roles to these clusters, there are a few issues
   to pay attention to. To make existing services highly available, proceed as
   follows. Note that moving to an &hasetup; cannot be done without &cloud;
   service interruption, because requires to stop &ostack; services.
  </para>
  &note-ha-teaming;
  <procedure>
   <step>
    <para>
     Make sure to have read FIXME.
    </para>
   </step>
   <step>
    <para>
     Set up your cluster(s) as described in <xref
     linkend="sec.depl.ostack.pacemaker"/>. 
    </para>
   </step>
   <step>
    <para>
     To move a particular role from a regular control node to a cluster, you
     need to stop the associated service(s) before re-deploying the role on a
     cluster:
    </para>
    <substeps>
     <step>
      <para>
       Log in to each node on which the role is deployed and stop it's
       associated service(s) (a role can have multiple services). Do so by
       running the service's start/stop script with the stop argument, for
       example:  
      </para>
      <screen>rcopenstack-keystone stop</screen>
      <para>
       See FIXME for a list of roles, services and start/stop scripts.
      </para>
     </step>
     <step>
      <para>
       The following roles need additional treatment: 
      </para>
      <variablelist>
       <varlistentry>
	<term>database-server (Database &barcl;)</term>
	<listitem>
	 <para>
	  Copy the content of <filename>/var/lib/pgsql/data/</filename> from
	  the original database node to the cluster node with DRBD or shared
	  storage.
	 </para>
	</listitem>
       </varlistentry>
       <varlistentry>
	<term>keystone-server (&o_ident; &barcl;)</term>
	<listitem>
	 <para>
	  If using &o_ident; with PKI tokens, the PKI keys on all nodes need
	  to be re-generated. This can be achieved by removing the contents of
	  <filename>/var/cache/*/keystone-signing/</filename> on the
	  nodes. Use a command similar to the following on the &admserv; as
	  &rootuser;:
	 </para>
	 <screen>for NODE in <replaceable>NODE1</replaceable>
	 <replaceable>NODE2</replaceable> <replaceable>NODE3</replaceable>; do
  ssh $NODE rm /var/cache/*/keystone-signing/*
done</screen>
	</listitem>
       </varlistentry>
       <varlistentry>
	<term>rabbitmq-server (RabbitMQ &barcl;) </term>
	<listitem>
	 <para>
	  Copy the content of <filename>/var/lib/rabbitmq/</filename> from
	  the original RabbitMQ node to the cluster node with DRBD or shared
	  storage.
	 </para>
	</listitem>
       </varlistentry>
      </variablelist>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Go to the &barcl; featuring the role you want to move to the
     cluster. Remove the node the role is currently running on from the left
     side of the <guimenu>Deployment</guimenu> section and replace it with a
     cluster from the <guimenu>Available Clusters</guimenu> section.
    </para>
   </step>
   <step>
    <para>
     Repeat these steps for all roles you would like to move to cluster. See
     FIXME for a list of services with HA support.
    </para>
   </step>
  </procedure>
  
  <important>
   <title>SSL Certificates</title>
   <para>
    Moving to an &hasetup; also requires to create SSL certificates for nodes
    in the cluster than run services using SSL. Certificates need to be issued
    for the generated names (see <xref
    linkend="ann.depl.ostack.pacemaker.prop_name"/>) as well as for all public
    names you have configured in the cluster.
   </para>
  </important>
  
  &ha_service_management;
  
 </sect1>
 <sect1 id="sec.depl.maintenance.backup.admin">
  <title>Backing Up and Restoring the &admserv;</title>
  <para>As an example of how to back up and restore the data on the
   &admserv;, a <filename>crowbar-backup</filename> script is shipped with
   the maintenance update for &productname; 3. </para>
  <warning>
   <title>No Support for <filename>crowbar-backup</filename></title>
   <para>The script serves as a functioning <emphasis>example</emphasis> of how
    to back up and restore an &admserv;. Using it in a production
    environment is <emphasis>not</emphasis> supported. </para>
   <para>However, you can adjust the script to your specific setup or use it as
    a template to create your own backup and restore routine.</para>
   <para>Find the latest updates for the script at <ulink
     url="https://github.com/SUSE-Cloud/cloud-tools"/>.</para>
  </warning>
  <para>The script is part of the <systemitem class="resource"
    >crowbar</systemitem> package and is installed to
    <filename>/usr/sbin/crowbar-backup</filename> on the &admserv;. It is
   tailored to back up and restore the &admserv; of the respective
   &productname; version the script is shipped with. When calling the
   <command>crowbar-backup</command> script with the <command>backup</command>
   option, it will create a TAR archive with all configuration data that is
   needed to restore the &admserv;. For a list of available options and a
   short help text, run</para>
  <screen>./usr/sbin/crowbar-backup <option>help</option></screen>

  <variablelist>
   <title>Example Commands for Using the Script</title>
   <varlistentry>
    <term/>
    <listitem>
     <para/>
    </listitem>
   </varlistentry>
  </variablelist>


  <!--
   
   >- Die Usage sieht so aus (ich denke ihr solltet die 4 bzw. 5 Kommandos 
   >  kurz erklären):
   >
   >_____________snip_____________
   >Usage: ./crowbar-backup [help|backup|restore|cleanup|purge] [<FILE>]
   >
   > backup [<FILE>]
   >     create a backup tarball of admin node config data in <FILE>
   > restore [<FILE>]
   >     restore a backup of an admin node config from tarball <FILE>
   > cleanup
   >     stop services and delete config files related to crowbar
   > purge
   >     wipe all packages and config files related to crowbar
   >
   > If <FILE> is omitted it defaults to /tmp/backup-crowbar.tar.gz
   >_____________snip___________
   >
   >- Bei jedem Aufruf des Scripts (außer bei help) erscheint folgende
   >Meldung,
   >  die man aktiv bestätigen muss:
   >____________snip__________
   
   >Please Note!
   >This is an unsupported example script. It is a working
   >documentation of the backup and restore process.
   >Use with care. You may need to adapt it to your setup.
   >Continue? (y/N)
   >____________snip_____________
   >

   >  Bitte verweist aber auf das Github repo in dem wir das Script fixen
   > und weiterentwickeln bzw. Support für weitere Cloud-Versionen
   > einbauen. https://github.com/SUSE-Cloud/cloud-tools
   >
   >Der Verweis auf das Repo ist auch nötig, da das Script aus dem Crowbar
   >Paket kommt, dass _vor_ einem Restore noch nicht installiert ist. Dh.
   >in dem Fall muss man sich das Script über einen anderen Weg holen.
   >
   
   >  

   (04:20:00 PM) taroth: "Dh. in dem Fall muss man sich das Script 
   über einen anderen Weg holen."= aus dem git repo?
   (04:20:09 PM) jdsn: genau 
   (04:20:15 PM) jdsn: oder aus dem RPM manuell auspacken 
   (04:20:23 PM) taroth: ok
   (04:20:43 PM) jdsn: oder nur das crowbar paket installieren und dann den Restore beginnen - sollte auch
   gehen 
   (04:20:51 PM) jdsn: aber man muss halt irgendwie rankommen 
   (04:21:29 PM) jdsn: oh, eins noch, das steht nur in einem Kommentar -
   beim Restore geht das Script davon aus, dass alle
   notwendigen Installationsquellen vorhanden sind 
   (04:21:48 PM) jdsn: sprich Cloud + evtl. HA Repo 
   (04:23:14 PM) taroth: ah, sehe es - zeile 183 
   (04:25:11 PM) taroth: die cleanup und purge kommandos, die du erwähnt hast  - wann
   brauche ich die? 
   (04:25:36 PM) jdsn: wenn man einen Restore testen möchte 
   (04:25:47 PM) jdsn: ein purge ist cleanup + uninstall 
   (04:25:58 PM) jdsn: also noch cleaner als cleanup 
   (04:26:17 PM) jdsn: oder wenn man einen kaputt configurierten Crowbar
   säubern möchte 
   (04:26:41 PM) jdsn: ohne Backup ist dann aber wirklich alles weg :)
   (04:27:00 PM) taroth: das wollte ich gerade fragen :) 
   (04:27:41 PM) jdsn: also mein üblicher Test war immer 1. Cloud
   installieren 2. crowbar-backup  backup &&
   crowbar-backup purge && crowbar-backup restore
   (04:27:59 PM) taroth: verstehe 
   (04:28:23 PM) taroth: und bevor ich ein backup starte, muss ich
   irgendwelche dienste stoppen oder gibt es irgendeinen
   maintenance mode in den ich den admin server versetzten
   muss/kann? 
   (04:28:44 PM) jdsn: das Skript stoppt, das was nötig ist selbst 
   (04:28:47 PM) jdsn: und startet es auch wieder 
   (04:29:12 PM) jdsn: also beim "crowbar-backup backup" 
   (04:29:23 PM) jdsn: beim cleanup und purge wird nur gestoppt 
   (04:30:21 PM) jdsn:https://github.com/SUSE-Cloud/cloud-tools/blob/master/backup/crowbar-backup#L107
   (04:30:23 PM) taroth: ok, wunderbar 
   (04:33:07 PM) taroth: dann denke ich, das passt erstmal soweit 
   (04:33:17 PM) jdsn: ok - danke 
   (04:33:22 PM) taroth: vielen dank für die mail und den input hier
   (04:33:29 PM) jdsn: gerne 
  -->
 </sect1>
 
</chapter>
