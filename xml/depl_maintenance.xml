<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.depl.maintenance">
 <title>&cloud; Maintenance</title>
 <info>
<dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
    <dm:maintainer>fs</dm:maintainer>
    <dm:status>editing</dm:status>
    <dm:deadline/>
    <dm:priority/>
    <dm:translation>no</dm:translation>
    <dm:languages/>
</dm:docmanager>
</info>
<para/>
 <sect1 xml:id="sec.depl.maintenance.updates">
  <title>Keeping the Nodes Up-to-date</title>

  <para>
   Keeping the nodes in &productname; up-to-date requires an appropriate
   setup of the update and pool repositories  and the deployment of
   either the <guimenu>Updater</guimenu> &barcl; or the &susemgr;
   &barcl;. For details, see
   <xref linkend="sec.depl.adm_conf.repos.scc"/>, <xref
   linkend="sec.depl.inst.nodes.post.updater"/>, and
   <xref linkend="sec.depl.inst.nodes.post.manager"/>.
 </para>

  <para>
   If one of those &barcl;s is deployed, patches are installed on the
   nodes. Installing patches that do not require a reboot of a node does not
   come with any service interruption. If a patch (for example, a kernel
   update) requires a reboot after the installation, services running on the
   machine that is rebooted will not be available within &cloud;.
   Therefore it is strongly recommended to install those patches during a
   maintenance window.
  </para>

  <note>
   <title>No Maintenance Mode</title>
   <para>
    As of &productname; &productnumber; it is not possible to put
    &cloud; into <quote>Maintenance Mode</quote>.
   </para>
  </note>

  <remark condition="clarity">
   2013-10-02 - fs: The following is mainly based on assumptions...
  </remark>

  <variablelist>
   <title>Consequences when Rebooting Nodes</title>
   <varlistentry>
    <term>&admserv;</term>
    <listitem>
     <para>
      While the &admserv; is offline, it is not possible to deploy new
      nodes. However, rebooting the &admserv; has no effect on starting
      &vmguest;s or on &vmguest;s already running.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&contrnode;s</term>
    <listitem>
     <para>
      The consequences a reboot of a &contrnode; has, depends on the
      services running on that node:
     </para>
     <formalpara>
      <title>Database, &o_ident;, RabbitMQ, &o_img;, &o_comp;:</title>
      <para>
       No new &vmguest;s can be started.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_objstore;:</title>
      <para>
       No object storage data is available. If &o_img; uses
       &o_objstore;, it will not be possible to start new &vmguest;s.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_blockstore;, &ceph;:</title>
      <para>
       No block storage data is available.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_netw;:</title>
      <para>
       No new &vmguest;s can be started. On running &vmguest;s the
       network will be unavailable.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_dash;</title>
      <para>
       &o_dash; will be unavailable. Starting and managing &vmguest;s
       can be done with the command line tools.
      </para>
     </formalpara>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&compnode;s</term>
    <listitem>
     <para>
      <remark condition="clarity">
       2013-10-02 - fs: How to ensure no new instances are started on a compute
       node while evacuating it? What about Windows compute nodes? What about
       VMware?
      </remark>
      Whenever a &compnode; is rebooted, all &vmguest;s running on
      that particular node will be shut down and must be manually restarted.
      Therefore it is recommended to <quote>evacuate</quote> the node by
      migrating &vmguest;s to another node, before rebooting it.
<!-- (see ???
      for details) -->
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

 <sect1 xml:id="sec.depl.maintenance.service_order">
  <title>Service Order on &cloud; Start-up or Shutdown</title>
  <para>
   In case you need to restart your complete &cloud; (after a complete shut
   down or a power outage), the services need to started in the following
   order:
  </para>
  <orderedlist>
   <listitem>
    <para>
     &contrnode;/Cluster on which the Database is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which RabbitMQ is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which &o_ident; is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     Any remaining &contrnode;/Cluster. The following additional rules apply:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The &contrnode;/Cluster on which the <literal>neutron-server</literal>
       role is deployed needs to be started before starting the node/cluster
       on which the <literal>neutron-l3</literal> role is deployed.
      </para>
     </listitem>
     <listitem>
      <para>
       The &contrnode;/Cluster on which the <literal>nova-controller</literal>
       role is deployed needs to be started before starting the node/cluster
       on which &o_orch; is deployed.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     &compnode;s
    </para>
   </listitem>
  </orderedlist>
  <para>
   If multiple roles are deployed on a single &contrnode;, the services are
   automatically started in the correct order on that node. If you have more
   than one node on which multiple roles are installed, make sure they are
   started in a way that the order listed above is met as best as possible.
  </para>
  <para>
   If you need to shut down &cloud;, the services need to be terminated in
   reverse order than on start-up:
  </para>

  <orderedlist>
   <listitem>
    <para>
     &compnode;s
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which &o_orch; is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which the <literal>nova-controller</literal>
     role is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which the <literal>neutron-l3</literal>
     role is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     All &contrnode;(s)/Cluster(s) on which neither of the following services
     is deployed: Database, RabbitMQ, and &o_ident;.
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which &o_ident; is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which RabbitMQ is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which the Database is deployed
    </para>
   </listitem>
  </orderedlist>
 </sect1>

 <sect1 xml:id="sec.depl.maintenance.upgrade">
  <title>Upgrading from &productname; 5 to &productname; 6</title>

  <para>
   Upgrading from &productname; 5 to &productname; 6 is done via a &wi;
   guiding you through the process. The process consists of four phases:
  </para>
  <orderedlist>
   <listitem>
    <para>
     Saving the configuration data of your &productname; 5 installation in a
     data dump.
    </para>
   </listitem>
   <listitem>
    <para>
     Re-Installing and setting up the &admserv; with &productname; 6.
    </para>
   </listitem>
   <listitem>
    <para>
     Upgrading the all nodes to &cloudos; and &productname; 6.
    </para>
   </listitem>
   <listitem>
    <para>
     Re-applying the &barcl;s.
    </para>
   </listitem>
  </orderedlist>

  <sect2 xml:id="sec.depl.maintenance.upgrade.require">
   <title>Requirements</title>
   <para>
    Before you start upgrading &productname;, make sure the following
    requirements are met:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      The &admserv; needs to have the latest &productname; 5 updates
      installed. One of these updates will add the new upgrade routine to the
      &crow; &wi;.
     </para>
    </listitem>
    <listitem>
     <para>
      All other nodes need to have the latest &productname; 5 updates
      <emphasis>and</emphasis> the latest &slsa; updates. If this is not the
      case, refer to <xref linkend="sec.depl.inst.nodes.post.updater"/> for
      instructions.
     </para>
    </listitem>
    <listitem>
     <para>
      All allocated nodes need to be turned on.
     </para>
    </listitem>
    <listitem>
     <para>
      During the upgrade of the &contrnode;s and the &compnode;s the
      &vmguest;s need to be shut down. However it is not necessary
      to do so at the beginning of the upgrade procedure. This step can be
      postponed until after the &admserv; has been upgraded to &productname; 6
      to keep the downtime as short as possible.
     </para>
     <important>
      <title>
       Instance Running on HyperV Nodes Will Not <quote>Survive</quote> an
       Upgrade
      </title>
      <para>
       As of &productname; 6, HyperV Nodes need to be re-installed after the
       upgrade procedure. This re-installation will overwrite the instance's
       data and therefore they will be lost. &kvm;, VMware and &xen;
       instances are not affected.
      </para>
     </important>
     <tip>
      <title>Back Up the &admserv;</title>
      <para>
       It is strongly recommended to create a backup of the &admserv; before
       starting the upgrade procedure, to be able to restore the server in
       case the upgrade fails. Refer to the chapter <link
       xlink:href="&suse-onlinedoc;/suse-cloud-5/book_cloud_deploy/data/sec_depl_maintenance_backup_admin.html">Backing
       Up and Restoring the Administration Server</link> in the &suse; Cloud 5
       documentation for instructions.
      </para>
     </tip>
    </listitem>
   </itemizedlist>
  </sect2>
  <sect2 xml:id="sec.depl.maintenance.upgrade.procedure">
   <title>The Upgrade Procedure</title>
   <para>
    To start the upgrade procedure, proceed as follows:
   </para>

   <procedure>
    <title>Upgrade Part 1: Create the Upgrade Data</title>
    <step>
     <para>
      Open a browser and point it to the &crow; &wi;, for example
      http://192.168.124.10/. Log in as user crowbar. The password is
      <literal>crowbar</literal>, if you have not changed the default.
     </para>
    </step>
    <step>
     <para>
      Open <menuchoice> <guimenu>Utilities</guimenu> <guimenu>Upgrade to Cloud
      6</guimenu> </menuchoice>.
     </para>
    </step>
    <step>
     <para>
      Follow the instructions in the &wi; to create and save the upgrade
      data. Part 1 of the upgrade procedure is finished when you have saved
      the data.
     </para>
     <important>
      <title>Location for Saving the Upgrade Data</title>
      <para>
       Make sure to save the upgrade data to a location that can be accessed
       from the &admserv; after having re-installed it. Do not save it on the
       &admserv; itself, since it might get overwritten when re-installing the
       machine.
      </para>
     </important>
    </step>
   </procedure>

   <para>
    When the upgrade data has been saved, the &admserv; needs to be
    re-installed with &productname; 6 on &cloudos;:
   </para>

   <procedure>
    <title>Part 2: Re-Installing the &admserv;</title>
    <step>
     <para>
      Check the network configuration of the &admserv; with the command
      <command>ifconfig</command>. Note the MAC address and the IP address of
      the interface named <literal>eth0</literal>. Also note the IP addresses
      and ranges of all &cloud; networks. You can either find them in
      <filename>/etc/crowbar/network.json</filename> or when checking the
      <guimenu>Networks</guimenu> section in &yast; &crow; (see <xref
      linkend="sec.depl.adm_inst.crowbar.network"/> for details).
     </para>
     <warning>
      <title>No Parallel Setup</title>
      <para>
       It is <emphasis>not</emphasis> possible to set up a second machine,
       install &productname; 6 and then switch the old machine with the new
       one. The MAC address of the network interfaces need to be the same
       before and after the upgrade.
      </para>
     </warning>
    </step>
    <step>
     <para>
      Reboot the &admserv; from a &cloudos; installation source and install
      the operating system plus &productname; 6 as an add-on products. For
      details, see <xref linkend="cha.depl.adm_inst"/>.
     </para>
     <tip>
      <title>Deleting Unused Mirror Data</title>
      <para>
       &productname; &productnumber; does not use any of the repositories that
       were required for &productname; 5. In case you have mirrored
       repositories to the &admserv; and <filename>/srv</filename> resides on
       a separate partition, it is safe to format this partition to free space
       for the new repositories.
      </para>
     </tip>
    </step>
    <step>
     <para>
      Optional: If you have installed a local &smt; server, configure it as
      described in <xref linkend="app.deploy.smt.config"/>. Make
      sure the repositories are set up and mirrored as described in <xref
      linkend="app.deploy.smt.repos"/>.
     </para>
    </step>
    <step>
     <para>
      Make sure all required repositories are made available as described in
      <xref linkend="cha.depl.repo_conf"/>.
     </para>
    </step>
    <step>
     <para>
      Configure the network of the &admserv; as described in <xref
      linkend="sec.depl.adm_inst.network"/>. Make sure to use the exact same
      settings as in the previous installation.
     </para>
    </step>
    <step>
     <para>
      Configure &productname; with &yast; Crowbar as described in <xref
      linkend="sec.depl.adm_inst.crowbar"/>. Make sure to configure the exact
      same network settings for &crow; as in the previous installation.
     </para>
    </step>
    <step>
     <para>
      The &admserv; setup is finished as soon as you have finished the
      configuration with &yast; &crow;. Do <emphasis>not</emphasis> start the
      regular &inst_crow;!
     </para>
    </step>
   </procedure>

   <para>
    When the &admserv; has been set up and configured, return to the upgrade
    &wi; to upgrade all nodes in &cloud;:
   </para>

   <procedure>
    <title>Part 3: Upgrading the Nodes</title>
    <step>
     <para>
      Open a browser and point it to the &crow; Web interface available on the
      &admserv;, for example <literal>http://192.168.124.10/</literal>.
     </para>
     <figure>
      <title>The &productname; Installer</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_inst_ui_initial.png" width="75%" format="png"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_inst_ui_initial.png" width="75%" format="png"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Choose <guimenu>Continue Upgrade from SUSE OpenStack Cloud 5</guimenu>
      and start the upgrade process by uploading the upgrade data downloaded
      in Part 1 of the upgrade procedure. Follow the on-screen instruction to
      finish the upgrade process. Depending on the amount of nodes in your
      installation this will take up to several hours.
     </para>

     <note>
      <title>Login Credentials</title>
      <para>
       During the upgrade procedure you will be asked to provide login
       credentials for the &crow; &wi; two times. First time you need to
       provide the default login credentials
       (<literal>crowbar/crowbar</literal>. On the second occasion you need to
       specify the ones you used with Cloud 5. These credentials are also the
       ones you need to provide for subsequent logins to the &crow; &wi;.
      </para>
     </note>

    </step>
   </procedure>

   <para>
    When all nodes have been upgraded, the &barcl;s need to be
    re-applied:
   </para>

   <procedure>
    <title>Part 4: Re-Applying the &barcl;s</title>
    <step>
     <para>
      Go to the Dashboard on the &crow; &wi; <menuchoice>
      <guimenu>Nodes</guimenu> <guimenu>Dashboard</guimenu> </menuchoice> and
      check whether all nodes have been successfully updated&mdash;all nodes
      should be listed in state <guimenu>Ready</guimenu>, indicated by a green
      dot.
     </para>
    </step>
    <step>
     <para>
      If nodes have not been upgraded successfully, they are marked with a
      yellow or gray dot. Log in to those nodes (see <xref
      linkend="var.depl.trouble.faq.ostack.login"/>) and check the log files
      (see <xref linkend="cha.deploy.logs"/> for reasons. Fix the issues and
      reboot the node to restart the upgrade process. For more information
      also refer to <xref linkend="q.depl.trouble.faq.ostack.problem"/>.
     </para>
    </step>
    <step>
     <para>
      When all nodes have bee upgraded successfully re-apply the &barcl;s. Go
      to <menuchoice> <guimenu>Barclamps</guimenu> <guimenu>All
      Barclamps</guimenu> </menuchoice> and apply the &barcl;s in the given
      order. For each &barcl; the service configuration and the deployment configuration
      is the same as on &productname; 5, since it was
      restored from the data dump.
     </para>
    </step>
    <step>
     <para>
      When all &barcl; have been successfully deployed, you can restart the
      &vmguest;s on the &compnode;s.
     </para>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.maintenance.hasetup">
  <title>Upgrading to an &haSetup;</title>

  <para>
   When making an existing &cloud; deployment highly available (by setting
   up HA clusters and moving roles to these clusters), there are a few issues
   to pay attention to. To make existing services highly available, proceed
   as follows. Note that moving to an &hasetup; cannot be done without
   &cloud; service interruption, because it requires &ostack; services
   to be restarted.
  </para>

  <important>
   <title>Teaming Network Mode is Required for HA</title>
   <para>
    Teaming network mode is required for an &hasetup; of &productname;. If
    you are planning to move your cloud to an &hasetup; at a later point in
    time, make sure to deploy &cloud; with teaming network mode from the
    beginning. Otherwise a migration to an &hasetup; is not supported.
   </para>
  </important>

  <procedure>
   <step>
    <para>
     Make sure to have read the sections
     <xref linkend="sec.depl.arch.components.ha"/> and
     <xref linkend="sec.depl.req.ha"/> of this manual and taken any
     appropriate action.
    </para>
   </step>
   <step>
    <para>
     Make the HA repositories available on the &admserv; as described in
     <xref linkend="sec.depl.adm_conf.repos.scc"/>. Run the command
     <command>chef-client</command> afterwards.
    </para>
   </step>
   <step>
    <para>
     Set up your cluster(s) as described in
     <xref linkend="sec.depl.ostack.pacemaker"/>.
    </para>
   </step>
   <step>
    <para>
     To move a particular role from a regular control node to a cluster, you
     need to stop the associated service(s) before re-deploying the role on
     a cluster:
    </para>
    <substeps performance="required">
     <step>
      <para>
       Log in to each node on which the role is deployed and stop its
       associated service(s) (a role can have multiple services). Do so by
       running the service's start/stop script with the stop argument, for
       example:
      </para>
<screen>rcopenstack-keystone stop</screen>
      <para>
       See <xref linkend="app.deploy.services"/> for a list of roles,
       services and start/stop scripts.
      </para>
     </step>
     <step>
      <para>
       The following roles need additional treatment:
      </para>
      <variablelist>
       <varlistentry>
        <term>database-server (Database &barcl;)</term>
        <listitem>
         <orderedlist spacing="normal">
          <listitem>
           <para>
            Stop the database on the node the Database &barcl; is
            deployed with the command:
           </para>
<screen>rcpostgresql stop</screen>
          </listitem>
          <listitem>
           <para>
            Copy <filename>/var/lib/pgsql</filename> to a temporary location
            on the node, for example:
           </para>
<screen>cp -ax /var/lib/pgsql /tmp</screen>
          </listitem>
          <listitem>
           <para>
            Redeploy the Database &barcl; to the cluster. The original
            node may also be part of this cluster.
           </para>
          </listitem>
          <listitem>
           <para>
            Log in to a cluster node and run the following command to
            determine which cluster node runs the
            <systemitem class="resource">postgresql</systemitem> service:
           </para>
<screen>crm_mon -1</screen>
          </listitem>
          <listitem>
           <para>
            Log in to the cluster node running
            <systemitem class="resource">postgresql</systemitem>.
           </para>
          </listitem>
          <listitem>
           <para>
            Stop the <systemitem class="resource">postgresql</systemitem>
            service:
           </para>
<screen>crm resource stop postgresql</screen>
          </listitem>
          <listitem>
           <para>
            Copy the data backed up earlier to the cluster node:
           </para>
<screen>rsync -av --delete
           <replaceable>NODE_WITH_BACKUP</replaceable>:/tmp/pgsql/ /var/lib/pgsql/</screen>
          </listitem>
          <listitem>
           <para>
            Restart the <systemitem class="resource">postgresql</systemitem>
            service:
           </para>
<screen>crm resource start postgresql</screen>
          </listitem>
         </orderedlist>
         <para>
          Copy the content of <filename>/var/lib/pgsql/data/</filename> from
          the original database node to the cluster node with DRBD or shared
          storage.
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term>keystone-server (&o_ident; &barcl;)</term>
        <listitem>
         <para>
          If using &o_ident; with PKI tokens, the PKI keys on all nodes
          need to be re-generated. This can be achieved by removing the
          contents of <filename>/var/cache/*/keystone-signing/</filename> on
          the nodes. Use a command similar to the following on the
          &admserv; as &rootuser;:
         </para>
<screen>for NODE in <replaceable>NODE1</replaceable>
         <replaceable>NODE2</replaceable> <replaceable>NODE3</replaceable>; do
  ssh $NODE rm /var/cache/*/keystone-signing/*
done</screen>
        </listitem>
       </varlistentry>
      </variablelist>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Go to the &barcl; featuring the role you want to move to the
     cluster. From the left side of the <guimenu>Deployment</guimenu> section,
     remove the node the role is currently running on. Replace it with a
     cluster from the <guimenu>Available Clusters</guimenu> section. Then
     apply the proposal and verify that application succeeded via the
     &crow; &wi;. You can also check the cluster status via &hawk;
     or the <command>crm</command> /
     <command>crm_mon</command> CLI tools.
    </para>
   </step>
   <step>
    <para>
     Repeat these steps for all roles you want to move to cluster. See
     <xref linkend="sec.depl.reg.ha.control.spof"/> for a list of services
     with HA support.
    </para>
   </step>
  </procedure>

  <important>
   <title>SSL Certificates</title>
   <para>
    Moving to an &hasetup; also requires to create SSL certificates for
    nodes in the cluster that run services using SSL. Certificates need to
    be issued for the generated names (see
    <xref linkend="ann.depl.ostack.pacemaker.prop_name"/>) and for all
    public names you have configured in the cluster.
   </para>
  </important>

  <important>
   <title>Service Management on the Cluster</title>
   <para>
    After a role has been deployed on a cluster, its services are managed by
    the HA software. You must <emphasis>never</emphasis> manually start
    or stop an HA-managed service or configure it to start on boot. Services
    may only be started or stopped by using the cluster management tools Hawk
    or the crm shell. See
    <link xlink:href="&suse-onlinedoc;/sle-ha-12/book_sleha/data/sec_ha_config_basics_resources.html"/>
    for more information.
   </para>
  </important>
 </sect1>

 <sect1 xml:id="sec.depl.maintenance.backup.admin">
  <title>Backing Up and Restoring the &admserv;</title>
  <para>
   Backing Up and Restoring the &admserv; can either be done via the &crow;
   &wi; or on the &admserv;'s command line via the <command>crowbarctl
   backup</command> command. Both tools provide the same functionality.
  </para>

  <sect2 xml:id="sec.depl.maintenance.backup.admin.ui">
   <title>Backup and Restore via the &crow; &wi;</title>
   <para>
    To use the Web interface for backing up and restoring the &admserv;, go to
    the &crow; &wi; on the &admserv;, for example
    <literal>http://192.168.124.10/</literal>. Log in as user <systemitem
    class="username">crowbar</systemitem>. The password is
    <literal>crowbar</literal> by default, if you have not changed it. Go to <menuchoice> <guimenu>Utilities</guimenu> <guimenu>Backup &amp; Restore</guimenu>
    </menuchoice>.
   </para>
   <figure>
    <title>Backup and Restore: Initial Page View</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="depl_backup_initial.png" width="75%" format="png"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="depl_backup_initial.png" width="100%" format="png"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    To create a backup, click the respective button. Provide a descriptive
    name (allowed characters are letters, numbers, dashes and underscores) and
    confirm with <guimenu>Create Backup</guimenu>. Alternatively, you can
    upload a backup, for example from a previous installation.
   </para>
   <para>
    Existing backups are listed with name and creation date. For each backup,
    three actions are available:
   </para>
   <variablelist>
    <varlistentry>
     <term><guimenu>Download</guimenu></term>
     <listitem>
      <para>
       Download a copy of the backup file. The TAR archive you receive with
       this download can be uploaded again via <guimenu>Upload Backup
       Image</guimenu>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><guimenu>Restore</guimenu></term>
     <listitem>
      <para>
       Restore the backup.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><guimenu>Delete</guimenu></term>
     <listitem>
      <para>
       Delete the backup.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <figure>
    <title>Backup and Restore: List of Backups</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="depl_backup_list.png" width="75%" format="png"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="depl_backup_list.png" width="75%" format="png"/>
     </imageobject>
    </mediaobject>
   </figure>

  </sect2>

  <sect2 xml:id="sec.depl.maintenance.backup.admin.cli">
   <title>Backup and Restore via the Command Line</title>
   <para>
    Backing up and restoring the &admserv; from the command line can be done
    with the command <command>crowbarctl backup</command>. For getting general
    help, run the command <command>crowbarctl --help backup</command>, help on
    a subcommand is available by running <command>crowbarctl
    <replaceable>SUBCOMMAND</replaceable> --help</command>.
    The following commands for creating and managing backups exist:
   </para>
   <variablelist>
    <varlistentry>
     <term>
      <command>crowbarctl backup create
      <replaceable>NAME</replaceable></command>
     </term>
     <listitem>
      <para>
       Create a new backup named <replaceable>NAME</replaceable>. It will be
       stored at <filename>/var/lib/crowbar/backup</filename>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>crowbarctl backup [--yes] <replaceable>NAME</replaceable></command></term>
     <listitem>
      <para>
       Restore the backup named <replaceable>NAME</replaceable>. You will be
       asked for confirmation before any existing proposals will get
       overwritten. If using the option <option>--yes</option>, confirmations
       are tuned off and the restore is forced.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>crowbarctl backup delete <replaceable>NAME</replaceable></command></term>
     <listitem>
      <para>
       Delete the backup named <replaceable>NAME</replaceable>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>crowbarctl backup download <replaceable>NAME</replaceable>
     <replaceable>[FILE]</replaceable></command></term>
     <listitem>
      <para>
       Download the backup named <replaceable>NAME</replaceable>. If you
       specify the optional <replaceable>[FILE]</replaceable>, the download is
       written to the specified file. Otherwise it is saved to the current
       working directory with an automatically generated file name. If
       specifying <literal>-</literal> for <replaceable>[FILE]</replaceable>,
       the output is written to STDOUT.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>crowbarctl backup list</command></term>
     <listitem>
      <para>
       List existing backups. You can optionally specify different
       output formats and filters&mdash;refer to <command>crowbarctl backup
       list --help</command> for details.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>
      <command>crowbarctl backup upload
      <replaceable>FILE</replaceable></command>
     </term>
     <listitem>
      <para>
       Upload a backup from <replaceable>FILE</replaceable>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
</chapter>
