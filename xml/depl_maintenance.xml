<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet
 href="urn:x-daps:xslt:profiling:novdoc-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "IGNORE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.depl.maintenance">
 <title>&cloud; Maintenance</title>
 <para/>
 <sect1 id="sec.depl.maintenance.updates">
  <title>Keeping the Nodes Up-to-date</title>
  <para>
   Keeping the nodes in &productname; up-to-date requires an appropriate
   setup of the update repositories (see
   <xref
     linkend="sec.depl.inst.admserv.post.local_repos.update"/> and the
   deployment of either the <guimenu>Updater</guimenu> &barcl; (see
   <xref
     linkend="sec.depl.inst.nodes.post.updater"/>) or the &susemgr;
   &barcl; (see <xref linkend="sec.depl.inst.nodes.post.manager"/>.
  </para>
  <para>
   If one of those &barcl;s is deployed, patches are installed on the nodes.
   Installing patches that do not require a reboot of a node does not come
   with any service interruption. In case a patch (for example a kernel
   update) requires a reboot after the installation, services running on the
   machine that is rebooted, will not be available within &cloud;. Therefore
   it is strongly recommended, to install those patches during a maintenance
   window.
  </para>
  <note>
   <title>No Maintenance Mode</title>
   <para>
    As of &productname; &productnumber; it is not possible to put &cloud;
    into <quote>Maintenance Mode</quote>.
   </para>
  </note>
  <remark condition="clarity">
   2013-10-02 - fs: The following is mainly based on assumptions...
  </remark>
  <variablelist>
   <title>Consequences when Rebooting Nodes</title>
   <varlistentry>
    <term>&admserv;</term>
    <listitem>
     <para>
      While the &admserv; is offline, it is not possible to deploy new nodes.
      However, rebooting the &admserv; has no effect on starting &vmguest;s or
      on &vmguest;s already running.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&contrnode;s</term>
    <listitem>
     <para>
      The consequences a reboot of a &contrnode; has, depends on the services
      running on that node:
     </para>
     <formalpara>
      <title>Database, &o_ident;, RabbitMQ, &o_img;, &o_comp;:</title>
      <para>
       No new &vmguest;s can be started.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_objstore;:</title>
      <para>
       No object storage data is available. If &o_img; uses &o_objstore;, it
       will not be possible to start new &vmguest;s.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_blockstore;, &ceph;:</title>
      <para>
       No block storage data is available.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_netw;:</title>
      <para>
       No new &vmguest;s can be started. On running &vmguest;s the network
       will be unavailable.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_dash;</title>
      <para>
       &o_dash; will be unavailable. Starting and managing &vmguest;s can be
       done with the command line tools.
      </para>
     </formalpara>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&compnode;s</term>
    <listitem>
     <para>
      <remark condition="clarity">
       2013-10-02 - fs: How to ensure no new instances are started on a compute
       node while evacuating it? What about Windows compute nodes? What about
       VMware?
      </remark>
      Whenever a &compnode; is rebooted, all &vmguest;s running on that
      particular node will be shut down and must be manually restarted.
      Therefore it is recommended to <quote>evacuate</quote> the node by
      migrating &vmguest;s to another node, before rebooting it. <!-- (see FIXME
      for details) -->
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.depl.maintenance.upgrade">
  <title>Upgrading from &productname; 2.0 to &productname; 3</title>
  <para>
   Upgrading from &productname; 2.0 to &productname; 3 is supported, with the
   following manual steps. Please note, however, that it will turn off the
   &ostack; cloud during the upgrade and it is not possible to downgrade back
   to &productname; 2.0 in case of a regression or problem. It is highly
   recommended to backup data before the upgrade.
  </para>
  <itemizedlist>
   <title>Prerequisites</title>
   <listitem>
    <para>
     all allocated nodes are turned on
    </para>
   </listitem>
   <listitem>
    <para>
     all nodes, including the &admserv;, have the latest &productname; 2.0
     updates installed. If this is not the case, refer to <xref
     linkend="sec.depl.inst.nodes.post.updater"/> for instructions.
    </para>
   </listitem>
   <listitem>
    <para>
     all instances running in &cloud; must be suspended
    </para>
   </listitem>
  </itemizedlist>
  
  <important>
   <title>
    Instance Running on HyperV Nodes will not <quote>survive</quote> an
    Upgrade
   </title>
   <para>
    As of &productname; 3, HyperV Nodes need to be re-installed after the
    upgrade procedure. This re-installation will overwrite the instance's data
    and therefore they will be lost. &kvm;, VMWare and &xen; instances are not
    affected.
   </para>
  </important>

  <procedure>
   <title> Upgrading &productname; 2.0 to 3</title>
   <step>
    <para>
     Make sure to subscribe to the SUSE Cloud 3 Pool and Update channels on
     the source that provides these channels for you &cloud; installation (for
     example an &smt; or &susemgr; server). 
    </para>
    <para>
     In case the channels are provided by an &smt; server (either remote or on
     the &admserv; itself), refer to <xref
     linkend="sec.depl.inst.admserv.post.smt_repos"/> for instructions on
     subscribing to the &cloud; 3 channels.
    </para>
   </step>
   <step>
    <para>
     Update the &productname; product repository on the &admserv;. To do so,
     insert the &productname; 3 DVD and run the following commands:
    </para>
    <screen>mount /dev/dvd /mnt
rsync -avP --delete /mnt/ /srv/tftpboot/repos/Cloud/
umount /mnt</screen>
   </step>
   <step>
    <para>
    Update the &cloud; repository links on the &admserv;.
    </para>
    <para>
     If you are using external URLs (for example from an external &smt; or
     &susemgr; server) for the SUSE Cloud 2.0 Pool and Update repositories,
     you need to update the file
     <filename>/etc/crowbar/provisioner.json</filename>. Open it in an editor
     and Search for the <menuchoice> <guimenu>suse</guimenu>
     <guimenu>autoyast</guimenu> <guimenu>repos</guimenu></menuchoice>
     entry. Replace the URLs pointing to the &productname; 2.0 Pool and Update
     repositories with the ones pointing to version 3. Also change the name of
     these repositories to SUSE-Cloud-3-Pool and SUSE-Cloud-3-Updates,
     respectively.
    </para>
    <para>
     In case you are using an &smt; server installed on the &admserv;, run the
     following commands to make the repositories available for &cloud;:
    </para>
    <screen>for REPO in SUSE-Cloud-3-{Pool,Updates}; do
ln -s /srv/www/htdocs/repo/\$RCE/$REPO/sle-11-x86_64 /srv/tftpboot/repos/${REPO/.0/}
done</screen>
   </step>
   <step>
    <para>
     Now you need to replace the &productname; 2.0 Pool and Update
     repositories used by the &admserv; with the ones for version 3. Modify
     the repository configuration by running <menuchoice>
     <guimenu>&yast;</guimenu> <guimenu>Software</guimenu> <guimenu>Software
     Repositories</guimenu></menuchoice> on the &admserv;. 
    </para>
   </step>
   <step>
    <para>
     Refresh the repositories and install the <systemitem
     class="resource">suse-cloud-upgrade</systemitem> package by running the
     following commands on the &admserv;:
    </para>
    <screen>zypper refresh
zypper install suse-cloud-upgrade</screen>
   </step>
   <step>
    <para>
     Run <command>suse-cloud-upgrade <option>upgrade</option></command> for
     the first time. It will completely shut down the &ostack; and the &crow;
     Web interface. In case the command fails, please contact the &suse;
     support. At this point of the upgrade workflow it is still possible to go
     back to version 2.0.
    </para>
   </step>
   <step>
    <para>
     Update all packages on the &admserv; by running <command>zypper
     <option>up</option></command>. During the update process you will need to
     accept the &productname; 3 EULA. 
    </para>
   </step>
   <step>
    <para>
     Run <command>suse-cloud-upgrade <option>upgrade</option></command> for
     the second time. Now the &crow; Web interface is available again. In case
     the command fails, please contact the &suse; support. From this point in
     the installation workflow on it is no longer possible to go
     back to version 2.0.
    </para>
   </step>
   <step>
    <para>
     Go to the &crow; Web interface and open <menuchoice>
     <guimenu>Barclamps</guimenu> <guimenu>Crowbar</guimenu>
     </menuchoice>. <guimenu>Edit</guimenu> the <guimenu>Provisioner</guimenu>
     proposal and <guimenu>Apply</guimenu> it. 
    </para>
   </step>
   <step>
    <para>
     Update packages on all nodes except the &admserv; as described in <xref
     linkend="sec.depl.inst.nodes.post.updater"/>. You should use the
     following configuration:
    </para>
    <simplelist>
     <member><guimenu>Use zypper</guimenu>: update</member>
     <member><guimenu>Enable GPG checks</guimenu>: true</member>
     <member><guimenu>Automatically agree with licenses</guimenu>: true</member>
    </simplelist>
   </step>
   <step>
    <para>
     Reboot all nodes except the &admserv;. This can either be done manually
     or by running <command>suse-cloud-upgrade
     <option>reboot-nodes</option></command>. Wait until all nodes are up and
     running again before proceeding.
    </para>
   </step>
   <step>
    <para>
     Go to the &crow; Web interface and open <menuchoice>
     <guimenu>Barclamps</guimenu> <guimenu>Openstack</guimenu>
     </menuchoice>. Open the &barcl; one-by-one in the given order by clicking
     <guimenu>Edit</guimenu> and re-apply each proposal with
     <guimenu>Apply</guimenu>.
    </para>
   </step>
   <step>
    <para>
     In case you have deployed Microsoft HyperV nodes, you need to re-install
     them. Go to the &crow; Web interface and open the
     <guimenu>Dashboard</guimenu>. For each HyperV node, click on it's name
     and then on <guimenu>Reinstall</guimenu>.
    </para>
   </step>
  </procedure>
  <para>
   Once the upgrade procedure has been finished, &cloud; should be up and
   running again. You may restart all suspended instances now.
  </para>
 </sect1>
</chapter>
