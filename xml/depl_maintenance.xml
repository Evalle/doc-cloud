<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet
 href="urn:x-daps:xslt:profiling:novdoc-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "IGNORE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.depl.maintenance">
 <title>&cloud; Maintenance</title>
 <para/>
 <sect1 id="sec.depl.maintenance.updates">
  <title>Keeping the Nodes Up-to-date</title>
  <para>
   Keeping the nodes in &productname; up-to-date requires an appropriate
   setup of the update repositories (see
   <xref
     linkend="sec.depl.inst.admserv.post.local_repos.update"/> and the
   deployment of either the <guimenu>Updater</guimenu> &barcl; (see
   <xref
     linkend="sec.depl.inst.nodes.post.updater"/>) or the &susemgr;
   &barcl; (see <xref linkend="sec.depl.inst.nodes.post.manager"/>.
  </para>
  <para>
   If one of those &barcl;s is deployed, patches are installed on the nodes.
   Installing patches that do not require a reboot of a node does not come
   with any service interruption. In case a patch (for example a kernel
   update) requires a reboot after the installation, services running on the
   machine that is rebooted, will not be available within &cloud;. Therefore
   it is strongly recommended to install those patches during a maintenance
   window.
  </para>
  <note>
   <title>No Maintenance Mode</title>
   <para>
    As of &productname; &productnumber; it is not possible to put &cloud;
    into <quote>Maintenance Mode</quote>.
   </para>
  </note>
  <remark condition="clarity">
   2013-10-02 - fs: The following is mainly based on assumptions...
  </remark>
  <variablelist>
   <title>Consequences when Rebooting Nodes</title>
   <varlistentry>
    <term>&admserv;</term>
    <listitem>
     <para>
      While the &admserv; is offline, it is not possible to deploy new nodes.
      However, rebooting the &admserv; has no effect on starting &vmguest;s or
      on &vmguest;s already running.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&contrnode;s</term>
    <listitem>
     <para>
      The consequences a reboot of a &contrnode; has, depends on the services
      running on that node:
     </para>
     <formalpara>
      <title>Database, &o_ident;, RabbitMQ, &o_img;, &o_comp;:</title>
      <para>
       No new &vmguest;s can be started.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_objstore;:</title>
      <para>
       No object storage data is available. If &o_img; uses &o_objstore;, it
       will not be possible to start new &vmguest;s.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_blockstore;, &ceph;:</title>
      <para>
       No block storage data is available.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_netw;:</title>
      <para>
       No new &vmguest;s can be started. On running &vmguest;s the network
       will be unavailable.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_dash;</title>
      <para>
       &o_dash; will be unavailable. Starting and managing &vmguest;s can be
       done with the command line tools.
      </para>
     </formalpara>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&compnode;s</term>
    <listitem>
     <para>
      <remark condition="clarity">
       2013-10-02 - fs: How to ensure no new instances are started on a compute
       node while evacuating it? What about Windows compute nodes? What about
       VMware?
      </remark>
      Whenever a &compnode; is rebooted, all &vmguest;s running on that
      particular node will be shut down and must be manually restarted.
      Therefore it is recommended to <quote>evacuate</quote> the node by
      migrating &vmguest;s to another node, before rebooting it. <!-- (see FIXME
      for details) -->
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.depl.maintenance.upgrade">
  <title>Upgrading from &productname; 3 to &productname; 4</title>

  <para>
   Upgrading from &productname; 3 to &productname; 4 is supported, with the
   following manual steps. However, note that it will turn off the
   &ostack; cloud during the upgrade and it is not possible to downgrade back
   to &productname; 3 in case of a regression or problem. It is highly
   recommended to back up data before the upgrade. In case you also plan to
   make your setup highly available, also refer to <xref
   linkend="sec.depl.maintenance.hasetup"/>.
  </para>
  <itemizedlist>
   <title>Prerequisites</title>
   <listitem>
    <para>
     all allocated nodes are turned on
    </para>
   </listitem>
   <listitem>
    <para>
     all nodes, including the &admserv;, have the latest &productname; 3
     updates installed. If this is not the case, refer to <xref
     linkend="sec.depl.inst.nodes.post.updater"/> for instructions.
    </para>
   </listitem>
   <listitem>
    <para>
     all instances running in &cloud; must be suspended
    </para>
   </listitem>
  </itemizedlist>
  
  <important>
   <title>
    Instance Running on HyperV Nodes will not <quote>survive</quote> an
    Upgrade
   </title>
   <para>
    As of &productname; 4, HyperV Nodes need to be re-installed after the
    upgrade procedure. This re-installation will overwrite the instance's data
    and therefore they will be lost. &kvm;, VMWare and &xen; instances are not
    affected.
   </para>
  </important>

  <procedure>
   <title> Upgrading &productname; 3 to 4</title>
   <step>
    <para>
     Make sure to subscribe to the SUSE Cloud 3 Pool and Update channels on
     the source that provides these channels for your &cloud; installation
     (for example an &smt; or &susemgr; server).
    </para>
    <para>
     In case the channels are provided by an &smt; server (either remote or on
     the &admserv; itself), refer to <xref
     linkend="sec.depl.inst.admserv.post.smt_repos"/> for instructions on
     subscribing to the &cloud; 4 channels.
    </para>
   </step>
   <step>
    <para>
     Update the &productname; product repository on the &admserv;. To do so,
     insert the &productname; 3 DVD and run the following commands:
    </para>
    <screen>mount /dev/dvd /mnt
rsync -avP --delete /mnt/ /srv/tftpboot/repos/Cloud/
umount /mnt</screen>
   </step>
   <step>
    <para>
    Update the &cloud; repository links on the &admserv;.
    </para>
    <para>
     If you are using external URLs (for example from an external &smt; or
     &susemgr; server) for the SUSE Cloud 3 Pool and Update repositories,
     you need to update the file
     <filename>/etc/crowbar/provisioner.json</filename>. Open it in an editor
     and Search for the <menuchoice> <guimenu>suse</guimenu>
     <guimenu>autoyast</guimenu> <guimenu>repos</guimenu></menuchoice>
     entry. Replace the URLs pointing to the &productname; 3 Pool and Update
     repositories with the ones pointing to version 4. Also change the name of
     these repositories to SUSE-Cloud-4-Pool and SUSE-Cloud-4-Updates,
     respectively.
    </para>
    <para>
     In case you are using an &smt; server installed on the &admserv;, run the
     following commands to make the repositories available for &cloud;:
    </para>
    <screen>for REPO in SUSE-Cloud-4.0-{Pool,Updates}; do
ln -s /srv/www/htdocs/repo/\$RCE/$REPO/sle-11-x86_64 /srv/tftpboot/repos/${REPO/.0/}
done</screen>
   </step>
   <step>
    <para>
     Now you need to replace the &productname; 3 Pool and Update
     repositories used by the &admserv; with the ones for version 4. Modify
     the repository configuration by running <menuchoice>
     <guimenu>&yast;</guimenu> <guimenu>Software</guimenu> <guimenu>Software
     Repositories</guimenu></menuchoice> on the &admserv;. 
    </para>
   </step>
   <step>
    <para>
     Refresh the repositories and install the <systemitem
     class="resource">suse-cloud-upgrade</systemitem> package by running the
     following commands on the &admserv;:
    </para>
    <screen>zypper refresh
zypper install suse-cloud-upgrade</screen>
   </step>
   <step>
    <para>
     Run <command>suse-cloud-upgrade <option>upgrade</option></command> for
     the first time. It will completely shut down the &ostack; and the &crow;
     Web interface. In case the command fails, contact the &suse;
     support (see <xref linkend="sec.depl.trouble.support"/> for more
     information). At this point of the upgrade workflow it is still possible
     to go back to version 3.
    </para>
   </step>
   <step>
    <para>
     Update all packages on the &admserv; by running <command>zypper
     <option>up</option></command>. During the update process you will need to
     accept the &productname; 4 EULA. 
    </para>
   </step>
   <step>
    <para>
     Run <command>suse-cloud-upgrade <option>upgrade</option></command> for
     the second time. Now the &crow; Web interface is available again. In case
     the command fails, contact the &suse; support (see <xref
     linkend="sec.depl.trouble.support"/> for more information). From this
     point in the installation workflow on it is no longer possible to go back
     to version 3.
    </para>
   </step>
   <step>
    <para>
     Go to the &crow; Web interface and open <menuchoice>
     <guimenu>Barclamps</guimenu> <guimenu>Crowbar</guimenu>
     </menuchoice>. <guimenu>Edit</guimenu> the <guimenu>Provisioner</guimenu>
     proposal and <guimenu>Apply</guimenu> it. 
    </para>
   </step>
   <step>
    <para>
     Update packages on all nodes except the &admserv; as described in <xref
     linkend="sec.depl.inst.nodes.post.updater"/>. You should use the
     following configuration:
    </para>
    <simplelist>
     <member><guimenu>Use zypper</guimenu>: update</member>
     <member><guimenu>Enable GPG checks</guimenu>: true</member>
     <member><guimenu>Automatically agree with licenses</guimenu>: true</member>
    </simplelist>
   </step>
   <step>
    <para>
     Reboot all nodes except the &admserv;. This <emphasis>must</emphasis> be
     done by running <command>suse-cloud-upgrade reboot-nodes</command>. Wait
     until all nodes are up and running again before proceeding.
    </para>
   </step>
   <step>
    <para>
     Go to the &crow; Web interface and open <menuchoice>
     <guimenu>Barclamps</guimenu> <guimenu>Openstack</guimenu>
     </menuchoice>. Open the &barcl; one-by-one in the given order by clicking
     <guimenu>Edit</guimenu> and re-apply each proposal with
     <guimenu>Apply</guimenu>.
    </para>
   </step>
   <step>
    <para>
     In case you have deployed Microsoft HyperV nodes, you need to re-install
     them. Go to the &crow; Web interface and open the
     <guimenu>Dashboard</guimenu>. For each HyperV node, click its name
     and then on <guimenu>Reinstall</guimenu>.
    </para>
   </step>
  </procedure>
  <para>
   After the upgrade procedure has been finished, &cloud; should be up and
   running again. You may restart all suspended instances now.
  </para>
 </sect1>
 <sect1 id="sec.depl.maintenance.hasetup">
  <title>Upgrading to an &haSetup;</title>
  <para>
   When making an existing &cloud; deployment highly available by setting up
   HA clusters and moving roles to these clusters, there are a few issues
   to pay attention to. To make existing services highly available, proceed as
   follows. Note that moving to an &hasetup; cannot be done without
   &cloud; service interruption, because it requires &ostack;
   services to be restarted.
  </para>

  &note-ha-teaming;

  <procedure>
   <step>
    <para>
     Make sure to have read the sections <xref
     linkend="sec.depl.arch.components.ha"/> and <xref
     linkend="sec.depl.req.ha"/> of this manual and taken any appropriate
     action.
    </para>
   </step>
   <step>
    <para>
     Make the HA repositories available on the &admserv; as described in <xref
     linkend="sec.depl.inst.admserv.post"/>. Run the command
     <command>chef-client</command> afterwards.
    </para>
   </step>
   
   <step>
    <para>
     Set up your cluster(s) as described in <xref
     linkend="sec.depl.ostack.pacemaker"/>. 
    </para>
   </step>
   <step>
    <para>
     To move a particular role from a regular control node to a cluster, you
     need to stop the associated service(s) before re-deploying the role on a
     cluster:
    </para>
    <substeps>
     <step>
      <para>
       Log in to each node on which the role is deployed and stop its
       associated service(s) (a role can have multiple services). Do so by
       running the service's start/stop script with the stop argument, for
       example:  
      </para>
      <screen>rcopenstack-keystone stop</screen>
      <para>
       See <xref linkend="app.deploy.services"/> for a list of roles, services
       and start/stop scripts.
      </para>
     </step>
     <step>
      <para>
       The following roles need additional treatment: 
      </para>
      <variablelist>
       <varlistentry>
	<term>database-server (Database &barcl;)</term>
	<listitem>
	 <orderedlist>
	  <listitem>
	   <para>
	    Stop the database on the node the Database &barcl; is deployed with
	    the command:
	   </para>
	   <screen>rcpostgresql stop</screen>
	  </listitem>
	  <listitem>
	   <para>
	    Copy <filename>/var/lib/pgsql</filename> to a temporary location
	    on the node, for example:
	   </para>
	   <screen>cp -ax /var/lib/pgsql /tmp</screen>
	  </listitem>
	  <listitem>
	   <para>
	    Redeploy the Database &barcl; to the cluster. The original node
	    may also be part of this cluster.
	   </para>
	  </listitem>
	  <listitem>
	   <para>
	    Log in to a cluster node and run the following command to
	    determine which cluster node runs the <systemitem
	    class="resource">postgresql</systemitem> service:
	   </para>
	   <screen>crm_mon -1</screen>
	  </listitem>
	  <listitem>
	   <para>
	    Log in to the cluster node running <systemitem
	    class="resource">postgresql</systemitem>.
	   </para>
	  </listitem>
	  <listitem>
	   <para>
	    Stop the <systemitem class="resource">postgresql</systemitem>
	    service:
	   </para>
	   <screen>crm resource stop postgresql</screen>
	  </listitem>
	  <listitem>
	   <para>
	    Copy the data backed up earlier to the cluster node:
	   </para>
	   <screen>rsync -av --delete
	   <replaceable>NODE_WITH_BACKUP</replaceable>:/tmp/pgsql/ /var/lib/pgsql/</screen>
	  </listitem>
	  <listitem>
	   <para>
	    Restart the <systemitem class="resource">postgresql</systemitem>
	    service:
	   </para>
	   <screen>crm resource start postgresql</screen>
	  </listitem>
	 </orderedlist>
	 <para>
	  Copy the content of <filename>/var/lib/pgsql/data/</filename> from
	  the original database node to the cluster node with DRBD or shared
	  storage.
	 </para>
	</listitem>
       </varlistentry>
       <varlistentry>
	<term>keystone-server (&o_ident; &barcl;)</term>
	<listitem>
	 <para>
	  If using &o_ident; with PKI tokens, the PKI keys on all nodes need
	  to be re-generated. This can be achieved by removing the contents of
	  <filename>/var/cache/*/keystone-signing/</filename> on the
	  nodes. Use a command similar to the following on the &admserv; as
	  &rootuser;:
	 </para>
	 <screen>for NODE in <replaceable>NODE1</replaceable>
	 <replaceable>NODE2</replaceable> <replaceable>NODE3</replaceable>; do
  ssh $NODE rm /var/cache/*/keystone-signing/*
done</screen>
	</listitem>
       </varlistentry>
      </variablelist>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Go to the &barcl; featuring the role you want to move to the
     cluster. Remove the node the role is currently running on from the left
     side of the <guimenu>Deployment</guimenu> section and replace it with a
     cluster from the <guimenu>Available Clusters</guimenu> section. Then
     apply the proposal and verify that application succeeded via the Crowbar
     Web UI.  You can also check the cluster status via &hawk;, the &hbgui; (hb_gui), 
     or the <command>crm</command> / <command>crm_mon</command> CLI tools.
    </para>
   </step>
   <step>
    <para>
     Repeat these steps for all roles you want to move to cluster. See
     <xref linkend="sec.depl.reg.ha.control.spof"/> for a list of services
     with HA support.
    </para>
   </step>
  </procedure>
  
  <important>
   <title>SSL Certificates</title>
   <para>
    Moving to an &hasetup; also requires to create SSL certificates for nodes
    in the cluster that run services using SSL. Certificates need to be issued
    for the generated names (see <xref
    linkend="ann.depl.ostack.pacemaker.prop_name"/>) as well as for all public
    names you have configured in the cluster.
   </para>
  </important>
  
  &ha_service_management;
  
 </sect1>
 <sect1 id="sec.depl.maintenance.backup.admin">
  <title>Backing Up and Restoring the &admserv;</title>
  <para>As an example of how to back up and restore the data on the
   &admserv;, a <filename>crowbar-backup</filename> script is shipped with
   the maintenance update for &productname; 3. </para>
  <warning>
   <title>No Support for <filename>crowbar-backup</filename></title>
   <para>The script serves as a functioning <emphasis>example</emphasis> of how
    to back up and restore an &admserv;. Using it in a production
    environment is <emphasis>not</emphasis> supported. </para>
   <para>However, you can adjust the script to your specific setup or use it as
    a template to create your own backup and restore routine.</para>
   <para>Find the latest updates for the script at <ulink
     url="https://github.com/SUSE-Cloud/cloud-tools"/>.</para>
  </warning>
  <para>The script is part of the <systemitem class="resource"
    >crowbar</systemitem> package and is installed to
    <filename>/usr/sbin/crowbar-backup</filename> on the &admserv;. It is
   tailored to back up and restore the &admserv; of the respective
   &productname; version the script is shipped with. When calling the
   <command>crowbar-backup</command> script with the <command>backup</command>
   option, it will create a TAR archive with all configuration data that is
   needed to restore the &admserv;. For a list of available options and a
   short help text, run</para>
  <screen>./usr/sbin/crowbar-backup <option>help</option></screen>

  <variablelist>
   <title>Example Commands for Using the Script</title>
   <varlistentry>
    <term>Creating a Backup</term>
    <listitem>
     <screen>/usr/sbin/crowbar-backup&nbsp;<option>backup</option>&nbsp;[<replaceable>FILENAME</replaceable>]</screen>
     <para>Creates a TAR archive with all configuration data that is needed to
      restore the &admserv;. If you do not specify a
       <replaceable>FILENAME</replaceable>, the data is written to
       <filename>/tmp/backup-crowbar.tar.gz</filename> by default. Any services
      that need to be stopped for creating the backup will be stopped
      automatically and will be restarted afterwards by the script as
      needed.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Restoring a Backup From a TAR Archive</term>
    <listitem>
     <screen>/usr/sbin/crowbar-backup&nbsp;<option>restore</option>&nbsp;[<replaceable>FILENAME</replaceable>]</screen>
     <para>Reads the configuration data for an &admserv; from the specified
      TAR archive and restores the data on the current &admserv;.</para>
    </listitem>
   </varlistentry>
    <varlistentry>
    <term>Cleaning Up the &admserv;</term>
    <listitem>
     <screen>/usr/sbin/crowbar-backup&nbsp;<option>cleanup</option></screen>
     <para>Stops services and deletes the configuration files on the
      &admserv; that are related to &crow;.</para>
     <para>Use only after you have created a backup of the &admserv; and
      only if you want to test the restoration process.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Wiping the &admserv;</term>
    <listitem>
     <screen>/usr/sbin/crowbar-backup&nbsp;<option>purge</option></screen>
     <para>Uninstalls all packages on the &admserv; that are related to
      &crow;. Additionally, deletes all configuration files that are related
      to &crow;.</para>
     <para>Use only after you have created a backup of the &admserv; and
      only if you want to test the restoration process.</para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
</chapter>
