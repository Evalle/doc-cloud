<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet
 href="urn:x-daps:xslt:profiling:novdoc-profile.xsl" 
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "IGNORE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.depl.ostack">
 <title>Deploying the &ostack; Services</title>
 <para>
  Once the nodes are installed and configured you can start deploying the
  &ostack; services in order to finalize the installation. The services need
  to be deployed in a given order, because they depend on one another.
  Deployment is done from the &crow; Web interface through recipes,
  so-called <quote>&barcl;s</quote>.
 </para>
 <para>
  The services controlling the cloud (including storage management and
  control services) need to be installed on the &contrnode;. However, you
  may <emphasis>not</emphasis> use your &contrnode; as a compute or storage
  host. Here is a list with services that may <emphasis>not</emphasis> be
  installed on the &contrnode;: <guimenu>Swift-storage</guimenu>,
  <guimenu>&ceph;-store</guimenu>, <guimenu>Nova-multi-compute</guimenu>.
  These services need to be installed on dedicated nodes.
 </para>
 <para>
  The &ostack; services need to be deployed in the following order. For
  general instructions on how to edit and deploy &barcl;, refer to
  <xref
  linkend="sec.depl.ostack.barclamps"/>. Deploying &swift; and
  &ceph; is optional; all other services must be deployed.
 </para>
 <orderedlist>
  <listitem>
<!-- Database -->
   <para>
    <xref linkend="sec.depl.ostack.db" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Keystone -->
   <para>
    <xref linkend="sec.depl.ostack.keystone" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Swift -->
   <para>
    <xref linkend="sec.depl.ostack.swift" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Ceph -->
   <para>
    <xref linkend="sec.depl.ostack.ceph" xrefstyle="select:title nopage"/>
   </para>
   &no-ceph-support;
  </listitem>
  <listitem>
<!-- Glance -->
   <para>
    <xref linkend="sec.depl.ostack.glance" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!--
  <listitem>
   -->
<!-- Nova -->
<!--
   <para>
    <xref linkend="sec.depl.ostack.nova" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
   -->
<!--<guimenu>Nova Dashboard</guimenu>-->
<!--
   <para>
    <xref linkend="sec.depl.dash.dash" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  -->
 </orderedlist>
 <sect1 id="sec.depl.ostack.barclamps">
  <title>&barcl;</title>

  <para>
   The &ostack; services are automatically installed on the nodes by using
   so-called &barcl;s&mdash;a set of recipes, templates, and installation
   instructions. All existing &barcl;s can be accessed from the &crow; Web
   interface by clicking on <guimenu>Barclamps</guimenu>. To edit a &barcl;,
   proceed as follows:
  </para>

  <procedure>
   <step>
    <para>
     Open a browser and point it to the &crow; Web interface available at
     port <literal>3000</literal> of the &admserv;, for example
     <ulink
     url="http://192.168.124.10:3000/"/>. Log in as user
     <systemitem
     class="username">crowbar</systemitem>. The password
     defaults to <literal>crowbar</literal>, if you have not changed it.
    </para>
    <para>
     Click <guimenu>Barclamps</guimenu> to open the <guimenu>All
     Barclamps</guimenu> menu. Alternatively you may filter the list to
     <guimenu>Crowbar</guimenu> or <guimenu>&ostack;</guimenu> &barcl;s by
     choosing the respective option from <guimenu>Barclamps</guimenu>. The
     <guimenu>Crowbar</guimenu> &barcl;s contain general recipes for setting
     up and configuring all nodes, while the <guimenu>&ostack;</guimenu> are
     dedicated to &ostack; service deployment and configuration.
    </para>
   </step>
   <step>
    <para>
     Click a &barcl;'s name. You can either <guimenu>Create</guimenu> a
     proposal or <guimenu>Edit</guimenu> an existing one.
    </para>
    <para>
     Most &ostack; &barcl;s consist of two sections: the
     <guimenu>Attributes</guimenu> section lets you change the
     configuration, and the <guimenu>Node Deployment</guimenu> section lets
     you choose onto which nodes to deploy the &barcl;.
    </para>
   </step>
   <step>
    <para>
     To edit the <guimenu>Attributes</guimenu> section, change the values
     via the Web form. Alternatively you can directly edit the configuration
     file by clicking <guimenu>Raw</guimenu>.
    </para>
    <warning>
     <title>Raw Mode</title>
     <para>
      Only use the <guimenu>Raw</guimenu> mode in case an option cannot be
      changed via Web form. Raw mode does not perform any syntax checks.
     </para>
     <para>
      If you switch between <guimenu>Raw</guimenu> mode and Web form
      (<guimenu>Custom</guimenu> mode), make sure to <guimenu>Save</guimenu>
      your changes before switching, otherwise they will be lost.
     </para>
    </warning>
    <para>
     In the <guimenu>Node Deployment</guimenu> section of the &ostack;
     &barcl; you can drag and drop nodes from the <guimenu>Available
     Nodes</guimenu> column to the desired role. You need to drop the node
     onto the role name. Do <emphasis>not</emphasis> drop a node onto the
     input field&mdash;this is rather used to filter the list of
     <guimenu>Available Nodes</guimenu>!
    </para>
    <para>
     One or more nodes are usually automatically pre-selected for available
     roles. If this pre-selection does not meet your requirements, remove it
     <emphasis>before</emphasis> dragging new nodes to the role. To remove a
     node from a role, click the respective <guimenu>Remove</guimenu> icon.
    </para>
   </step>
   <step>
    <para>
     To save and deploy your edits, click <guimenu>Apply</guimenu>. To just
     save your changes without deploying them, click
     <guimenu>Save</guimenu>. To remove the complete proposal, click
     <guimenu>Delete</guimenu>. A proposal that already has been deployed
     can only be deleted manually, see
     <xref
     linkend="sec.depl.ostack.barclamps.delete"/> for details.
    </para>
    <para>
     If you deploy a proposal onto a node where a previous one is still
     active, the new proposal will overwrite the old one.
    </para>
    <note>
     <title>Wait Until a Proposal has been Deployed</title>
     <para>
      Deploying a proposal might take some time (up to several minutes). It
      is strongly recommended to always wait until you see the note
      <quote>Successfully applied the proposal</quote> before proceeding on
      to the next proposal.
     </para>
    </note>
   </step>
  </procedure>

  <warning>
   <title>&barcl; Deployment Failure</title>
   <para>
    In case the deployment of a &barcl; fails, make sure to fix the reason
    that has caused the failure and deploy the &barcl; again. Refer to the
    respective troubleshooting section at
    <xref
    linkend="sec.depl.trouble.faq.ostack"/> for help. A deployment
    failure may leave your node in an inconsistent state.
   </para>
  </warning>

  <sect2 id="sec.depl.ostack.barclamps.delete">
   <title>Deactivate a Proposal that Already has been Deployed</title>
   <para>
    To finally deactivate a proposal that already has been deployed, you
    first need to <guimenu>Deactivate</guimenu> it in the &crow; Web
    interface. Run the following commands as &rootuser; on the &admserv;
    afterwards:
   </para>
<screen>P_NAME="<replaceable>proposal_name_to_delete</replaceable>"
SERVICE="<replaceable>service_name</replaceable>"
crowbar $SERVICE proposal delete $P_NAME
crowbar $SERVICE delete $P_NAME</screen>
   <para>
    <replaceable>proposal_name_to_delete</replaceable> needs to be replaced
    by the name of the proposal you want to delete.
    <replaceable>service_name</replaceable> needs to be replaced by one of
    the following strings representing the &ostack; services:
    <literal>ceph</literal>, <literal>database</literal>,
    <literal>glance</literal>, <literal>keystone</literal>,
    <literal>nova_dashboard</literal>, <literal>nova</literal>,
    <literal>swift</literal>.
   </para>
  </sect2>
 </sect1>
 <sect1 id="sec.depl.ostack.db">
  <title>Deploying the Database</title>

  <para>
   The very first service that needs to be deployed is the
   <guimenu>Database</guimenu>. The database service using PostgreSQL is used
   by all other services. It must be installed on the &contrnode;.
  </para>

  <para>
   The only attribute you may change is the maximum number of database
   connections (<guimenu>Global Connection Limit </guimenu>). The default
   value should work in most cases.
  </para>
 </sect1>
 <sect1 id="sec.depl.ostack.keystone">
  <title>Deploying Keystone</title>

  <para>
   <guimenu>Keystone</guimenu> is another core component that is used by all
   other &ostack; services. It provides authentication and authorization
   services. <guimenu>Keystone</guimenu> needs to be installed on the
   &contrnode;. You can configure the following parameters of this &barcl;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Algorithm for Token Generation</guimenu>
    </term>
    <listitem>
     <para>
      Set the algorithm used by Keystone to generate the tokens. Defaults to
      PKI.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Default Tenant</guimenu>
    </term>
    <listitem>
     <para>
      Tenant for the users. Do not change the default value of
      <literal>openstack</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Regular User/Administrator Username/Password</term>
    <listitem>
     <para>
      Username and password for the regular user and the administrator. Both
      accounts can be used to log in to the &cloud; &dash; to manage
      Keystone users and access.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Administrator Token (long-lived)</guimenu>
    </term>
    <listitem>
     <para>
      The permanent administrator token (random string).
     </para>
    </listitem>
   </varlistentry>
<!--
   <varlistentry>
    <term><guimenu>Security Attributes</guimenu>
    </term>
    <listitem>
     <para>
      When sticking with the default value <literal>HTTP</literal>, public
      communication will not be encrypted. Choose <literal>HTTPS</literal>
      to use SSL for encryption and specify the path to the certificate
      files. Note that you need to create and copy the certificate prior to
      deploying Keystone, see <xref linkend="sec.depl.inst.nodes.post.ssl"/>
      for instructions.
     </para>
    </listitem>
   </varlistentry>
-->
  </variablelist>
 </sect1>
 <sect1 id="sec.depl.ostack.rabbit">
  <title>Deploying RabbitMQ</title>

  <para>
   The RabbitMQ messaging system enables the &contrnode; to communicate with
   the other nodes via Advanced Message Queue Protocol (AMQP). Deploying it is
   mandatory. RabbitMQ needs to be installed on the &contrnode;. It is
   recommended not to change the default values of the proposal's attributes.
  </para>
 </sect1>
 <sect1 id="sec.depl.ostack.swift">
  <title>Deploying &swift; (optional)</title>

  <para>
   &swift; adds an object storage service to &cloud; that lets you store
   single files such as images or snapshots. It offers high data security by
   storing the data redundantly on a pool of &stornode;s&mdash;therefore
   &swift; needs to be installed on at least two dedicated nodes.
  </para>

  <para>
   It is recommended not to change the defaults in the &barcl; proposal,
   unless you know exactly what you require. However you should change the
   <guimenu>Cluster Admin Password</guimenu>. If you plan to change the
   <guimenu>Zone</guimenu> value, it is important to know that you need at
   least as many &stornode;s as <guimenu>Zones</guimenu>.
  </para>

  <para>
   The &swift; service consists of three different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Swift-ring-compute</guimenu>
    </term>
    <listitem>
     <para>
      The ring maintains the information about the location of objects,
      replicas, and devices. It can be compared to an index, that is used by
      various &ostack; services to look up the physical location of objects.
      <guimenu>Swift-ring-compute</guimenu> must only be installed on a
      single node; it is recommended to use the &contrnode;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Swift-proxy</guimenu>
    </term>
    <listitem>
     <para>
      The Swift proxy server takes care of routing requests to Swift.
      Installing a single instance of <guimenu>Swift-proxy</guimenu> on
      the &contrnode; is recommended.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Swift-dispersion</guimenu>
    </term>
    <listitem>
     <para>
      The Swift dispersion tools are needed to measure the health of the
      cluster. Swift-dispersion needs to be installed on the same node as
      Swift-proxy, installing both on the &contrnode; is recommended.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Swift-storage</guimenu>
    </term>
    <listitem>
     <para>
      The virtual object storage service. Install this role on all dedicated
      &swift; &stornode;s (at least two), but not on any other node.
     </para>
     <warning>
      <title>Swift-storage Needs Dedicated Machines</title>
      <para>
       Never install the Swift-storage service on a node that runs other
       &ostack; services.
      </para>
     </warning>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.depl.ostack.ceph">
  <title>Deploying &ceph; (optional, unsupported)</title>

  <para>
   &ceph; adds a redundant block storage service to &cloud;. It lets you
   store persistent devices that can be mounted from &vmguest;s. It offers
   high data security by storing the data redundantly on a pool of
   &stornode;s&mdash;therefore &ceph; needs to be installed on at least two
   dedicated nodes.
  </para>

  <para>
   For more information on the &ceph; project, vist
   <ulink
   url="http://ceph.com/"/>.
  </para>
  &no-ceph-support;
  <para>
   The &ceph; &barcl; only has one configuration option: telling &ceph; which
   devices to use on the nodes. Enter a comma-separated list of devices that
   should be used by &ceph;. For example:
  </para>
  <screen>"dev/sdb, /dev/sdc, /dev/sdd</screen>

  <important>
   <title>Devices</title>
   <para>
    Not all of the devices used for &ceph; need to exist on all nodes. All
    devices from a node matching the list will be used. They must
    <emphasis>not</emphasis> be mounted prior to deploying &ceph;. Any data
    stored on these devices will be lost.
   </para>
  </important>

  <para>
   The &ceph; service consists of three different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Ceph-mon-master</guimenu>
    </term>
    <listitem>
     <para>
      Master cluster monitor daemon for the &ceph; distributed file system.
      <guimenu>Ceph-mon-master</guimenu> must only be installed on a single
      node; it is recommended to use the &contrnode;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Ceph-mon</guimenu>
    </term>
    <listitem>
     <para>
      Cluster monitor daemon for the &ceph; distributed file system.
      <guimenu>Ceph-mon</guimenu> needs to be installed on two or four
      &stornode;s.
     </para>
     <important>
      <title>Number of &ceph; Monitor Nodes</title>
      <para>
       In addition to the node running the &ceph;-mon-master service an
       additional two or four nodes also need to run the
       <guimenu>&ceph;-mon</guimenu> service. The sum of the
       <guimenu>&ceph;-mon-master</guimenu> and the
       <guimenu>&ceph;-mon</guimenu> nodes must always be an odd number
       (either three or five).
      </para>
      <para>
       Nodes running <guimenu>Ceph-mon</guimenu> cannot be deleted or
       temporarily be disabled.
      </para>
     </important>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Ceph-store</guimenu>
    </term>
    <listitem>
     <para>
      The virtual block storage service. Install this role on all dedicated
      &ceph; &stornode;s (at least two), but not on any other node.
     </para>
     <warning>
      <title><guimenu>Ceph</guimenu>-store Needs Dedicated Machines</title>
      <para>
       Never deploy <guimenu>Ceph-store</guimenu> on a node that runs other
       non-&ceph; &ostack; services. The only service that may be deployed
       together with it is <guimenu>Ceph-mon</guimenu>.
      </para>
     </warning>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   Deploying &ceph; requires to perform the steps in a given order:
  </para>

  <procedure>
   <step>
    <para>
     Edit the &barcl; proposal to specify the devices to be used by &ceph;
     as described above.
    </para>
   </step>
   <step>
    <para>
     Drag and drop a node (for example, the &contrnode;) to the
     <guimenu>&ceph;-mon-master</guimenu> role.
    </para>
   </step>
   <step>
    <para>
     Drag and drop two or four nodes to the <guimenu>&ceph;-mon</guimenu>
     role. Note that the maximum number of <guimenu>&ceph;-mon</guimenu>
     nodes cannot exceed four and that the sum of
     <guimenu>&ceph;-mon-master</guimenu> and <guimenu>&ceph;-mon</guimenu>
     nodes must be odd.
    </para>
   </step>
   <step>
    <para>
     Drag and drop all dedicated &ceph; &stornode;s to the
     <guimenu>&ceph;-store</guimenu> (at least two). You may also use the
     nodes with the <guimenu>&ceph;-mon</guimenu> roles, but not the
     <guimenu>&ceph;-mon-master</guimenu> node (you can add that one later).
    </para>
   </step>
   <step>
    <para>
     Click <guimenu>Apply</guimenu> to deploy your proposal. This can take
     some time.
    </para>
   </step>
   <step>
    <para>
     If you also want to use the <guimenu>&ceph;-mon-master</guimenu> as a
     &stornode;, drag and drop it to the <guimenu>&ceph;-store</guimenu>
     role and click <guimenu>Apply</guimenu> again. Note that it is not
     recommended to use the &contrnode; for non-management purposes such as
     storage or compute.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 id="sec.depl.ostack.glance">
  <title>Deploying Glance</title>

  <para>
   Glance provides discovery, registration, and delivery services for
   virtual disk images. An image is needed to start an &vmguest;&mdash;it is
   its pre-installed root-partition. All images you want to use in your
   cloud to boot &vmguest;s from, are provided by Glance.
  </para>

  <para>
   Glance should be deployed onto the &contrnode;. There are a lot of
   options to configure Glance. The most important ones are explained
   below&mdash;for a complete reference refer to
   <ulink
   url="http://github.com/crowbar/crowbar/wiki/Glance--barclamp"/>.
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Image Store Directory</guimenu>
    </term>
    <listitem>
     <para>
      Directory in which all images uploaded to Glance are stored. If you
      want to put the images onto a separate partition or volume, you need
      to mount this partition or volume on the &contrnode; prior to
      deploying the Glance proposal (see
      <xref
      linkend="sec.depl.inst.nodes.post.contrnode"/>). Specify
      the mount point of the partition or volume here.
     </para>
    </listitem>
   </varlistentry>
<!--
   <varlistentry>
    <term><guimenu>Security Attributes</guimenu>
    </term>
    <listitem>
     <para>
      When sticking with the default value <literal>HTTP</literal>, public
      communication will not be encrypted. Choose <literal>HTTPS</literal>
      to use SSL for encryption and specify the path to the certificate
      files. Note that you need to create and copy the certificate prior to
      deploying Glance, see <xref linkend="sec.depl.inst.nodes.post.ssl"/>
      for instructions.
     </para>
    </listitem>
   </varlistentry>
-->
   <varlistentry>
    <term>API/Registry <guimenu>Bind to All Addresses</guimenu>
    </term>
    <listitem>
     <para>
      Set these two options to <literal>true</literal> to enable users to
      upload images to Glance. If unset, only the operator will be able to
      upload images.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Caching</guimenu>
    </term>
    <listitem>
     <para>
      Enable and configure image caching in this section. By default, image
      caching is disabled. Learn more about Glance's caching feature at
      <ulink
      url="http://docs.openstack.org/developer/glance/cache.html"/>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

 <sect1 id="sec.depl.ostack.cinder">
  <title>Deploying Cinder</title>
  <para>
   Cinder, the successor of Nova Volume, provides volume block storage. It
   allows to add persistent storage to a &vmguest; that will persist until
   deleted (contrary to ephemeral volumes that will only persist until the
   &vmguest; is running). 
  </para>
  <para>
   Cinder can provide volume storage by using a local file, one or more local
   disks, or the Dell EqualLogic SAN (EQLX). The latter is only available on
   specific Dell hardware. Using a local file is not recommended for
   production systems for performance reasons.
  </para>

  <para>
   Cinder should be deployed onto the &contrnode;. The following attributes
   can be set to configure Cinder:
  </para>
  
  <variablelist>
   <varlistentry>
    <term><guimenu>Name of Volume</guimenu></term>
    <listitem>
     <para>
      <remark condition="clarity">
       2013-06-18 - fs: Does this name have any impact?  
      </remark> 
      FIXME
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Type of Volume</guimenu></term>
    <listitem>
     <para>
      Choose the volume type for Cinder: 
     </para>
     <itemizedlist>
      <listitem>
       <para>
	<guimenu>raw</guimenu>: local disk(s)
       </para>
      </listitem>
      <listitem>
       <para>
	<guimenu>local</guimenu>: local file
       </para>
      </listitem>
      <listitem>
       <para>
	<guimenu>eqlx</guimenu>: Dell EqualLogic SAN
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Disk-based Parameters</guimenu></term>
    <listitem>
     <para>
      This attribute can only be configured if <guimenu>Type of
      Volume</guimenu> was set to <guimenu>raw</guimenu>. Choose whether to
      only use the <guimenu>first</guimenu> available disk or
      <guimenu>all</guimenu> available disks.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>File-based Parameters</guimenu></term>
    <listitem>
     <para>
      This attribute can only be configured if <guimenu>Type of
      Volume</guimenu> was set to <guimenu>local</guimenu>. Choose the name
      and path for the file containing the volumes and set the
      <guimenu>Maximum File Size</guimenu>. Make sure not to overcommit the
      size, since it will result in data loss.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>EQLX Parameters</guimenu></term>
    <listitem>
     <para>
      This attribute can only be configured if <guimenu>Type of
      Volume</guimenu> was set to <guimenu>eqlx</guimenu>. Refer to your Dell
      EqualLogic SAN documentation for more information.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

 <sect1 id="sec.depl.ostack.quantum">
  <title>Deploying Quantum</title>
  <para>
   Quantum provides network connectivity between interface devices managed by
   other &ostack; services (most likely Nova). The service works by allowing
   users to create their own networks and then attach interfaces to them.
  </para>
  <para>
   Quantum should be deployed onto the &contrnode;. The following attributes
   can be set to configure Quantum:
  </para>

  <remark condition="clarity">
   2013-06-18 - fs: The following seems to be WIP, will be fixed after beta1
  </remark>
  
  <variablelist>
   <varlistentry>
    <term>Mode ??</term>
    <listitem>
     <para>
      FIXME
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Network Plugin ??</guimenu></term>
    <listitem>
     <para>
      Fixme
      <!-- attr networking_plugin linuxbridge|openvswitch: linuxbridge will be
      supported only with vlans -->
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

 <sect1 id="sec.depl.ostack.nova">
  <title>Deploying Nova</title>

  <para>
   Nova provides key services for managing the &cloud;, sets up the
   &compnode;s. The Nova service consists of two different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Nova-multi-controller</guimenu>
    </term>
    <listitem>
     <para>
      Distributing and scheduling the &vmguest;s is managed by the
      <guimenu>Nova-multi-controller</guimenu>. It also provides networking
      and messaging services. <guimenu>Nova-multi-controller</guimenu> needs
      to be installed on the &contrnode;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Nova-multi-compute</guimenu>
    </term>
    <listitem>
     <para>
      Provides the hypervisor (&kvm; or &xen;) and tools needed to manage
      the &vmguest;s. <guimenu>Nova-multi-compute</guimenu> needs to be
      installed on every &compnode;. The &compnode;s are the
      <quote>workhorses</quote> of the cloud&mdash;each &vmguest; is started
      on a &compnode;.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>The following attributes can be configured:</para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Verbose</guimenu></term>
    <listitem>
     <para>
      <remark condition="clarity">
       2013-06-18 - fs: Purpose? Implications?
      </remark>
      FIXME
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Hypervisor</guimenu>
    </term>
    <listitem>
     <para>
      Choose between the <guimenu>&kvm;</guimenu> and
      <guimenu>&xen;</guimenu> hypervisors (other available options are
      currently not supported by &suse;). As of &productname;
      &productnumber; choosing more than one hypervisor is not supported.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <guimenu>Virtual RAM to Physical RAM allocation ratio</guimenu>
    </term>
    <listitem>
     <para>
      Set the <quote>overcommit ratio</quote> for RAM for &vmguest;s on the
      &compnode;s. A ratio of <literal>1.0</literal> means no overcommitment.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <guimenu>Virtual CPU to Physical CPU allocation ratio</guimenu>
    </term>
    <listitem>
     <para>
      Set the <quote>overcommit ratio</quote> for CPUs for &vmguest;s on the
      &compnode;s. A ratio of <literal>1.0</literal> means no overcommitment.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.depl.ostack.dash">
  <title>Deploying the Nova Dashboard</title>

  <para>
   The last service that needs to be deployed is the Nova Dashboard. It
   provides a Web interface for users to start and stop &vmguest;s and for
   administrators to manage users, groups, roles, etc. Nova Dashboard should
   be installed on the &contrnode;. 
  </para>

 </sect1>
 <sect1 id="sec.depl.ostack.final">
  <title>How to Proceed</title>

  <para>
   With a successful deployment of the Nova Dashboard, the &productname;
   installation is finished. In order to be able to test your setup by
   starting an &vmguest; one last step remains to be done&mdash;uploading an
   image to the Glance service. Refer to <xref linkend="sec.adm.cli.img"/>
   for instructions. Images for &cloud; can be built in SUSE
   Studio&mdash;see this blog post for details:
   <ulink
   url="http://blog.susestudio.com/2012/10/kvm-build-format-suse-cloud-support.html"/>.
  </para>

  <para>
   Now you can hand over to the cloud administrator to set up users, roles,
   flavors, etc.&mdash; refer to the <xref linkend="book.cloud.admin"/> for
   details.
  </para>
 </sect1>
</chapter>
