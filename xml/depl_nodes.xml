<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet
 href="urn:x-daps:xslt:profiling:novdoc-profile.xsl" 
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "IGNORE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.depl.ostack">
 <title>Deploying the &ostack; Services</title>
 <para>
  Once the nodes are installed and configured you can start deploying the
  &ostack; services in order to finalize the installation. The services need
  to be deployed in a given order, because they depend on one another.
  Deployment is done from the &crow; Web interface through recipes,
  so-called <quote>&barcl;s</quote>.
 </para>
 <para>
  The services controlling the cloud (including storage management and control
  services) need to be installed on the &contrnode;(s). However, you may
  <emphasis>not</emphasis> use your &contrnode;(s) as a compute node or
  storage host for &o_objstore; or &ceph;. Here is a list with services that
  may <emphasis>not</emphasis> be installed on the &contrnode;(s):
  <guimenu>Swift-storage</guimenu>, <guimenu>&ceph;-store</guimenu>,
  <guimenu>Nova-multi-compute</guimenu>.  These services need to be installed
  on dedicated nodes.
 </para>
 <para>
  The &ostack; services need to be deployed in the following order. For
  general instructions on how to edit and deploy &barcl;, refer to
  <xref
  linkend="sec.depl.ostack.barclamps"/>. Deploying &o_objstore; and
  &ceph; is optional; all other services must be deployed.
 </para>
 <orderedlist>
  <listitem>
<!-- Database -->
   <para>
    <xref linkend="sec.depl.ostack.db" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Keystone -->
   <para>
    <xref linkend="sec.depl.ostack.keystone" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- RabbitMQ -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.rabbit" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Swift -->
   <para>
    <xref linkend="sec.depl.ostack.swift" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Ceph -->
   <para>
    <xref linkend="sec.depl.ostack.ceph" xrefstyle="select:title nopage"/>
   </para>
   &no-ceph-support;
  </listitem>
  <listitem>
<!-- Glance -->
   <para>
    <xref linkend="sec.depl.ostack.glance" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Cinder -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.cinder" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Neutrum -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.quantum" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Nova -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.nova" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Nova Dashboard -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.dash" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
 </orderedlist>
 <sect1 id="sec.depl.ostack.barclamps">
  <title>&barcl;s</title>

  <para>
   The &ostack; services are automatically installed on the nodes by using
   so-called &barcl;s&mdash;a set of recipes, templates, and installation
   instructions. All existing &barcl;s can be accessed from the &crow; Web
   interface by clicking on <guimenu>Barclamps</guimenu>. To edit a &barcl;,
   proceed as follows:
  </para>

  <procedure>
   <step>
    <para>
     Open a browser and point it to the &crow; Web interface available at
     port <literal>3000</literal> of the &admserv;, for example
     <ulink
     url="http://192.168.124.10:3000/"/>. Log in as user
     <systemitem
     class="username">crowbar</systemitem>. The password
     defaults to <literal>crowbar</literal>, if you have not changed it.
    </para>
    <para>
     Click <guimenu>Barclamps</guimenu> to open the <guimenu>All
     Barclamps</guimenu> menu. Alternatively you may filter the list to
     <guimenu>Crowbar</guimenu> or <guimenu>&ostack;</guimenu> &barcl;s by
     choosing the respective option from <guimenu>Barclamps</guimenu>. The
     <guimenu>Crowbar</guimenu> &barcl;s contain general recipes for setting
     up and configuring all nodes, while the <guimenu>&ostack;</guimenu> are
     dedicated to &ostack; service deployment and configuration.
    </para>
   </step>
   <step>
    <para>
     You can either <guimenu>Create</guimenu> a proposal or
     <guimenu>Edit</guimenu> an existing one.
    </para>
    <para>
     Most &ostack; &barcl;s consist of two sections: the
     <guimenu>Attributes</guimenu> section lets you change the
     configuration, and the <guimenu>Node Deployment</guimenu> section lets
     you choose onto which nodes to deploy the &barcl;.
    </para>
   </step>
   <step>
    <para>
     To edit the <guimenu>Attributes</guimenu> section, change the values
     via the Web form. Alternatively you can directly edit the configuration
     file by clicking <guimenu>Raw</guimenu>.
    </para>
    <warning>
     <title>Raw Mode</title>
     <para>
      Only use the <guimenu>Raw</guimenu> mode in case an option cannot be
      changed via Web form. Raw mode does not perform any syntax checks.
     </para>
     <para>
      If you switch between <guimenu>Raw</guimenu> mode and Web form
      (<guimenu>Custom</guimenu> mode), make sure to <guimenu>Save</guimenu>
      your changes before switching, otherwise they will be lost.
     </para>
    </warning>
    <para>
     In the <guimenu>Node Deployment</guimenu> section of the &ostack;
     &barcl; you can drag and drop nodes from the <guimenu>Available
     Nodes</guimenu> column to the desired role. You need to drop the node
     onto the role name. Do <emphasis>not</emphasis> drop a node onto the
     input field&mdash;this is rather used to filter the list of
     <guimenu>Available Nodes</guimenu>!
    </para>
    <para>
     One or more nodes are usually automatically pre-selected for available
     roles. If this pre-selection does not meet your requirements, remove it
     <emphasis>before</emphasis> dragging new nodes to the role. To remove a
     node from a role, click the respective <guimenu>Remove</guimenu> icon.
    </para>
   </step>
   <step>
    <para>
     To save and deploy your edits, click <guimenu>Apply</guimenu>. To just
     save your changes without deploying them, click
     <guimenu>Save</guimenu>. To remove the complete proposal, click
     <guimenu>Delete</guimenu>. A proposal that already has been deployed
     can only be deleted manually, see
     <xref
     linkend="sec.depl.ostack.barclamps.delete"/> for details.
    </para>
    <para>
     If you deploy a proposal onto a node where a previous one is still
     active, the new proposal will overwrite the old one.
    </para>
    <note>
     <title>Wait Until a Proposal has been Deployed</title>
     <para>
      Deploying a proposal might take some time (up to several minutes). It
      is strongly recommended to always wait until you see the note
      <quote>Successfully applied the proposal</quote> before proceeding on
      to the next proposal.
     </para>
    </note>
   </step>
  </procedure>

  <warning>
   <title>&barcl; Deployment Failure</title>
   <para>
    In case the deployment of a &barcl; fails, make sure to fix the reason
    that has caused the failure and deploy the &barcl; again. Refer to the
    respective troubleshooting section at
    <xref
    linkend="sec.depl.trouble.faq.ostack"/> for help. A deployment
    failure may leave your node in an inconsistent state.
   </para>
  </warning>

  <sect2 id="sec.depl.ostack.barclamps.delete">
   <title>Delete a Proposal that Already has been Deployed</title>
   <para>
    To delete a proposal that already has been deployed, you
    first need to <guimenu>Deactivate</guimenu> it in the &crow; Web
    interface. Deactivating a proposal will remove software and services
    having been deployed by this proposal from the affected nodes. After a
    proposal has been deactivated, you can <guimenu>Delete</guimenu> it in the
    &crow; Web interface and <guimenu>Create</guimenu> a new proposal from the
    &barcl; overview. 
   </para>
  </sect2>
 </sect1>
 <sect1 id="sec.depl.ostack.db">
  <title>Deploying the Database</title>

  <para>
   The very first service that needs to be deployed is the
   <guimenu>Database</guimenu>. The database service using PostgreSQL is used
   by all other services. It must be installed on a &contrnode;.
  </para>

  <para>
   The only attribute you may change is the maximum number of database
   connections (<guimenu>Global Connection Limit </guimenu>). The default
   value should work in most cases&mdash;only change it for large deployments
   in case the log files show database connection failures.
  </para>
 </sect1>
 <sect1 id="sec.depl.ostack.keystone">
  <title>Deploying &o_ident;</title>

  <para>
   <guimenu>&o_ident;</guimenu> is another core component that is used by all
   other &ostack; services. It provides authentication and authorization
   services. <guimenu>&o_ident;</guimenu> needs to be installed on a
   &contrnode;. You can configure the following parameters of this &barcl;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Algorithm for Token Generation</guimenu>
    </term>
    <listitem>
     <para>
      Set the algorithm used by &o_ident; to generate the tokens. It's
      strongly recommended to use <literal>PKI</literal>, since it will reduce
      network traffic.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Default Tenant</guimenu>
    </term>
    <listitem>
     <para>
      Tenant for the users. Do not change the default value of
      <literal>openstack</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Regular User/Administrator Username/Password</term>
    <listitem>
     <para>
      Username and password for the regular user and the administrator. Both
      accounts can be used to log in to the &cloud; &dash; to manage
      &o_ident; users and access.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry id="sec.depl.ostack.keystone.ssl">
    <term><guimenu>Protocoll</guimenu>
    </term>
    <listitem>
     <para>
      When sticking with the default value <guimenu>HTTP</guimenu>, public
      communication will not be encrypted . Choose <guimenu>HTTPS</guimenu>
      to use SSL for encryption. See <xref linkend="sec.depl.req.ssl"/> for
      background information and <xref
      linkend="sec.depl.inst.nodes.post.ssl"/> for installation instructions.
      The following additional configuration options will become available
      when choosing <guimenu>HTTPS</guimenu>:
     </para>
     <variablelist>
      <varlistentry>
       <term>
	<guimenu>SSL Certificate File</guimenu> / <guimenu>SSL (Private) Key
	File</guimenu>
       </term>
       <listitem>
	<para>
	 Location of the certificate key pair files.
	</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL Certificate is insecure</guimenu></term>
       <listitem>
	<para>
	 Check this option if using self-signed certificates in order to
	 disable certificate checks. You should only self-signed certificates
	 for non-production deployments. Always use properly signed
	 certificates in production environments. Never check this option in a
	 production deployment! 
	</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>
	<guimenu>Require Client Certificate</guimenu> / <guimenu>SSL CA
	Certificates File</guimenu>
       </term>
       <listitem>
	<para>
	 If your certificates are signed by a trusted third party
	 organization, <guimenu>Require Client Certificate</guimenu> should be
	 set to <guimenu>false</guimenu>, since the <quote>official</quote>
	 certification authorities (CA) are already known by the system. If
	 the certificates are signed by a CA within your organization or by
	 any other CA not known by the system, a CA certificate is needed.
	</para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.depl.ostack.rabbit">
  <title>Deploying RabbitMQ</title>

  <para>
   The RabbitMQ messaging system enables services to communicate with
   the other nodes via Advanced Message Queue Protocol (AMQP). Deploying it is
   mandatory. RabbitMQ needs to be installed on a &contrnode;. It is
   recommended not to change the default values of the proposal's attributes.
  </para>
  <variablelist>
   <varlistentry>
    <term><guimenu>Virtual Host</guimenu></term>
    <listitem>
     <para>
      Name of the default virtual host to be created and used by the RabbitMQ
      server (<literal>default_vhost</literal> config option in
      <filename>rabbitmq.config</filename>).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Port</term>
    <listitem>
     <para>
      Port the RabbitMQ server listens on (<literal>tcp_listeners</literal>
      config option in <filename>rabbitmq.config</filename>).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>User</term>
    <listitem>
     <para>
      RabbitMQ default user (<literal>default_user</literal> config option in
      <filename>rabbitmq.config</filename>).
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.depl.ostack.swift">
  <title>Deploying &o_objstore; (optional)</title>

  <para>
   &o_objstore; adds an object storage service to &cloud; that lets you store
   single files such as images or snapshots. It offers high data security by
   storing the data redundantly on a pool of &stornode;s&mdash;therefore
   &o_objstore; needs to be installed on at least two dedicated nodes.
  </para>

  <para>
   It is recommended not to change the defaults in the &barcl; proposal,
   unless you know exactly what you require. However you should change the
   <guimenu>Cluster Admin Password</guimenu>. If you plan to change the
   <guimenu>Zone</guimenu> value, it is important to know that you need at
   least as many &stornode;s as <guimenu>Zones</guimenu>.
  </para>

  <para>
   The &o_objstore; service consists of four different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Swift-ring-compute</guimenu>
    </term>
    <listitem>
     <para>
      The ring maintains the information about the location of objects,
      replicas, and devices. It can be compared to an index, that is used by
      various &ostack; services to look up the physical location of objects.
      <guimenu>Swift-ring-compute</guimenu> must only be installed on a
      single node; it is recommended to use a &contrnode;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Swift-proxy</guimenu>
    </term>
    <listitem>
     <para>
      The &o_objstore; proxy server takes care of routing requests to &o_objstore;.
      Installing a single instance of <guimenu>Swift-proxy</guimenu> on
      a &contrnode; is recommended.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Swift-dispersion</guimenu>
    </term>
    <listitem>
     <para>
      The &o_objstore; dispersion tools are needed to measure the health of the
      cluster. Swift-dispersion needs to be installed on the same node as
      Swift-proxy, installing both on a &contrnode; is recommended.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Swift-storage</guimenu>
    </term>
    <listitem>
     <para>
      The virtual object storage service. Install this role on all dedicated
      &o_objstore; &stornode;s (at least two), but not on any other node.
     </para>
     <warning>
      <title>Swift-storage Needs Dedicated Machines</title>
      <para>
       Never install the Swift-storage service on a node that runs other
       &ostack; services.
      </para>
     </warning>
    </listitem>
   </varlistentry>
  </variablelist>
  <para>
   In order to be able to properly configure &o_objstore; it's important to
   understand how it places the data. Data is always stored redundantly within
   the hierarchy. The &o_objstore; hierarchy in &cloud; is formed out of
   zones, nodes, hard drives and logical partitions. Zones are physically
   separated clusters, for example different server rooms each with it's own
   power supply and network segment. A failure of one zone must not affect
   another zone. The next level in the hierarchy are the individual
   &o_objstore; storage nodes (on which <guimenu>Swift-storage</guimenu> has
   been deployed) followed by the hard drives. Logical partitions come last.
  </para>
  <para>
   &o_objstore; automatically places three copies of each object on the
   highest hierarchy level possible. If three zones are available, the each
   copy of the object will be placed in a different zone. In a one zone setup
   with more than two nodes, the object copies will each be stored on a
   different node. In a one zone setup with two nodes, the copies will be
   distributed on different hard disks. If no other hierarchy element fits,
   logical partitions are used.
  </para>
  <para>
   The following attributes can be set to configure &o_objstore;:
  </para>
  <variablelist>
   <varlistentry>
    <term><guimenu>Allow Public Containers</guimenu></term>
    <listitem>
     <para>
      Allows to enable public access to containers id set to
      <literal>true</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Zones</guimenu></term>
    <listitem>
     <para>
      Number of zones (see above). If you do not have different independent
      installations of storage nodes, set the number of zones to
      <literal>1</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Partitions</term>
    <listitem>
     <para>
      Partition power. The number entered here is used to compute the number
      of logical partitions to be created in the cluster by using it as a
      power of 2 (2^X).
     </para>
     <para>
      It's recommended to use a minimum of 100 partitions per disk. To measure
      the partition power for your setup, do the following: Multiply the
      number of disks from all &o_objstore; nodes with 100 and then round up
      to the nearest power of two. Keep in mind that the first disk of each
      node is not used by &o_objstore;, but rather for the operating system.
     </para>
     <formalpara>
      <title>Example: 10 &o_objstore; nodes with 5 HDD each</title>
      <para>
       Four hard disks on each node are used for &o_objstore;, so there is a
       total of forty disks. Multiplied with 100 gives 4000. The nearest power
       of two, 4096, equals 2^12. So the partition power that needs to be
       entered is <literal>12</literal>.  
      </para>
     </formalpara>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Minimum Partitions per Hour</term>
    <listitem>
     <para>
      This option sets the number of hours before a logical partition is
      considered for relocation. <literal>24</literal> is the recommended
      value.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Replicas</guimenu></term>
    <listitem>
     <para>
      The number of copies generated for each object. Set this value to
      <literal>3</literal>, the tested and recommended value.
     </para>
    </listitem>
   </varlistentry>
<!--
   <varlistentry>
    <term>Cluster Hash</term>
    <listitem>
     <para>
      FIXME
     </para>
    </listitem>
   </varlistentry>
-->
   <varlistentry>
    <term>Cluster Admin Password</term>
    <listitem>
     <para>
      The &o_objstore; administrator password.
     </para>
    </listitem>
   </varlistentry>
<!--
   <varlistentry>
    <term>User</term>
    <listitem>
     <para>
      FIXME
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Group</term>
    <listitem>
     <para>
      FIXME
     </para>
    </listitem>
   </varlistentry>
-->
   <varlistentry>
    <term>Debug</term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <literal>true</literal>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <para>
   Apart from the general configuration described above, the &o_objstore;
   &barcl; lets you also activate and configure <guimenu>Additional
   Middlewares</guimenu>. The features these middlewares provide can be used
   via the &o_objstore; command line client only.
  </para>
  <variablelist>
   <varlistentry>
    <term><guimenu>S3</guimenu></term>
    <listitem>
     <para>
      Provides an S3 compatible API on top of &o_objstore;. 
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>StaticWeb</guimenu></term>
    <listitem>
     <para>
      Enables to serve container data as a static web site with an index file
      and optional file listings. See <ulink
      url="http://docs.openstack.org/developer/swift/misc.html#module-swift.common.middleware.staticweb"/>
      for details.
     </para>
     <para>
      This middleware requires to set <guimenu>Allow Public
      Containers</guimenu> to <literal>true</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>TempURL</guimenu></term>
    <listitem>
     <para>
      Enables to create URLs to provide time limited access to objects. See
      <ulink
	url="http://docs.openstack.org/developer/swift/misc.html#module-swift.common.middleware.tempurl"/>
      for details.
     </para>
    </listitem>
   </varlistentry>
<!-- Can be ignored according to saschpe
   <varlistentry>
    <term><guimenu>FormPOST</guimenu></term>
    <listitem>
     <para>
      Enables to upload files to a container via web form. See <ulink
      url="http://docs.openstack.org/developer/swift/misc.html#module-swift.common.middleware.formpost"/>
      for details.
     </para>
    </listitem>
   </varlistentry>
-->
   <varlistentry>
    <term><guimenu>Domain Remap</guimenu></term>
    <listitem>
     <para>
      Translates container and and account parts of a domain to path
      parameters that the &o_objstore; proxy server understands. Can be used
      to create short URLs that are easy to remember, for example by rewriting
      <literal>home.&exampleuser;.&exampledomain;/$ROOT/exampleuser;/home/myfile</literal>
      to <literal>home.&exampleuser;.&exampledomain;/myfile</literal>.  See
      <ulink
      url="http://docs.openstack.org/developer/swift/misc.html#module-swift.common.middleware.domain_remap"/>
      for details.
      <!-- <guimenu>Path root</guimenu> <guimenu>Storage Domain</guimenu> -->
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CNAME Lookup</term>
    <listitem>
     <para>
      CNAME Lookup translates an unknown domain in the host header to
      something that ends with the configured <guimenu>Storage
      Domain</guimenu> by looking up the given domain's CNAME record in
      DNS. See <ulink
      url="http://docs.openstack.org/developer/swift/misc.html#module-swift.common.middleware.cname_lookup"/>
      for details.
      <!-- <guimenu>Lookup depth</guimenu> <guimenu>Storage Domain</guimenu> -->
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Ratelimit</term>
    <listitem>
     <para>
      Ratelimit enables you to throttle resources such as requests per minute
      to provide denial of service protection. See <ulink
      url="http://docs.openstack.org/developer/swift/ratelimit.html"/> for
      details.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

 </sect1>
 <sect1 id="sec.depl.ostack.ceph">
  <title>Deploying &ceph; (optional, unsupported)</title>

  <para>
   &ceph; adds a redundant block storage service to &cloud;. It lets you
   store persistent devices that can be mounted from &vmguest;s. It offers
   high data security by storing the data redundantly on a pool of
   &stornode;s&mdash;therefore &ceph; needs to be installed on at least two
   dedicated nodes.
  </para>

  <para>
   For more information on the &ceph; project, vist
   <ulink
   url="http://ceph.com/"/>.
  </para>
  &no-ceph-support;
  <para>
   The &ceph; &barcl; only has one configuration option: telling &ceph; which
   devices to use on the nodes. Enter a comma-separated list of devices that
   should be used by &ceph;. For example:
  </para>
  <screen>"dev/sdb, /dev/sdc, /dev/sdd</screen>

  <important>
   <title>Devices</title>
   <para>
    Not all of the devices used for &ceph; need to exist on all nodes. All
    devices from a node matching the list will be used. They must
    <emphasis>not</emphasis> be mounted prior to deploying &ceph;. Any data
    stored on these devices will be lost.
   </para>
  </important>

  <para>
   The &ceph; service consists of three different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Ceph-mon-master</guimenu>
    </term>
    <listitem>
     <para>
      Master cluster monitor daemon for the &ceph; distributed file system.
      <guimenu>Ceph-mon-master</guimenu> must only be installed on a single
      node; it is recommended to use a &contrnode; (FIXME is this true?).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Ceph-mon</guimenu>
    </term>
    <listitem>
     <para>
      Cluster monitor daemon for the &ceph; distributed file system.
      <guimenu>Ceph-mon</guimenu> needs to be installed on two or four
      &stornode;s.
     </para>
     <important>
      <title>Number of &ceph; Monitor Nodes</title>
      <para>
       In addition to the node running the &ceph;-mon-master service an
       additional two or four nodes also need to run the
       <guimenu>&ceph;-mon</guimenu> service. The sum of the
       <guimenu>&ceph;-mon-master</guimenu> and the
       <guimenu>&ceph;-mon</guimenu> nodes must always be an odd number
       (either three or five).
      </para>
      <para>
       Nodes running <guimenu>Ceph-mon</guimenu> cannot be deleted or
       temporarily be disabled.
      </para>
     </important>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Ceph-store</guimenu>
    </term>
    <listitem>
     <para>
      The virtual block storage service. Install this role on all dedicated
      &ceph; &stornode;s (at least two), but not on any other node.
     </para>
     <warning>
      <title><guimenu>Ceph</guimenu>-store Needs Dedicated Machines</title>
      <para>
       Never deploy <guimenu>Ceph-store</guimenu> on a node that runs other
       non-&ceph; &ostack; services. The only service that may be deployed
       together with it is <guimenu>Ceph-mon</guimenu>.
      </para>
     </warning>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   Deploying &ceph; requires to perform the steps in a given order:
  </para>

  <procedure>
   <step>
    <para>
     Edit the &barcl; proposal to specify the devices to be used by &ceph;
     as described above.
    </para>
   </step>
   <step>
    <para>
     Drag and drop a node (for example, a &contrnode;) to the
     <guimenu>&ceph;-mon-master</guimenu> role.
    </para>
   </step>
   <step>
    <para>
     Drag and drop two or four nodes to the <guimenu>&ceph;-mon</guimenu>
     role. Note that the maximum number of <guimenu>&ceph;-mon</guimenu>
     nodes cannot exceed four and that the sum of
     <guimenu>&ceph;-mon-master</guimenu> and <guimenu>&ceph;-mon</guimenu>
     nodes must be odd.
    </para>
   </step>
   <step>
    <para>
     Drag and drop all dedicated &ceph; &stornode;s to the
     <guimenu>&ceph;-store</guimenu> (at least two). You may also use the
     nodes with the <guimenu>&ceph;-mon</guimenu> roles, but not the
     <guimenu>&ceph;-mon-master</guimenu> node (you can add that one later).
    </para>
   </step>
   <step>
    <para>
     Click <guimenu>Apply</guimenu> to deploy your proposal. This can take
     some time.
    </para>
   </step>
   <step>
    <para>
     If you also want to use the <guimenu>&ceph;-mon-master</guimenu> as a
     &stornode;, drag and drop it to the <guimenu>&ceph;-store</guimenu>
     role and click <guimenu>Apply</guimenu> again. Note that it is not
     recommended to use a &contrnode; for non-management purposes such as
     storage or compute.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 id="sec.depl.ostack.glance">
  <title>Deploying &o_img;</title>

  <para>
   &o_img; provides discovery, registration, and delivery services for
   virtual disk images. An image is needed to start an &vmguest;&mdash;it is
   its pre-installed root-partition. All images you want to use in your
   cloud to boot &vmguest;s from, are provided by &o_img;.
  </para>

  <para>
   &o_img; must be deployed onto a &contrnode;. There are a lot of
   options to configure &o_img;. The most important ones are explained
   below&mdash;for a complete reference refer to
   <ulink
   url="http://github.com/crowbar/crowbar/wiki/Glance--barclamp"/>.
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Notification Strategy</guimenu></term>
    <listitem>
     <para>
      FIXME: <!-- http://docs.openstack.org/developer/glance/notifications.html -->
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Backing Type</guimenu></term>
    <listitem>
     <para>
      Choose whether to use &o_objstore; to store the images or to store them
      in a image file on the &contrnode;. If you have deployed &o_objstore;
      anyway, it is recommended to use it for &o_img; as well.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Image Store Directory</guimenu>
    </term>
    <listitem>
     <para>
      This option is only available when having chosen <menuchoice>
      <guimenu>Backing Type</guimenu> <guimenu>File</guimenu> </menuchoice>.
      Specify the directory to host the image file. The directory specified
      here can also be an NFS share. See <xref
      linkend="sec.depl.inst.nodes.post.nfs"/> for more information.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Glance Swift Container</guimenu></term>
    <listitem>
     <para>
      This option is only available when having chosen <menuchoice>
      <guimenu>Backing Type</guimenu> <guimenu>Swift</guimenu> </menuchoice>.
      Sets the name of the container to use for the images in &o_objstore;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>API: Bind to All Addresses</guimenu></term>
    <listitem>
     <para>
      Set this option to <guimenu>true</guimenu> to enable users to
      upload images to &o_img;. If unset, only the operator will be able to
      upload images.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Verbose</guimenu></term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Caching</guimenu>
    </term>
    <listitem>
     <para>
      Enable and configure image caching in this section. By default, image
      caching is disabled. Learn more about &o_img;'s caching feature at
      <ulink url="http://docs.openstack.org/developer/glance/cache.html"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Database: SQL Idle Timeout</guimenu></term>
    <listitem>
     <para>
      Time after which idle database connections will be dropped.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Use Syslog</guimenu></term>
    <listitem>
     <para>
      Use syslog logging if set to <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Protocol</term>
    <listitem>
     <para>
      Enable encrypted communication for &o_img; by choosing
      <guimenu>HTTPS</guimenu>. Refer to <xref
      linkend="sec.depl.ostack.keystone.ssl"/> for configuration details.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

 <sect1 id="sec.depl.ostack.cinder">
  <title>Deploying &o_blockstore;</title>
  <para>
   &o_blockstore;, the successor of Nova Volume, provides volume block
   storage. It adds persistent storage to a &vmguest; that will
   persist until deleted (contrary to ephemeral volumes that will only persist
   until the &vmguest; is running).
  </para>
  <para>
   &o_blockstore; can provide volume storage by using a local file, one or
   more local disks, or the Dell EqualLogic SAN (EQLX). The latter is only
   available on specific Dell hardware. Using a local file is not recommended
   for production systems for performance reasons.
  </para>

  <para>
   The &o_blockstore; service consists of two different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>cinder-controller</guimenu></term>
    <listitem>
     <para>
      The &o_blockstore; controller provides the scheduler and the
      API. Installing <guimenu>cinder-controller</guimenu> on a &contrnode; is
      recommended.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>cinder-volume</guimenu></term>
    <listitem>
     <para>
      The virtual block storage service. It can be installed on a &contrnode;,
      but it's recommended to deploy it on a dedicated node supplied with
      sufficient networking capacity, since it will generate a lot of network
      traffic. <guimenu>cinder-volume</guimenu> can only be deployed on a
      single node.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   The following attributes can be set to configure &o_blockstore;:
  </para>
  
  <para>
   FIXME: Provide details about the different storage options - this also has
   impact on other parts of this document.
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Name of Volume</guimenu></term>
    <listitem>
     <para>
      FIXME
      <remark condition="clarity">
       2013-06-18 - fs: Does this name have any impact?  
      </remark> 
      FIXME
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Type of Volume</guimenu></term>
    <listitem>
     <para>
      Choose the volume type for &o_blockstore;: 
     </para>
     <itemizedlist>
      <listitem>
       <para>
	<guimenu>raw</guimenu>: local disk(s)
       </para>
      </listitem>
      <listitem>
       <para>
	<guimenu>local</guimenu>: local file
       </para>
      </listitem>
      <listitem>
       <para>
	<guimenu>eqlx</guimenu>: Dell EqualLogic SAN
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Disk-based Parameters</guimenu></term>
    <listitem>
     <para>
      This option is only available when having chosen <menuchoice>
      <guimenu>Type of Volume</guimenu> <guimenu>raw</guimenu>
      </menuchoice>. Choose whether to only use the <guimenu>first</guimenu>
      available disk or <guimenu>all</guimenu> available disks.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>File-based Parameters</guimenu></term>
    <listitem>
     <para>
      This option is only available when having chosen <menuchoice>
      <guimenu>Type of Volume</guimenu> <guimenu>local</guimenu>
      </menuchoice> Choose the name
      and path for the file containing the volumes and set the
      <guimenu>Maximum File Size</guimenu>. Make sure not to overcommit the
      size, since it will result in data loss.
     </para>
    </listitem>
   </varlistentry>
<!--
   <varlistentry>
    <term><guimenu>EQLX Parameters</guimenu></term>
    <listitem>
     <para>
      This attribute can only be configured if <guimenu>Type of
      Volume</guimenu> was set to <guimenu>eqlx</guimenu>. Refer to your Dell
      EqualLogic SAN documentation for more information.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Protocol</term>
    <listitem>
     <para>
      Enable encrypted communication for &o_blockstore; by choosing
      <guimenu>HTTPS</guimenu>. Refer to <xref
      linkend="sec.depl.ostack.keystone.ssl"/> for configuration details.
     </para>
    </listitem>
   </varlistentry>
--> 
 </variablelist>
 </sect1>

 <sect1 id="sec.depl.ostack.quantum">
  <title>Deploying &o_netw;</title>
  <para>
   &o_netw; provides network connectivity between interface devices managed by
   other &ostack; services (most likely &o_comp;). The service works by enabling
   users to create their own networks and then attach interfaces to them.
  </para>
  <para>
   &o_netw; must be deployed on a &contrnode;. The following attributes
   can be set to configure &o_netw;:
  </para>

  <remark condition="clarity">
   2013-06-18 - fs: The following seems to be WIP, will be fixed after beta1
  </remark>
  
  <variablelist>
   <varlistentry>
    <term><guimenu>Plugin</guimenu></term>
    <listitem>
     <para>
      Choose the plugin to be used with &o_netw;. The
      <guimenu>linuxbridge</guimenu> plugin only supports VLANs in
      &productname;, whereas the <guimenu>openvswitch</guimenu> plugin
      supports GRE and flat networks.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Mode</guimenu></term>
    <listitem>
     <para>
      This option is only available when having chosen <menuchoice>
      <guimenu>Plugin</guimenu> <guimenu>openvswitch</guimenu> </menuchoice>.
      Set the network type to be set up by the plugin: <guimenu>gre</guimenu>
      (Generic Routing Encapsulation) or <guimenu>flat</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>DHCP Domain</guimenu></term>
    <listitem>
     <para>
      Domain to use for building the host names.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Protocol</term>
    <listitem>
     <para>
      Enable encrypted communication for &o_netw; by choosing
      <guimenu>HTTPS</guimenu>. Refer to <xref
      linkend="sec.depl.ostack.keystone.ssl"/> for configuration details.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

 <sect1 id="sec.depl.ostack.nova">
  <title>Deploying &o_comp;</title>

  <para>
   &o_comp; provides key services for managing the &cloud;, sets up the
   &compnode;s. The &o_comp; service consists of four different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Nova-multi-controller</guimenu>
    </term>
    <listitem>
     <para>
      Distributing and scheduling the &vmguest;s is managed by the
      <guimenu>Nova-multi-controller</guimenu>. It also provides networking
      and messaging services. <guimenu>Nova-multi-controller</guimenu> needs
      to be installed on a &contrnode;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
    <guimenu>Nova-multi-compute-esxi</guimenu> /
    <guimenu>Nova-multi-compute-kvm</guimenu> /
    <guimenu>Nova-multi-compute-qemu</guimenu> /
    <guimenu>Nova-multi-compute-xen</guimenu>
    </term>
    <listitem>
     <para>
      Provides the hypervisors (HyperV, &kvm;, QEMU, VMWare ESX and &xen;) and
      tools needed to manage the &vmguest;s. Only one hypervisor can be
      deployed on a single compute node but you can use different hypervisors
      in your cloud by deploying different hypervisors to different
      &compnode;s. A <literal>Nova-multi-compute</literal> role needs to be
      installed on every &compnode;. However, not all hypervisors need to be
      deployed.
     </para>
     <para>
      Each image that will be made available in &cloud; to start a &vmguest;
      is bound to a hypervisor. Each hypervisor can be deployed on multiple
      &compnode;s (except for the VMWare ESX role, see below). In a
      multi-hypervisor deployment you should make sure to deploy the
      <literal>Nova-multi-compute</literal> roles in a way, that enough
      compute power is available for each hypervisor.
     </para>
     <note>
      <title>Re-assigning Hypervisors</title>
      <para>
       <remark condition="clarity">
	2013-08-05 - fs: Is this true?
       </remark>
       Existing <literal>Nova-multi-compute</literal> can be changed in a
       productive &cloud; without service interruption. You need to
       <quote>evacuate</quote> (see FIXME) the node, re-assign a new
       <literal>Nova-multi-compute</literal> role via the &o_comp; &barcl; and
       <guimenu>Apply</guimenu> the
       change. <guimenu>Nova-multi-compute-esxi</guimenu> can only be deployed
       on a single node.
      </para>
     </note>

     <important>
      <title>Deploying VMWare ESX (esxi)</title>
      <para>
       
       <remark condition="clarity">
	2013-08-05 - fs: What network requirements/adjustments are needed ??
       </remark>
       
       VMWare ESX is not supported <quote>natively</quote> by &cloud;&mdash;it
       rather delegates requests to an existing vCenter. It requires
       preparations at the vCenter and post install adjustments of the
       &compnode;. See <xref linkend="app.deploy.vmware"/> for instructions.
       <guimenu>Nova-multi-compute-esxi</guimenu> can only be deployed on a
       single &compnode;.
      </para>
     </important>
     
    </listitem>
   </varlistentry>
  </variablelist>

  <para>The following attributes can be configured:</para>
  <para>
   FIXME: VMWare attributes
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Verbose</guimenu></term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Shared Storage</term>
    <listitem>
     <para>
      FIXME (NFS volumes??)
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Enable Libvirt Migration</guimenu>
    </term>
    <listitem>
     <para>
      Allows to move &vmguest;s to a different &compnode;. Useful when a
      &compnode; needs to be shut down or rebooted for maintenance or when the
      load of the &compnode; is very high. &vmguest;s can  be moved while
      running (Live Migration).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <guimenu>Virtual RAM to Physical RAM allocation ratio</guimenu>
    </term>
    <listitem>
     <para>
      Set the <quote>overcommit ratio</quote> for RAM for &vmguest;s on the
      &compnode;s. A ratio of <literal>1.0</literal> means no
      overcommitment. Changing this value is not recommended.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <guimenu>Virtual CPU to Physical CPU allocation ratio</guimenu>
    </term>
    <listitem>
     <para>
      Set the <quote>overcommit ratio</quote> for CPUs for &vmguest;s on the
      &compnode;s. A ratio of <literal>1.0</literal> means no overcommitment.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>KVM Options: Enable Kernel Samepage Merging</guimenu></term>
    <listitem>
     <para>
      Kernel SamePage Merging (KSM) is a Linux Kernel feature which merges
      identical memory pages from multiple running processes into one memory
      region. Enabling it optimizes memory usage on the &compnode;s when using
      the &kvm; hypervisor at the cost of slightly increasing CPU usage.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSL Support: Protocol</term>
    <listitem>
     <para>
      Enable encrypted communication for &o_comp; by choosing
      <guimenu>HTTPS</guimenu>. Refer to <xref
      linkend="sec.depl.ostack.keystone.ssl"/> for configuration details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSL Support for noVNC: Protocol</term>
    <listitem>
     <para>
      After having started an instance you can display its VNC console in the
      Nova &dash; via the browser using the noVNC implementation. By default
      this connection is not encrypted and can potentially be eavesdropped.
     </para>
     <para>
      Enable encrypted communication for noVNC by choosing
      <guimenu>HTTPS</guimenu>. Refer to <xref
      linkend="sec.depl.ostack.keystone.ssl"/> for configuration details.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.depl.ostack.dash">
  <title>Deploying the Nova &dash;</title>

  <para>
   The last service that needs to be deployed is the Nova &dash;. It
   provides a Web interface for users to start and stop &vmguest;s and for
   administrators to manage users, groups, roles, etc. Nova &dash; should
   be installed on a &contrnode;. 
  </para>

  <para>The following attributes can be configured:</para>

  <variablelist>
   <varlistentry>
    <term>SSL Support: Protocol</term>
    <listitem>
     <para>
      Enable encrypted communication for Nova &dash; by choosing
      <guimenu>HTTPS</guimenu>. Refer to <xref
      linkend="sec.depl.ostack.keystone.ssl"/> for configuration details.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.depl.ostack.final">
  <title>How to Proceed</title>

  <para>
   With a successful deployment of the Nova &dash;, the &productname;
   installation is finished. In order to be able to test your setup by
   starting an &vmguest; one last step remains to be done&mdash;uploading an
   image to the &o_img; service. Refer to <xref linkend="sec.adm.cli.img"/>
   for instructions. Images for &cloud; can be built in SUSE
   Studio&mdash;see this blog post for details:
   <ulink
   url="http://blog.susestudio.com/2012/10/kvm-build-format-suse-cloud-support.html"/>.
  </para>
  <para>
   Now you can hand over to the cloud administrator to set up users, roles,
   flavors, etc.&mdash; refer to the <xref linkend="book.cloud.admin"/> for
   details. The default credentials for the Nova &dash; are username
   <literal>admin</literal> and password <literal>crowbar</literal>.
  </para>
 </sect1>
</chapter>
