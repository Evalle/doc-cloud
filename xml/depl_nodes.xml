<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet
 href="urn:x-daps:xslt:profiling:novdoc-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "IGNORE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.depl.ostack">
 <title>Deploying the &ostack; Services</title>
 <para>
  Once the nodes are installed and configured you can start deploying the
  &ostack; services in order to finalize the installation. The services need
  to be deployed in a given order, because they depend on one another. The
  <guimenu>Pacemaker</guimenu> service for an &hasetup; is the only exception
  from this rule&mdash;it can be setup at any time. However, when deploying
  &productname; from scratch, it is recommended to deploy the
  <guimenu>Pacemaker</guimenu> proposal(s) first. Deployment for all services
  is done from the &crow; Web interface through recipes, so-called
  <quote>&barcl;s</quote>.
 </para>
 <para>
  The services controlling the cloud (including storage management and control
  services) need to be installed on the &contrnode;(s) (refer to <xref
  linkend="sec.depl.arch.components.control"/> for more information). However,
  you may <emphasis>not</emphasis> use your &contrnode;(s) as a compute node
  or storage host for &o_objstore; or &ceph;. Here is a list with services
  that may <emphasis>not</emphasis> be installed on the &contrnode;(s):
  <guimenu>swift-storage</guimenu>, <guimenu>&ceph;-store</guimenu>,
  <guimenu>Nova-multi-compute</guimenu>.  These services need to be installed
  on dedicated nodes.
 </para>
 <para>
  When deploying an &hasetup; the controller nodes are replaced by one or more
  controller clusters consisting of at least two nodes (three are
  recommended). Setting up three separate clusters&mdash;for data, services,
  and networking&mdash;is recommended. See <xref linkend=" sec.depl.req.ha"/>
  for more information on requirements and recommendations for an &hasetup;.
 </para>
 <para>
  The &ostack; services need to be deployed in the following order. For
  general instructions on how to edit and deploy &barcl;, refer to <xref
  linkend="sec.depl.ostack.barclamps"/>. Deploying Pacemaker (only needed for
  an &hasetup;), &o_objstore; and &ceph; is optional; all other services must
  be deployed.
 </para>
 <orderedlist>
  <listitem>
<!-- Pacemaker -->
   <para>
    <xref linkend="sec.depl.ostack.pacemaker" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Database -->
   <para>
    <xref linkend="sec.depl.ostack.db" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Keystone -->
   <para>
    <xref linkend="sec.depl.ostack.keystone" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- RabbitMQ -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.rabbit" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Ceph -->
   <para>
    <xref linkend="sec.depl.ostack.ceph" xrefstyle="select:title nopage"/>
   </para>
   &no-ceph-support;
  </listitem>
  <listitem>
<!-- Swift -->
   <para>
    <xref linkend="sec.depl.ostack.swift" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Glance -->
   <para>
    <xref linkend="sec.depl.ostack.glance" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Cinder -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.cinder" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Neutrum -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.quantum" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Nova -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.nova" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Nova Dashboard -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.dash" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
 </orderedlist>
 <sect1 id="sec.depl.ostack.barclamps">
  <title>&Barcl;s</title>

  <para>
   The &ostack; services are automatically installed on the nodes by using
   so-called &barcl;s&mdash;a set of recipes, templates, and installation
   instructions. All existing &barcl;s can be accessed from the &crow; Web
   interface by clicking on <guimenu>Barclamps</guimenu>. To edit a &barcl;,
   proceed as follows:
  </para>

  <procedure>
   <step>
    <para>
     Open a browser and point it to the &crow; Web interface available at
     port <literal>3000</literal> of the &admserv;, for example
     <literal>http://192.168.124.10:3000/</literal>. Log in as user
     <systemitem
     class="username">crowbar</systemitem>. The password
     defaults to <literal>crowbar</literal>, if you have not changed it.
    </para>
    <para>
     Click <guimenu>Barclamps</guimenu> to open the <guimenu>All
     Barclamps</guimenu> menu. Alternatively you may filter the list to
     <guimenu>Crowbar</guimenu> or <guimenu>OpenStack</guimenu> &barcl;s by
     choosing the respective option from <guimenu>Barclamps</guimenu>. The
     <guimenu>Crowbar</guimenu> &barcl;s contain general recipes for setting
     up and configuring all nodes, while the <guimenu>OpenStack</guimenu> are
     dedicated to &ostack; service deployment and configuration.
    </para>
   </step>
   <step>
    <para>
     You can either <guimenu>Create</guimenu> a proposal or
     <guimenu>Edit</guimenu> an existing one.
    </para>
    <para>
     Most &ostack; &barcl;s consist of two sections: the
     <guimenu>Attributes</guimenu> section lets you change the
     configuration, and the <guimenu>Node Deployment</guimenu> section lets
     you choose onto which nodes to deploy the &barcl;.
    </para>
   </step>
   <step>
    <para>
     To edit the <guimenu>Attributes</guimenu> section, change the values
     via the Web form. Alternatively you can directly edit the configuration
     file by clicking <guimenu>Raw</guimenu>.
    </para>
    <warning>
     <title>Raw Mode</title>
     <para>
      If you switch between <guimenu>Raw</guimenu> mode and Web form
      (<guimenu>Custom</guimenu> mode), make sure to <guimenu>Save</guimenu>
      your changes before switching, otherwise they will be lost.
     </para>
    </warning>
    <para>
     In the <guimenu>Node Deployment</guimenu> section of the &ostack;
     &barcl; you can drag and drop nodes from the <guimenu>Available
     Nodes</guimenu> column to the desired role. You need to drop the node
     onto the role name. Do <emphasis>not</emphasis> drop a node onto the
     input field&mdash;this is rather used to filter the list of
     <guimenu>Available Nodes</guimenu>!
    </para>
    <para>
     One or more nodes are usually automatically pre-selected for available
     roles. If this pre-selection does not meet your requirements, remove it
     <emphasis>before</emphasis> dragging new nodes to the role. To remove a
     node from a role, click the respective <guimenu>Remove</guimenu> icon.
    </para>
   </step>
   <step>
    <para>
     To save and deploy your edits, click <guimenu>Apply</guimenu>. To just
     save your changes without deploying them, click
     <guimenu>Save</guimenu>. To remove the complete proposal, click
     <guimenu>Delete</guimenu>. A proposal that already has been deployed
     can only be deleted manually, see
     <xref
     linkend="sec.depl.ostack.barclamps.delete"/> for details.
    </para>
    <para>
     If you deploy a proposal onto a node where a previous one is still
     active, the new proposal will overwrite the old one.
    </para>
    <note>
     <title>Wait Until a Proposal has been Deployed</title>
     <para>
      Deploying a proposal might take some time (up to several minutes). It
      is strongly recommended to always wait until you see the note
      <quote>Successfully applied the proposal</quote> before proceeding on
      to the next proposal.
     </para>
    </note>
   </step>
  </procedure>

  <warning>
   <title>&barcl; Deployment Failure</title>
   <para>
    In case the deployment of a &barcl; fails, make sure to fix the reason
    that has caused the failure and deploy the &barcl; again. Refer to the
    respective troubleshooting section at
    <xref
    linkend="sec.depl.trouble.faq.ostack"/> for help. A deployment
    failure may leave your node in an inconsistent state.
   </para>
  </warning>

  <sect2 id="sec.depl.ostack.barclamps.delete">
   <title>Delete a Proposal that Already has been Deployed</title>
   <para>
    To delete a proposal that already has been deployed, you
    first need to <guimenu>Deactivate</guimenu> it in the &crow; Web
    interface. Deactivating a proposal will remove software and services
    having been deployed by this proposal from the affected nodes. After a
    proposal has been deactivated, you can <guimenu>Delete</guimenu> it in the
    &crow; Web interface and <guimenu>Create</guimenu> a new proposal from the
    &barcl; overview.
   </para>
  </sect2>
 </sect1>


 <sect1 id="sec.depl.ostack.pacemaker">
  <title>Deploying Pacemaker (Optional, &hasetup; Only)</title>
  <para>
   By setting up one or more clusters by deploying Pacemaker, you can make the
   &cloud; control controller functions highly available (see <xref
   linkend="sec.depl.req.ha"/> for details). Since it is possible (and
   recommended) to deploy more than one cluster, a proposal needs to be
   created for each cluster.
  </para>
  <para>
   Deploying Pacemaker is optional. In case you do not want to deploy it, skip
   this section and start the node deployment by deploying the database as
   described in <xref linkend="sec.depl.ostack.db"/>.
  </para>

  <note>
   <title>Number of Cluster Nodes</title>
   <para>
    To set up a cluster, at least two nodes are required. If setting up a
    cluster with replicated storage via DRBD (for example for a cluster for
    the database and RabbitMQ), exactly two nodes are required. For all other
    setups an odd number of nodes with a minimum of three nodes is strongly
    recommended. See <xref linkend="sec.depl.reg.ha.general"/> for more
    information.
   </para>
  </note>
  
  <para>
   To create a proposal, go to <menuchoice> <guimenu>Barclamps</guimenu>
   <guimenu>OpenStack</guimenu> </menuchoice> and click
   <guimenu>Edit</guimenu> for the Pacemaker &barcl;. A drop-down box where
   you can enter a name and a description for the proposal opens. Click
   <guimenu>Create</guimenu> to open the configuration screen for the
   proposal.
  </para>
  <informalfigure>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_pacemaker_proposal.png" width="75%"
		format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_pacemaker_proposal.png" width="75%"
		format="png"/>
    </imageobject>
   </mediaobject>
  </informalfigure>
  
  <important id="ann.depl.ostack.pacemaker.prop_name">
   <title>Proposal Name</title>
   <para>
    The name you enter for the proposal will be used to generate host names for
    the virtual IPs of HAProxy. The name uses the following scheme:
   </para>
   <screen><replaceable>NAME</replaceable>.cluster-<replaceable>PROPOSAL NAME</replaceable>.<replaceable>FQDN</replaceable></screen>
   <para>
    When the proposal name is set to <literal>data</literal>, this results
    in, for example, <literal>controller.cluster-data.&exampledomain;</literal>.
   </para>
  </important>
  
  <para>
   The following options are configurable in the Pacemaker configuration
   screen:
  </para>
  <variablelist>
   <varlistentry>
    <term>
     <guimenu>Policy when cluster does not have quorum</guimenu>
    </term>
    <listitem>
     <para>
      Whenever communication fails between one or more nodes and the rest of
      the cluster a <quote>cluster partition</quote> occurs. The nodes of a
      cluster are split in partitions but are still active. They can only
      communicate with nodes in the same partition and are unaware of the
      separated nodes. The cluster partition that has the majority of nodes is
      defined to have <quote>quorum</quote>.
     </para>
     <para>
      This configuration option defines what to do with the cluster
      partition(s) that do not have the quorum. See <ulink
      url="https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_configuration_basics_global.html"/>,
      section <citetitle>Option no-quorum-policy</citetitle>
      for details.
     </para>
     <para>
      The recommended setting is to choose <guimenu>Stop</guimenu>. However,
      <guimenu>Ignore</guimenu> is enforced for two-node clusters to ensure
      that the remaining node continues to operate normally in case the other
      node fails. For clusters using shared resources, choosing
      <guimenu>freeze</guimenu> may be used to ensure that these resources
      continue to be available.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry id="vle.pacemaker.barcl.stonith">
    <term><guimenu>Configuration mode for &stonith;</guimenu></term>
    <listitem>
     <para>
      <quote>Misbehaving</quote> nodes in a cluster are shut down to prevent
      it from causing trouble. This mechanism is referred to as &stonith;
      (<quote>Shoot the other node in the head</quote>). &stonith; can be
      configured in a variety of ways, refer to <ulink
      url="https://www.suse.com/documentation/sle_ha/book_sleha/data/cha_ha_fencing.html"/>
      for details. The following configuration options exist:
     </para>
     <variablelist>
      <varlistentry>
       <term><guimenu>Configured manually</guimenu></term>
       <listitem>
	<para>
	 &stonith; will not be configured when deploying the &barcl;. It needs
	 to be configured manually as described in <ulink
	 url="https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_fencing_config.html"/>. For
	 experts only.
	</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>
        <guimenu>Configured with IPMI data from the IPMI &barcl;</guimenu>
       </term>
       <listitem>
	<para>
	 Using this option automatically sets up &stonith; with data received
	 from the IPMI &barcl;. Being able to use this option requires that
	 IPMI is configured for all cluster nodes. This should be done by
	 default, when deploying cloud. To check or change the IPMI
	 deployment, go to <menuchoice> <guimenu>Barclamps</guimenu>
	 <guimenu>Crowbar</guimenu> <guimenu>IPMI</guimenu>
	 <guimenu>Edit</guimenu> </menuchoice>. Also make sure the
	 <guimenu>Enable BMC</guimenu> option is set to
	 <guimenu>true</guimenu> on this &barcl;.
	</para>
	
	<important>
	 <title>&stonith; Devices Must Support IPMI</title>
	 <para>
	  In order to configure &stonith; with the IPMI data,
	  <emphasis>all</emphasis> &stonith; devices must support IPMI. Problems
	  with this setup may occur with IPMI implementations that are
	  not strictly standards compliant. In this case it is recommended to
	  set up &stonith; with &stonith; block devices (SBD).
	</para>
	</important>
	
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>
        <guimenu>Configured with &stonith; Block Devices (SBD)</guimenu>
       </term>
       <listitem>
	<para>
	 This option requires to manually set up shared storage and a watchdog
	 on the cluster nodes before applying the proposal. Refer to <ulink
	 url="https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_storage_protect_fencing.html"/>
	 for in-depth information and setup instructions. Once the storage
	 devices are partitioned and the watchdog is running, specify the
	 block devices used for shared storage for each node. The paths to the
	 block devices must be entered with the <quote>by-id</quote> path:
	 <filename>/dev/disk/by-id/<replaceable>DEVICE</replaceable></filename>. 
	</para>
	<para>
	 Deploying the &barcl; will automatically complete the SBD setup on
	 the cluster nodes by starting the SBD daemon and configuring the
	 fencing resource.
	</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>
	<guimenu>
	 Configured with one shared resource for the whole cluster
	</guimenu>
       </term>
       <listitem>
	<para>
	 All nodes will use the exact same configuration. Specify the
	 <guimenu>Fencing Agent</guimenu> to use and enter
	 <guimenu>Parameters</guimenu> for the agent.
	</para>
	 <para>
	  To get a list of &stonith; devices which are supported by the High
	  Availability Extension, run the following command on an already
	  installed cluster nodes: <command>stonith -L</command>.  The list of
	  parameters depends on the respective agent. To view a list of
	  parameters use the following command: <command>stonith -t
	  <replaceable>agent</replaceable> -n</command>.
	</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Configured with one resource per node</guimenu></term>
       <listitem>
	<para>
	 All nodes in the cluster use the same <guimenu>Fencing
	 Agent</guimenu>, but can be configured with different
	 parameters. This setup is, for example, required when nodes are in
	 different chassis and therefore need different ILO parameters.
	</para>
	<para>
	  To get a list of &stonith; devices which are supported by the High
	  Availability Extension, run the following command on an already
	  installed cluster nodes: <command>stonith -L</command>.  The list of
	  parameters depends on the respective agent. To view a list of
	  parameters use the following command: <command>stonith -t
	  <replaceable>agent</replaceable> -n</command>.
	</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>
        <guimenu>Configured for nodes running in libvirt</guimenu>
       </term>
       <listitem>
	<para>
	 Use this setting for completely virtualized test installations. This
	 option is not supported.
	</para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Enable Mail Notifications</term>
    <listitem>
     <para>
      Get notified of cluster node failures via e-mail. If set to
      <guimenu>true</guimenu>, you need to specify which <guimenu>SMTP
      Server</guimenu> to use, a prefix for the mails' subject and sender and
      recipient addresses. Note that the SMTP server must be accessible by the
      cluster nodes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Prepare Cluster for DRBD</term>
    <listitem>
     <para>
      Set up DRBD for replicated storage on the cluster. This option requires
      a two-node cluster with a spare hard disk for each node. The disks
      should have a minimum size of 100 GB. Using DRBD is recommended for
      making the database and RabbitMQ highly available. For other clusters,
      set this option to <guimenu>False</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Public name for public virtual IP</guimenu></term>
    <listitem>
     <para>
      The public name is the host name that will be used instead of the
      generated public name (see <xref
      linkend="ann.depl.ostack.pacemaker.prop_name"/>) for the public virtual
      IP of &haproxy; (when registering public endpoints, for instance). Any
      name specified here needs to be resolved by a name server placed outside
      of the &cloud; network.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Password for hacluster user in &hawk;</guimenu></term>
    <listitem>
     <para>
      The password for the user <systemitem
      class="username">hacluster</systemitem>. The default value is
      <literal>crowbar</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Setup non-web GUI</term>
    <listitem>
     <para>
      Pacemaker supports two GUIs to monitor the cluster status. A Web
      user interface that can be installed by deploying the hawk-server role
      (see below), and the &hbgui; application. The latter can be
      installed on the cluster by setting this option to
      <literal>true</literal>. To access the GUI, log in to a cluster node
      with <command>ssh -X</command> as user <systemitem
      class="username">hacluster</systemitem> and start
      <command>crm_gui</command>. Note that the GUI on &cloud; should only be
      used to monitor the cluster status and not to change its configuration.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <figure>
   <title>The Pacemaker &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_pacemaker.png" width="75%"
		format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_pacemaker.png" width="75%"
		format="png"/>
    </imageobject>
   </mediaobject>
  </figure>
  <para>
   The Pacemaker service consists of two different roles. Deploying the
   <guimenu>hawk-server</guimenu> role is optional:
  </para>
  <variablelist>
   <varlistentry>
    <term><guimenu>pacemaker-cluster-member</guimenu></term>
    <listitem>
     <para>
      Deploy this role on all nodes that should become member of the cluster
      except for the one where <guimenu>pacemaker-cluster-founder</guimenu> is
      deployed. 
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>hawk-server</guimenu></term>
    <listitem>
     <para>
      Deploying this role is optional. If deployed, sets up the &hawk; Web
      interface which lets you monitor the status of the cluster. The Web
      interface can be accessed via
      <literal>http://<replaceable>IP-ADDRESS</replaceable>:7630</literal>.
      Note that the GUI on &cloud; can only be used to monitor the cluster
      status and not to change its configuration.
     </para>
     <para>
      <guimenu>hawk-server</guimenu> may be deployed on at least one cluster
      node. It is recommended to deploy it on all cluster nodes.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <figure>
   <title>The Pacemaker &Barcl;: Node deployment Example</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_pacemaker_node_deployment.png" width="75%"
		format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_pacemaker_node_deployment.png" width="75%"
		format="png"/>
    </imageobject>
   </mediaobject>
  </figure>
  <para>
   Once a cluster has been successfully deployed, it is listed under
   <guimenu>Available Clusters</guimenu> in the <guimenu>Deployment</guimenu>
   section and can be used for role deployment like a regular node. 
  </para>

  <warning>
   <title>Deploying Roles on Single Cluster Nodes</title>
   <para>
    When using clusters, roles from other &barcl;s must never be deployed to
    single nodes that are already part of a cluster. The only exceptions from
    this rule are the following roles:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Ceph-mon
     </para>
    </listitem>
    <listitem>
     <para>
      cinder-volume
     </para>
    </listitem>
    <listitem>
     <para>
      swift-proxy + swift-dispersion
     </para>
    </listitem>
    <listitem>
     <para>
      swift-ring-compute
     </para>
    </listitem>
    <listitem>
     <para>
      swift-storage
     </para>
    </listitem>
   </itemizedlist>
  </warning>

  <figure>
   <title>Available Clusters in the Deployment Section</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_database_cluster_deployment.png" width="75%"
		format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_database_cluster_deployment.png" width="75%"
		format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  &ha_service_management;
  
  <note>
   <title>Testing the Cluster Setup</title>
   <para>
    To check whether all cluster resources are running, either use the &hawk;
    Web interface or run the command <command>crm_mon
    <option>-1r</option></command>. If it is not the case, clean up the
    respective resource with <command>crm <option>resource</option>
    <option>cleanup</option> <replaceable>RESOURCE</replaceable> </command>,
    so it gets respawned.
   </para>
   <para>
    Also make sure that &stonith; correctly works before continuing with the
    &cloud; setup. This is especially important when having chosen a &stonith;
    configuration requiring manual setup. To test if &stonith; works, log in to
    a node on the cluster and run the following command:
   </para>
   <screen>pkill -9 corosync</screen>
   <para>
    In case &stonith; is correctly configured, the node will reboot.
   </para>
   <para>
    Before testing on a production cluster, plan a maintenance window in case
    issues should arise.
   </para>
  </note>
 </sect1>
 

 <sect1 id="sec.depl.ostack.db">
  <title>Deploying the Database</title>

  <para>
   The very first service that needs to be deployed is the
   <guimenu>Database</guimenu>. The database service is using PostgreSQL and
   is used by all other services. It must be installed on a &contrnode;. The
   Database can be made highly available by deploying it on a cluster.
  </para>

  <remark condition="clarity">
   2014-03-28 - fs: How to set up shared storage or DRBD for the data?
  </remark>
  
  <para>
   The only attribute you may change is the maximum number of database
   connections (<guimenu>Global Connection Limit </guimenu>). The default
   value should work in most cases&mdash;only change it for large deployments
   in case the log files show database connection failures.
  </para>
  <figure>
   <title>The Database &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_database.png" width="75%"
		format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_database.png" width="75%"
		format="png"/>
    </imageobject>
   </mediaobject>
  </figure>
  
  <sect2 id="sec.depl.ostack.db.ha">
   <title>&haSetup; for the Database</title>
   <para>
    To make the database highly available, deploy it on a cluster rather than
    on a single &contrnode;. This also makes it necessary to provide shared
    storage for the cluster that hosts the database data. This can either be
    achieved by setting up a cluster with DRBD support (see <xref
    linkend="sec.depl.ostack.pacemaker"/>) or by using
    <quote>traditional</quote> shared storage like an NFS share. It is
    recommended to use a dedicated cluster to deploy the database together
    with RabbitMQ, since both services require shared storage.
   </para>
   <para>
    Deploying the database on a cluster makes an additional <guimenu>High
    Availability</guimenu> section available in the
    <guimenu>Attributes</guimenu> section of the proposal. Configure the
    <guimenu>Storage Mode</guimenu> in this section. There are two options: 
   </para>
   <variablelist>
    <varlistentry>
     <term><guimenu>DRBD</guimenu></term>
     <listitem>
      <para>
       This option requires a two-node cluster that has been setup with
       DRBD. Also specify the <guimenu>Size to Allocate for DRBD Device (in
       Gigabytes)</guimenu>. The suggested value of 50 GB should be sufficient.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Shared Storage</term>
     <listitem>
      <para>
       Use a shared block device or an NFS mount for shared
       storage. Concordantly with the mount command, you need to
       specify three attributes: <guimenu>Name of Block Device or NFS Mount
       Specification</guimenu> (the mount point), the <guimenu>Filesystem
       Type</guimenu> and the <guimenu>Mount Options</guimenu>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>

   &nfs_export_for_ha;

  </sect2>
 </sect1>
 <sect1 id="sec.depl.ostack.keystone">
  <title>Deploying &o_ident;</title>

  <para>
   <guimenu>Keystone</guimenu> is another core component that is used by all
   other &ostack; services. It provides authentication and authorization
   services. <guimenu>Keystone</guimenu> needs to be installed on a
   &contrnode;.  &o_ident; can be made highly available by deploying it on a
   cluster. You can configure the following parameters of this &barcl;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Algorithm for Token Generation</guimenu>
    </term>
    <listitem>
     <para>
      Set the algorithm used by &o_ident; to generate the tokens. It is
      strongly recommended to use <literal>PKI</literal>, since it will reduce
      network traffic.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Default Credentials: Default Tenant</guimenu>
    </term>
    <listitem>
     <para>
      Tenant for the users. Do not change the default value of
      <literal>openstack</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     Default Credentials: Regular User/Administrator User Name/Password
    </term>
    <listitem>
     <para>
      User name and password for the regular user and the administrator. Both
      accounts can be used to log in to the &cloud; &dash; to manage
      &o_ident; users and access.
     </para>
     <figure>
      <title>The &o_ident; &Barcl;</title>
      <mediaobject>
       <imageobject role="fo">
	<imagedata fileref="depl_barclamp_keystone.png" width="75%"
		   format="png"/>
       </imageobject>
       <imageobject role="html">
	<imagedata fileref="depl_barclamp_keystone.png" width="75%"
		   format="png"/>
       </imageobject>
      </mediaobject>
     </figure>
    </listitem>
   </varlistentry>
   <varlistentry id="sec.depl.ostack.keystone.ssl">
    <term><guimenu>SSL Support: Protocol</guimenu>
    </term>
    <listitem>
     <para>
      When sticking with the default value <guimenu>HTTP</guimenu>, public
      communication will not be encrypted. Choose <guimenu>HTTPS</guimenu>
      to use SSL for encryption. See <xref linkend="sec.depl.req.ssl"/> for
      background information and <xref
      linkend="sec.depl.inst.nodes.post.ssl"/> for installation instructions.
      The following additional configuration options will become available
      when choosing <guimenu>HTTPS</guimenu>:
     </para>
     <variablelist>
      <varlistentry>
       <term><guimenu>Generate (self-signed) certificates</guimenu></term>
       <listitem>
	<para>
	 When set to <literal>true</literal>, self-signed certificates are
	 automatically generated and copied to the correct locations. This
	 setting is for testing purposes only and should never be used in
	 production environments!
	</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>
	<guimenu>SSL Certificate File</guimenu> / <guimenu>SSL (Private) Key
	File</guimenu>
       </term>
       <listitem>
	<para>
	 Location of the certificate key pair files.
	</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL Certificate is insecure</guimenu></term>
       <listitem>
	<para>
	 Set this option to <literal>true</literal> when using self-signed
	 certificates,in order to disable certificate checks. This setting is
	 for testing purposes only and should never be used in production
	 environments!
	</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Require Client Certificate</guimenu></term>
       <listitem>
	<para>
	 Set this option to <literal>true</literal> when using your own
	 certificate authority (CA) for signing. Having done so, you also need
	 to specify a path to the <guimenu>CA Certificates File</guimenu>. If
	 your certificates are signed by a trusted third party organization,
	 <guimenu>Require Client Certificate</guimenu> should be set to
	 <guimenu>false</guimenu>, since the <quote>official</quote>
	 certification authorities (CA) are already known by the system.
	</para>
	<figure>
	 <title>The SSL Dialog</title>
	 <mediaobject>
	  <imageobject role="fo">
	   <imagedata fileref="depl_barclamp_ssl.png" width="75%" format="png"/>
	  </imageobject>
	  <imageobject role="html">
	   <imagedata fileref="depl_barclamp_ssl.png" width="75%" format="png"/>
	  </imageobject>
	 </mediaobject>
	</figure>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
  </variablelist>


  <sect2 id="sec.depl.ostack.keystone.ldap">
   <title>LDAP Authentication with &o_ident;</title>
   <para>
    By default &o_ident; uses an SQL database back-end store for
    authentication. Alternatively, LDAP can be used. Using LDAP requires the
    &contrnode; on which &o_ident; is installed to be able to contact the LDAP
    server. See <xref linkend="app.deploy.network_json"/> for instructions on
    how to adjust the network setup.
   </para>
   <para>
    To configure LDAP integration, you need to open the &o_ident; &barcl;
    <guimenu>Attribute </guimenu>configuration in <guimenu>Raw</guimenu>
    mode. Search for the <guimenu>ldap</guimenu> section.
   </para>
   <figure>
    <title>The &o_ident; &Barcl;: Raw Mode</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="depl_barclamp_keystone_raw.png" width="75%"
		 format="png"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="depl_barclamp_keystone_raw.png" width="75%"
		 format="png"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    Adjust the settings according to your LDAP setup. The default
    configuration does not include all attributes that can be set&mdash;a
    complete list of options is available in the file
    <filename>/opt/dell/chef/data_bags/crowbar/bc-template-keystone.schema</filename>
    on the &admserv; (search for <literal>ldap</literal>). There are three
    types of attribute values: strings (e.g. the value for
    <literal>url</literal>:<literal>"ldap://localhost"</literal>), bool
    (e.g. the value for <literal>use_dumb_member</literal>:
    <literal>false</literal>) and integer (e.g. the value for
    <literal>page_size</literal>: <literal>0</literal>). Attribute names and
    string values always need to be quoted with double quotes; bool and
    integer values must not be quoted.
   </para>

   <important>
    <title>Using LDAP over SSL (ldaps) is recommended</title>
    <para>
     In a production environment, it is recommended to use LDAP over SSL
     (ldaps), otherwise passwords will be transferred as plain text.
    </para>
   </important>

<!-- fs 2014-03-31: Commenting because of bnc #866096
                    Keeping because of bnc #859929

   <para>
    Apart from the LDAP configuration, you need to replace &o_ident;'s
    default SQL identity driver with the hybrid driver. Search for the
    <literal>identity</literal> section and replace the value for
    <guimenu>driver</guimenu>, resulting in the following code:
   </para>
   <screen>  "identity": {
    "driver": "keystone.identity.backends.hybrid.Identity"
  },</screen>
-->
  </sect2>
  <sect2 id="sec.depl.ostack.keystone.ha">
   <title>&haSetup; for &o_ident;</title>
   <para>
    Making &o_ident; highly available requires no special
    configuration&mdash;it is sufficient to deploy it on a cluster.
   </para>
  </sect2>
 </sect1>
 <sect1 id="sec.depl.ostack.rabbit">
  <title>Deploying RabbitMQ</title>

  <para>
   The RabbitMQ messaging system enables services to communicate with the
   other nodes via Advanced Message Queue Protocol (AMQP). Deploying it is
   mandatory. RabbitMQ needs to be installed on a &contrnode;. RabbitMQ can be
   made highly available by deploying it on a cluster.It is recommended not to
   change the default values of the proposal's attributes.
  </para>
  <variablelist>
   <varlistentry>
    <term><guimenu>Virtual Host</guimenu></term>
    <listitem>
     <para>
      Name of the default virtual host to be created and used by the RabbitMQ
      server (<literal>default_vhost</literal> configuration option in
      <filename>rabbitmq.config</filename>).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Port</term>
    <listitem>
     <para>
      Port the RabbitMQ server listens on (<literal>tcp_listeners</literal>
      configuration option in <filename>rabbitmq.config</filename>).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>User</term>
    <listitem>
     <para>
      RabbitMQ default user (<literal>default_user</literal> configuration
      option in <filename>rabbitmq.config</filename>).
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <figure>
   <title>The RabbitMQ &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_rabbitmq.png" width="75%"
		format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_rabbitmq.png" width="75%"
		format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 id="sec.depl.ostack.rabbit.ha">
   <title>&haSetup; for RabbitMQ</title>
   <para>
    To make RabbitMQ highly available, deploy it on a cluster rather than
    on a single &contrnode;. This also makes it necessary to provide shared
    storage for the cluster that hosts the RabbitMQ data. This can either be
    achieved by setting up a cluster with DRBD support (see <xref
    linkend="sec.depl.ostack.pacemaker"/>) or by using
    <quote>traditional</quote> shared storage like an NFS share. It is
    recommended to use a dedicated cluster to deploy RabbitMq together
    with the database, since both services require shared storage.
   </para>
   <para>
    Deploying RabbitMQ on a cluster makes an additional <guimenu>High
    Availability</guimenu> section available in the
    <guimenu>Attributes</guimenu> section of the proposal. Configure the
    <guimenu>Storage Mode</guimenu> in this section. There are two options: 
   </para>
   <variablelist>
    <varlistentry>
     <term><guimenu>DRBD</guimenu></term>
     <listitem>
      <para>
       This option requires a two-node cluster that has been setup with
       DRBD. Also specify the <guimenu>Size to Allocate for DRBD Device (in
       Gigabytes)</guimenu>. The suggested value of 50 GB should be sufficient.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Shared Storage</term>
     <listitem>
      <para>
       Use a shared block device or an NFS mount for shared
       storage. Concordantly with the mount command, you need to
       specify three attributes: <guimenu>Name of Block Device or NFS Mount
       Specification</guimenu> (the mount point), the <guimenu>Filesystem
       Type</guimenu> and the <guimenu>Mount Options</guimenu>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>

   &nfs_export_for_ha;

  </sect2>
 </sect1>
 <sect1 id="sec.depl.ostack.ceph">
  <title>Deploying &ceph; (optional, unsupported)</title>

  <para>
   &ceph; adds a redundant block storage service to &cloud;. It lets you
   store persistent devices that can be mounted from &vmguest;s. It offers
   high data security by storing the data redundantly on a pool of
   &stornode;s&mdash;therefore &ceph; needs to be installed on at least two
   dedicated nodes.
  </para>

  <para>
   For more information on the &ceph; project, visit
   <ulink
   url="http://ceph.com/"/>.
  </para>
  &no-ceph-support;
  <para>
   The &ceph; &barcl; only has one configuration option:
  </para>
  <variablelist>
   <varlistentry>
    <term><guimenu>Disk Selection Method</guimenu></term>
    <listitem>
     <para>
      Choose whether to only use the first available disk or all available
      disks. <quote>Available disks</quote> are all disks currently not used
      by the system. Note that one disk (usually
      <filename>/dev/sda</filename>) of every block storage node is already
      used for the operating system and is not available for &ceph;.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   The &ceph; service consists of two different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Ceph-mon</guimenu>
    </term>
    <listitem>
     <para>
      Cluster monitor daemon for the &ceph; distributed file system.
      <guimenu>Ceph-mon</guimenu> needs to be installed on two, three, or four
      &stornode;s. Deploying &ceph; with only one or more than four monitor
      nodes will fail.
     </para>
     <para>
      Nodes running <guimenu>Ceph-mon</guimenu> cannot be deleted or
      temporarily be disabled once &cloud; is deployed.
     </para>
     <para>
      <guimenu>Ceph-mon</guimenu> may be deployed on any &contrnode; and on
      dedicated <guimenu>Ceph-osd</guimenu> nodes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Ceph-osd</guimenu>
    </term>
    <listitem>
     <para>
      The virtual block storage service. Install this role on all dedicated
      &ceph; &stornode;s (at least two), but not on any other node.
     </para>
     <warning>
      <title><guimenu>Ceph-osd</guimenu> Needs Dedicated Machines</title>
      <para>
       Never deploy <guimenu>Ceph-osd</guimenu> on a node that runs other
       non-&ceph; &ostack; services. The only service that may be deployed
       together with it is <guimenu>Ceph-mon</guimenu>.
      </para>
     </warning>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &ceph; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_ceph.png" width="75%"
		format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_ceph.png" width="75%"
		format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 id="sec.depl.ostack.ceph.ha">
   <title>&haSetup; for &ceph;</title>
   <para>
    &ceph; is HA-enabled by design, so there is no need for a special &hasetup;.
   </para>
  </sect2>

 </sect1>
 <sect1 id="sec.depl.ostack.swift">
  <title>Deploying &o_objstore; (optional)</title>

  <para>
   &o_objstore; adds an object storage service to &cloud; that lets you store
   single files such as images or snapshots. It offers high data security by
   storing the data redundantly on a pool of &stornode;s&mdash;therefore
   &o_objstore; needs to be installed on at least two dedicated nodes.
  </para>

<!--
  <para>
   It is recommended not to change the defaults in the &barcl; proposal,
   unless you know exactly what you require. However you should change the
   <guimenu>Cluster Admin Password</guimenu>. If you plan to change the
   <guimenu>Zone</guimenu> value, it is important to know that you need at
   least as many &stornode;s as <guimenu>Zones</guimenu>.
  </para>
-->

  <para>
   In order to be able to properly configure &o_objstore; it is important to
   understand how it places the data. Data is always stored redundantly within
   the hierarchy. The &o_objstore; hierarchy in &cloud; is formed out of
   zones, nodes, hard drives, and logical partitions. Zones are physically
   separated clusters, for example different server rooms each with its own
   power supply and network segment. A failure of one zone must not affect
   another zone. The next level in the hierarchy are the individual
   &o_objstore; storage nodes (on which <guimenu>swift-storage</guimenu> has
   been deployed) followed by the hard drives. Logical partitions come last.
  </para>
  <para>
   &o_objstore; automatically places three copies of each object on the
   highest hierarchy level possible. If three zones are available, the each
   copy of the object will be placed in a different zone. In a one zone setup
   with more than two nodes, the object copies will each be stored on a
   different node. In a one zone setup with two nodes, the copies will be
   distributed on different hard disks. If no other hierarchy element fits,
   logical partitions are used.
  </para>
  <para>
   The following attributes can be set to configure &o_objstore;:
  </para>
  <variablelist>
   <varlistentry>
    <term><guimenu>Allow Public Containers</guimenu></term>
    <listitem>
     <para>
      Allows to enable public access to containers id set to
      <literal>true</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Zones</guimenu></term>
    <listitem>
     <para>
      Number of zones (see above). If you do not have different independent
      installations of storage nodes, set the number of zones to
      <literal>1</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Create 2^X Logical Partitions</term>
    <listitem>
     <para>
      Partition power. The number entered here is used to compute the number
      of logical partitions to be created in the cluster by using it as a
      power of 2 (2^X).
     </para>
     <para>
      It's recommended to use a minimum of 100 partitions per disk. To measure
      the partition power for your setup, do the following: Multiply the
      number of disks from all &o_objstore; nodes with 100 and then round up
      to the nearest power of two. Keep in mind that the first disk of each
      node is not used by &o_objstore;, but rather for the operating system.
     </para>
     <formalpara>
      <title>Example: 10 &o_objstore; nodes with 5 HDD each</title>
      <para>
       Four hard disks on each node are used for &o_objstore;, so there is a
       total of forty disks. Multiplied with 100 gives 4000. The nearest power
       of two, 4096, equals 2^12. So the partition power that needs to be
       entered is <literal>12</literal>.
      </para>
     </formalpara>

     <important>
      <title>Value Cannot be Changed Once the Proposal Has Been Deployed</title>
      <para>
       Changing the number of logical partition after &o_objstore; has been
       deployed is not supported. Therefore the value for the partition power
       should be calculated from the maximum number of partitions this cloud
       installation is likely going to need at any point in time.
      </para>
     </important>

    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Minimum Hours before Partition is reassigned</term>
    <listitem>
     <para>
      This option sets the number of hours before a logical partition is
      considered for relocation. <literal>24</literal> is the recommended
      value.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Replicas</guimenu></term>
    <listitem>
     <para>
      The number of copies generated for each object. Set this value to
      <literal>3</literal>, the tested and recommended value.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Cluster Admin Password</term>
    <listitem>
     <para>
      The &o_objstore; administrator password.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Debug</term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <literal>true</literal>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <figure>
   <title>The &o_objstore; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_swift.png" width="75%"
		format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_swift.png" width="75%"
		format="png"/>
    </imageobject>
   </mediaobject>
  </figure>
  <para>
   Apart from the general configuration described above, the &o_objstore;
   &barcl; lets you also activate and configure <guimenu>Additional
   Middlewares</guimenu>. The features these middlewares provide can be used
   via the &o_objstore; command line client only. The Ratelimit and S3
   middlewares certainly provide for the most interesting features, whereas it
   is recommended to only enable further middlewares for specific use-cases.
  </para>
  <variablelist>
   <varlistentry>
    <term><guimenu>S3</guimenu></term>
    <listitem>
     <para>
      Provides an S3 compatible API on top of &o_objstore;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>StaticWeb</guimenu></term>
    <listitem>
     <para>
      Enables to serve container data as a static Web site with an index file
      and optional file listings. See <ulink
      url="http://docs.openstack.org/developer/swift/misc.html#module-swift.common.middleware.staticweb"/>
      for details.
     </para>
     <para>
      This middleware requires to set <guimenu>Allow Public
      Containers</guimenu> to <literal>true</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>TempURL</guimenu></term>
    <listitem>
     <para>
      Enables to create URLs to provide time limited access to objects. See
      <ulink
	url="http://docs.openstack.org/developer/swift/misc.html#module-swift.common.middleware.tempurl"/>
      for details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>FormPOST</guimenu></term>
    <listitem>
     <para>
      Enables to upload files to a container via web form. See <ulink
      url="http://docs.openstack.org/developer/swift/misc.html#module-swift.common.middleware.formpost"/>
      for details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Domain Remap</guimenu></term>
    <listitem>
     <para>
      Translates container and account parts of a domain to path
      parameters that the &o_objstore; proxy server understands. Can be used
      to create short URLs that are easy to remember, for example by rewriting
      <literal>home.&exampleuser;.&exampledomain;/$ROOT/exampleuser;/home/myfile</literal>
      to <literal>home.&exampleuser;.&exampledomain;/myfile</literal>.  See
      <ulink
      url="http://docs.openstack.org/developer/swift/misc.html#module-swift.common.middleware.domain_remap"/>
      for details.
      <!-- <guimenu>Path root</guimenu> <guimenu>Storage Domain</guimenu> -->
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Ratelimit</guimenu></term>
    <listitem>
     <para>
      Ratelimit enables you to throttle resources such as requests per minute
      to provide denial of service protection. See <ulink
      url="http://docs.openstack.org/developer/swift/ratelimit.html"/> for
      details.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   The &o_objstore; service consists of four different roles. Deploying
   <guimenu>swift-dispersion</guimenu> is optional:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>swift-storage</guimenu>
    </term>
    <listitem>
     <para>
      The virtual object storage service. Install this role on all dedicated
      &o_objstore; &stornode;s (at least two), but not on any other node.
     </para>
     <warning>
      <title>swift-storage Needs Dedicated Machines</title>
      <para>
       Never install the swift-storage service on a node that runs other
       &ostack; services.
      </para>
     </warning>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>swift-ring-compute</guimenu>
    </term>
    <listitem>
     <para>
      The ring maintains the information about the location of objects,
      replicas, and devices. It can be compared to an index, that is used by
      various &ostack; services to look up the physical location of objects.
      <guimenu>swift-ring-compute</guimenu> must only be installed on a
      single node; it is recommended to use a &contrnode;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>swift-proxy</guimenu>
    </term>
    <listitem>
     <para>
      The &o_objstore; proxy server takes care of routing requests to
      &o_objstore;.  Installing a single instance of
      <guimenu>swift-proxy</guimenu> on a &contrnode; is recommended.  The
      <guimenu>swift-proxy</guimenu> role can be made highly available by
      deploying it on a cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>swift-dispersion</guimenu>
    </term>
    <listitem>
     <para>
      Deploying <guimenu>swift-dispersion</guimenu> is optional. The
      &o_objstore; dispersion tools can be used to test the health of the
      cluster. It creates a heap of dummy objects (using 1% of the total space
      available). The state of these objects can be queried using the
      swift-dispersion-report query. <guimenu>swift-dispersion</guimenu> needs
      to be installed on a &contrnode;.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <figure>
   <title>The &o_objstore; &Barcl;: Node Deployment Example</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_swift_node_deployment.png" width="75%"
		format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_swift_node_deployment.png" width="75%"
		format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 id="sec.depl.ostack.swift.ha">
   <title>&haSetup; for &swift;</title>
   <para>
    &swift; replicates by design, so there is no need for a special
    &hasetup;. Make sure to fulfill the requirements listed in <xref
    linkend="sec.depl.reg.ha.storage.swift"/>.
   </para>
  </sect2>

 </sect1>
 <sect1 id="sec.depl.ostack.glance">
  <title>Deploying &o_img;</title>

  <para>
   &o_img; provides discovery, registration, and delivery services for virtual
   disk images. An image is needed to start an &vmguest;&mdash;it is its
   pre-installed root-partition. All images you want to use in your cloud to
   boot &vmguest;s from, are provided by &o_img;. &o_img; must be deployed
   onto a &contrnode;. &o_img; can be made highly available by deploying it on
   a cluster.
  </para>

  <para>
   There are a lot of options to configure &o_img;. The most important ones
   are explained below&mdash;for a complete reference refer to <ulink
   url="http://github.com/crowbar/crowbar/wiki/Glance--barclamp"/>.
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Notification Strategy</guimenu></term>
    <listitem>
     <para>
      &o_img; notifications can be used for auditing and troubleshooting. By
      default they (<guimenu>Noop</guimenu>) are disabled. When choosing
      <guimenu>RabbitMQ</guimenu>, notifications are send to the RabbitMQ
      service.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Default Storage Backend</guimenu></term>
    <listitem>
     <para>
      Choose whether to use &o_objstore; or &ceph; (<guimenu>Rados</guimenu>)
      to store the images. If you have deployed neither of these services, the
      images can alternatively be stored in an image file on the &contrnode;
      (<guimenu>File</guimenu>). If you have deployed &o_objstore;, it is
      recommended to use it for &o_img; as well.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Image Storage: Image Store Directory</guimenu>
    </term>
    <listitem>
     <para>
      This option is only available when having chosen the file storage
      back-end. Specify the directory to host the image file. The directory
      specified here can also be an NFS share. See <xref
      linkend="sec.depl.inst.nodes.post.nfs"/> for more information.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Image Storage: Swift Container</guimenu></term>
    <listitem>
     <para>
      This option is only available when having chosen the &o_objstore; storage
      back-end. Sets the name of the container to use for the images in
      &o_objstore;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Image Storage: RADOS User/RADOS Pool</term>
    <listitem>
     <para>
      This option is only available when having chosen the RADOS storage
      back-end. Specify a user name for the Cephx authentication and a &ceph;
      pool name for the images here.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSL Support: Protocol</term>
    <listitem>
     <para>
      Choose whether to encrypt public communication
      (<guimenu>HTTPS</guimenu>) or not (<guimenu>HTTP</guimenu>). If choosing
      <guimenu>HTTPS</guimenu>, refer to <xref
      linkend="sec.depl.ostack.keystone.ssl"/> for configuration details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>API: Bind to All Addresses</guimenu></term>
    <listitem>
     <para>
      Set this option to <guimenu>true</guimenu> to enable users to
      upload images to &o_img;. If unset, only the operator will be able to
      upload images.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Caching</guimenu>
    </term>
    <listitem>
     <para>
      Enable and configure image caching in this section. By default, image
      caching is disabled. Learn more about &o_img;'s caching feature at
      <ulink url="http://docs.openstack.org/developer/glance/cache.html"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Database: SQL Idle Timeout</guimenu></term>
    <listitem>
     <para>
      Time after which idle database connections will be dropped.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Logging: Verbose</guimenu></term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <figure>
   <title>The &o_img; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_glance.png" width="75%"
		format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_glance.png" width="75%"
		format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 id="sec.depl.ostack.glance.ha">
   <title>&haSetup; for &o_img;</title>
   <para>
    &o_img; can be made highly available by deploying it on a cluster. It is
    also strongly recommended to do so for the image data, too. The recommended
    way to achieve this is to use &o_objstore; or an external &ceph; cluster
    for the image repository. If using a directory on the node instead (file
    storage back-end), you should set up shared storage on the cluster for it.
   </para>
  </sect2>
 </sect1>

 <sect1 id="sec.depl.ostack.cinder">
  <title>Deploying &o_blockstore;</title>
  <para>
   &o_blockstore;, the successor of Nova Volume, provides volume block
   storage. It adds persistent storage to a &vmguest; that will
   persist until deleted (contrary to ephemeral volumes that will only persist
   while the &vmguest; is running).
  </para>
  <para>
   &o_blockstore; can provide volume storage by using a local file, one or
   more local disks, &ceph; (RADOS) or network storage solutions from NetApp,
   EMC, or EqualLogic. Using a local file is not recommended
   for production systems for performance reasons.
  </para>

  <para>
   The attributes that can be set to configure &o_blockstore; depend on the
   <guimenu>Type of Volume</guimenu>. The only general option is <guimenu>SSL
   Support: Protocol</guimenu> (see <xref
   linkend="sec.depl.ostack.keystone.ssl"/> for configuration details).
  </para>

  <bridgehead renderas="sect2">
   <guimenu>Raw devices</guimenu> (local disks)
  </bridgehead>

  <variablelist>
   <title><guimenu>Disk-based Parameters</guimenu></title>
   <varlistentry>
    <term><guimenu>Disk Selection Method</guimenu></term>
    <listitem>
     <para>
      Choose whether to only use the <guimenu>first</guimenu> available disk
      or <guimenu>all</guimenu> available disks. <quote>Available
      disks</quote> are all disks, currently not used by the system. Note that
      one disk (usually <filename>/dev/sda</filename>) of every block storage
      node is already used for the operating system and is not available for
      &o_blockstore;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Name of Volume</guimenu></term>
    <listitem>
     <para>
      Specify a name for the &o_blockstore; volume.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect2">
   <guimenu>Local file</guimenu>
  </bridgehead>

  <variablelist>
   <title><guimenu>File-based Parameters</guimenu></title>
   <varlistentry>
    <term><guimenu>Volume File Name</guimenu></term>
    <listitem>
     <para>
      Absolute path to the file to be used for block storage.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Maximum File Size</guimenu></term>
    <listitem>
     <para>
      Maximum size of the volume file. Make sure not to overcommit the
      size, since it will result in data loss.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Name of Volume</guimenu></term>
    <listitem>
     <para>
      Specify a name for the &o_blockstore; volume.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <note>
   <title>Using <guimenu>Local File</guimenu> for block storage</title>
   <para>
    Using a file for block storage is not recommended for production systems,
    because of performance and data security reasons.
   </para>
  </note>

  <bridgehead renderas="sect2">
   <guimenu>NetApp</guimenu>
  </bridgehead>

  <variablelist>
   <title><guimenu>NetApp Parameters</guimenu></title>
   <varlistentry>
    <term><guimenu>NetApp Driver Mode</guimenu></term>
    <listitem>
     <para>
      &cloud; can either use the 7-Mode direct driver or the direct driver for
      clustered data ONTAP via iSCSI or NFS. Choose the driver your NetApp is
      licensed for.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Server host name</guimenu></term>
    <listitem>
     <para>
      The management IP address for the 7-Mode storage controller or the
      cluster management IP address for the clustered Data ONTAP.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Transport Type</guimenu></term>
    <listitem>
     <para>
      Transport protocol for communicating with the storage controller or
      clustered Data ONTAP. Supported protocols are http and https. Choose the
      protocol your NetApp is licensed for.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Server port</guimenu></term>
    <listitem>
     <para>
      The 7-Mode controller/clustered Data ONTAP port to use for
      communication. Port 80 is usually used for HTTP, 443 for HTTPS.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     User Name / Password
    </term>
    <listitem>
     <para>
      Login credentials for 7-Mode controller/clustered Data ONTAP management.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Storage Service to use while provisioning</guimenu></term>
    <listitem>
     <para>
      Comma-separated list of NetApp volumes to be used for provisioning on
      7-Mode controller. This option is used to restrict provisioning to the
      specified NetApp controller volumes. In case this option is not
      specified all NetApp controller volumes except the controller root
      volume are used for provisioning &ostack; volumes.
     </para>
     <para>
      This setting only applies to the <guimenu>7-Mode iSCSI direct
      diver</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <guimenu>
      The vFiler Unit Name if Using vFiler to Host OpenStack Volumes
     </guimenu>
    </term>
    <listitem>
     <para>
      The vFiler unit to be used for provisioning of &ostack; volumes. Use
      this only if using MultiStore®.
     </para>
     <para>
      This setting only applies to the <guimenu>7-Mode iSCSI direct
      diver</guimenu>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect3">
   <guimenu>EMC</guimenu> (EMC² Storage) 
  </bridgehead>

  <remark condition="clarity">
   2013-10-04 - fs: The following Attributes are missing in the GUI and need
   to be described once they are added:
   - Server Type: VMAX or VNX
   - MaskingView (VMAX only)
   Crowbar support for registering compute nodes with VNX is also missing
  </remark>

  <variablelist>
   <title>EMC Parameters</title>
   <varlistentry>
    <term><guimenu>IP address of the ECOM server</guimenu> / <guimenu>Port of the ECOM server</guimenu></term>
    <listitem>
     <para>
      IP address and Port of the ECOM server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>User Name / Password for accessing the ECOM server</term>
    <listitem>
     <para>
      Login credentials for the ECOM server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
    <guimenu>Thin pool where user wants to create volume from</guimenu>
    </term>
    <listitem>
     <para>
      Only thin LUNs are supported by the plugin. Thin pools can be created
      using Unisphere for VMAX and VNX.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   For more information on the EMC driver refer to the &ostack; documentation
   at <ulink
   url="http://docs.openstack.org/grizzly/openstack-block-storage/admin/content/emc-smis-iscsi-driver.html"/>.
  </para>

  <bridgehead renderas="sect3">
   <guimenu>EqualLogic</guimenu>
  </bridgehead>

  <para>
   EqualLogic support is only included as a technology preview and not
   supported.
  </para>

  <bridgehead renderas="sect3">
   <guimenu>Rados</guimenu> (&ceph;)
  </bridgehead>

  <variablelist>
   <title>RADOS Parameters</title>
   <varlistentry>
    <term><guimenu>RADOS pool for Cinder volumes</guimenu></term>
    <listitem>
     <para>
      Name of the pool used to store the &o_blockstore; volumes. 
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>RADOS user for CephX authentication</guimenu></term>
    <listitem>
     <para>
      &ceph; user name.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect3">
   <guimenu>Other driver</guimenu> (&ceph;)
  </bridgehead>

  <para>
   Lets you manually pick and configure a driver. Only use this option for
   testing purposes, it is not supported.
  </para>

  <figure>
   <title>The &o_blockstore; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_cinder.png" width="75%"
		format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_cinder.png" width="75%"
		format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The &o_blockstore; service consists of two different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>cinder-controller</guimenu></term>
    <listitem>
     <para>
      The &o_blockstore; controller provides the scheduler and the
      API. Installing <guimenu>cinder-controller</guimenu> on a &contrnode; is
      recommended.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>cinder-volume</guimenu></term>
    <listitem>
     <para>
      The virtual block storage service. It can be installed on a &contrnode;,
      but it's recommended to deploy it on one or more dedicated nodes
      supplied with sufficient networking capacity, since it will generate a
      lot of network traffic.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_blockstore; &Barcl;: Node Deployment Example</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_cinder_node_deployment.png" width="75%"
		format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_cinder_node_deployment.png" width="75%"
		format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 id="sec.depl.ostack.cinder.ha">
   <title>&haSetup; for &o_blockstore;</title>
   <para>
    While the <guimenu>cinder-controller</guimenu> role can be deployed on a
    cluster, deploying <guimenu>cinder-volume</guimenu> on a cluster is not
    supported. Therefore it is generally recommended to deploy
    <guimenu>cinder-volume</guimenu> on several nodes&mdash;this ensures the
    service continues to be available even when a node fails. In addition
    with &ceph; or a network storage solution, such a setup minimizes the
    potential downtime.
   </para>
   <para>
    In case using &ceph; or a network storage is no option, you need to set up
    a shared storage directory (for example with NFS), mount it on all cinder
    volume nodes and use the <guimenu>Local File</guimenu> back-end with this
    shared directory. Using <guimenu>Raw Devices</guimenu> is not an option,
    since local disks cannot be shared.
   </para>
  </sect2>
 </sect1>

 <sect1 id="sec.depl.ostack.quantum">
  <title>Deploying &o_netw;</title>
  <para>
   &o_netw; provides network connectivity between interface devices managed by
   other &ostack; services (most likely &o_comp;). The service works by
   enabling users to create their own networks and then attach interfaces to
   them.
  </para>

  <para>
   &o_netw; must be deployed on a &contrnode;. The following attributes
   can be set to configure &o_netw;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>DHCP Domain</guimenu></term>
    <listitem>
     <para>
      Domain to use for building the host names.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Plugin</guimenu></term>
    <listitem>
     <para>
      Choose the plugin to be used with &o_netw;. The
      <guimenu>linuxbridge</guimenu> plugin only supports VLANs in
      &productname;, whereas the <guimenu>openvswitch</guimenu> plugin
      supports GRE, VLAN and flat networks. The default plugin is
      <guimenu>openvswitch</guimenu> with <guimenu>gre</guimenu>.
     </para>

     <important>
      <title>
       Do not Use the Default with VMWare vSphere and Microsoft &hyper; /
       Windows Server
      </title>
      <para>
       If you plan to enable the VMWare vSphere support, you must not choose
       <guimenu>openvswitch</guimenu>, since it is not supported in this
       scenario.
      </para>
     </important>
     <para>
      The <guimenu>vmware</guimenu> option lets you use an existing VMWare NSX
      installation. Using this plugin is <emphasis>not</emphasis> a
      prerequisite for the VMWare vSphere hypervisor support. However, it is
      needed when wanting to have security groups supported on VMWare compute
      nodes. 
     </para>
     <para>
      &cloud; also supports Cisco Nexus switches with the
      <guimenu>cisco</guimenu> plugin. Please refer to <xref
      linkend="app.deploy.cisco"/> for setup instructions.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Mode</guimenu></term>
    <listitem>
     <para>
      This option is only available when having chosen <menuchoice>
      <guimenu>Plugin</guimenu> <guimenu>openvswitch</guimenu> </menuchoice>
      or <menuchoice>
      <guimenu>Plugin</guimenu> <guimenu>cisco</guimenu> </menuchoice>.
      Set the network type to be set up by the plugin: <guimenu>gre</guimenu>
      (Generic Routing Encapsulation), <guimenu>flat</guimenu> or
      <guimenu>vlan</guimenu>.
     </para>

     <note>
      <title>openvswitch and VLANs</title>
      <para>
       When using openvswitch with VLAN support, it is required to use
       different physical devices for the admin and the nova_fixed networks,
       so the network controller node needs at least two network cards.
      </para>
     </note>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <guimenu>VMWare NSX *</guimenu> / <guimenu>UUID of the NVP *</guimenu>
    </term>
    <listitem>
     <para>
      These options are only available when having chosen <menuchoice>
      <guimenu>Plugin</guimenu> <guimenu>vmware</guimenu></menuchoice>. The
      user you specify here (<guimenu>VMWare NSX Username</guimenu> and
      <guimenu>VMWare NSX Password)</guimenu> needs to be a user with
      administrator permissions on the NSX server.
     </para>
     <para>
      Enter the IP address and the port number
      (<replaceable>IP-ADDRESS</replaceable>:<replaceable>PORT</replaceable>)
      of the controller API endpoint to <guimenu>VMWare NSX
      Controllers</guimenu>. If the port number is omitted, port 443 will be
      used. You may also enter multiple API endpoints (comma-separated),
      provided they all belong to the same controller cluster. When multiple
      API endpoints are specified, the plugin will load balance requests on
      the various API endpoints.
     </para>
     <para>
      The UUIDs for the transport zone and the gateway service can be obtained
      from the NSX server. They will be used when networks are created.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Protocol</term>
    <listitem>
     <para>
      Choose whether to encrypt public communication
      (<guimenu>HTTPS</guimenu>) or not (<guimenu>HTTP</guimenu>). If choosing
      <guimenu>HTTPS</guimenu>, refer to <xref
      linkend="sec.depl.ostack.keystone.ssl"/> for configuration details.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <figure>
   <title>The &o_netw; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_network.png" width="75%"
		format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_network.png" width="75%"
		format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The &o_netw; service consists of two different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>neutron-server</guimenu></term>
    <listitem>
     <para>
      <guimenu>neutron-server</guimenu> provides the scheduler and the
      API. It should be installed on a &contrnode;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>neutron-l3</guimenu></term>
    <listitem>
     <para>
      This service runs the various agents that manage the network traffic of
      all the cloud instances. It acts as the DHCP and DNS server as well as a
      gateway for all cloud instances. It's recommend to deploy this role on
      a dedicated node supplied with sufficient network capacity.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_netw; &barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_neutron_node_deployment.png" width="75%"
		format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_neutron_node_deployment.png" width="75%"
		format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 id="sec.depl.ostack.network.ha">
   <title>&haSetup; for &o_netw;</title>
   <para>
    &o_netw; can be made highly available by deploying
    <guimenu>neutron-server</guimenu> and <guimenu>neutron-l3</guimenu> on a
    cluster. While <guimenu>neutron-server</guimenu> may be deployed on a
    cluster shared with other services, it is strongly recommended to
    use a dedicated cluster solely for the <guimenu>neutron-l3</guimenu> role.
   </para>
   <para>
    If the network is configured to use openvswitch with VLAN support each
    node of the cluster <guimenu>neutron-l3</guimenu> runs on needs at
    least four network cards (because two separate bonds, one for the
    nova_fixed and another one for the other networks, are needed).
   </para>
  </sect2>

 </sect1>

 <sect1 id="sec.depl.ostack.nova">
  <title>Deploying &o_comp;</title>

  <para> 
   &o_comp; provides key services for managing the &cloud;, sets up the
   &compnode;s. &cloud; currently supports KVM, Xen and Microsoft Hyper V and
   VMWare vSphere. The unsupported QEMU option is included to enable test
   setups with virtualized nodes. The following attributes can be configured
   for &o_comp;:
  </para>

  <variablelist>
   <varlistentry>
    <term>
     <guimenu>
      Scheduler Options: Virtual RAM to Physical RAM allocation ratio
     </guimenu>
    </term>
    <listitem>
     <para>
      Set the <quote>overcommit ratio</quote> for RAM for &vmguest;s on the
      &compnode;s. A ratio of <literal>1.0</literal> means no
      overcommitment. Changing this value is not recommended.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <guimenu>
      Scheduler Options: Virtual CPU to Physical CPU allocation ratio
     </guimenu>
    </term>
    <listitem>
     <para>
      Set the <quote>overcommit ratio</quote> for CPUs for &vmguest;s on the
      &compnode;s. A ratio of <literal>1.0</literal> means no overcommitment.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Live Migration Support: Setup Shared Storage</term>
    <listitem>
     <para>
      <!-- FIXME double check the following -->
      Sets up a directory <filename>/var/lib/nova/instances</filename> on the
      &contrnode; on which <guimenu>nova-multi-controller</guimenu> and
      exports it via NFS to all compute nodes. This setup is required for live
      migration of &xen; &vmguest;s (but not for &kvm;) and can be used to
      provide central handling of instance data. Enabling this option is only
      recommended if &xen; live migration is required&mdash;otherwise it
      should be disabled.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Live Migration Support: Enable Libvirt Migration</guimenu>
    </term>
    <listitem>
     <para>
      Allows to move &kvm; and &xen; &vmguest;s to a different &compnode;
      running the same hypervisor (cross hypervisor migrations are not
      supported). Useful when a &compnode; needs to be shut down or rebooted
      for maintenance or when the load of the &compnode; is very
      high. &Vmguest;s can be moved while running (Live Migration).
     </para>

     <warning>
      <title>Libvirt Migration and Security</title>
      <para>
       Enabling the libvirt migration option will open a TCP port on the
       &compnode;s that allows to access to all &vmguest;s from all machines
       in the admin network. Please ensure that only authorized machines have
       access to the admin network when enabling this option.
      </para>
     </warning>

    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>KVM Options: Enable Kernel Samepage Merging</guimenu></term>
    <listitem>
     <para>
      Kernel SamePage Merging (KSM) is a Linux Kernel feature which merges
      identical memory pages from multiple running processes into one memory
      region. Enabling it optimizes memory usage on the &compnode;s when using
      the &kvm; hypervisor at the cost of slightly increasing CPU usage.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>VMware vCenter Settings</term>
    <listitem>
     <para>
      Setting up VMware support is described in a separate section. See <xref
      linkend="app.deploy.vmware"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSL Support: Protocol</term>
    <listitem>
     <para>
      Choose whether to encrypt public communication (<guimenu>HTTPS</guimenu>)
      or not (<guimenu>HTTP</guimenu>). If choosing
      <guimenu>HTTPS</guimenu>,refer to <xref
      linkend="sec.depl.ostack.keystone.ssl"/> for configuration details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSL Support for noVNC: Protocol</term>
    <listitem>
     <para>
      After having started an instance you can display its VNC console in
      the &ostack; &dash; (&o_dash;) via the browser using the noVNC
      implementation. By default this connection is not encrypted and can
      potentially be eavesdropped.
     </para>
     <para>
      Enable encrypted communication for noVNC by choosing
      <guimenu>HTTPS</guimenu>. Refer to <xref
      linkend="sec.depl.ostack.keystone.ssl"/> for configuration details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Verbose</guimenu></term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_comp; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_nova.png" width="75%"
		format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_nova.png" width="75%"
		format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The &o_comp; service consists of six different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>nova-multi-controller</guimenu>
    </term>
    <listitem>
     <para>
      Distributing and scheduling the &vmguest;s is managed by the
      <guimenu>Nova-multi-controller</guimenu>. It also provides networking
      and messaging services. <guimenu>Nova-multi-controller</guimenu> needs
      to be installed on a &contrnode;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
    <guimenu>nova-multi-compute-hyperv</guimenu> /
    <guimenu>nova-multi-compute-kvm</guimenu> /
    <guimenu>nova-multi-compute-qemu</guimenu> /
    <guimenu>nova-multi-compute-vmware</guimenu> /
    <guimenu>nova-multi-compute-xen</guimenu>
    </term>
    <listitem>
     <para>
      Provides the hypervisors (&hyper;, &kvm;, QEMU, VMware vSphere and
      &xen;) and tools needed to manage the &vmguest;s. Only one hypervisor
      can be deployed on a single compute node but you can use different
      hypervisors in your cloud by deploying different hypervisors to
      different &compnode;s. A <literal>Nova-multi-compute</literal> role
      needs to be installed on every &compnode;. However, not all hypervisors
      need to be deployed.
     </para>
     <para>
      Each image that will be made available in &cloud; to start an &vmguest;
      is bound to a hypervisor. Each hypervisor can be deployed on multiple
      &compnode;s (except for the VMWare vSphere role, see below). In a
      multi-hypervisor deployment you should make sure to deploy the
      <literal>nova-multi-compute</literal> roles in a way, that enough
      compute power is available for each hypervisor.
     </para>
     <note>
      <title>Re-assigning Hypervisors</title>
      <para>
       <remark condition="clarity">
	2013-08-05 - fs: Is this true?
       </remark>
       Existing <literal>nova-multi-compute</literal> nodes can be changed in
       a productive &cloud; without service interruption. You need to
       <quote>evacuate</quote> <!-- (see FIXME) --> the node, re-assign a new
       <literal>nova-multi-compute</literal> role via the &o_comp; &barcl; and
       <guimenu>Apply</guimenu> the
       change. <guimenu>nova-multi-compute-vmware</guimenu> can only be
       deployed on a single node.
      </para>
     </note>

     <important>
      <title>Deploying &hyper;</title>
      <para>
       <literal>nova-multi-compute-hyperv</literal> can only be deployed to
       &compnode;s running either Microsoft &hyper; Server or Windows Server
       2012. Being able to set up such &compnode;s requires to set up a
       netboot environment for Windows. Refer to <xref
       linkend="app.deploy.hyperv"/> for details.
      </para>
      <para>
       The default password for &hyper; &compnode;s will be
       <quote>crowbar</quote>.
      </para>
     </important>

     <important>
      <title>Deploying VMware vSphere (vmware)</title>
      <para>
       <remark condition="clarity">
	2013-08-05 - fs: What network requirements/adjustments are needed ??
       </remark>
       VMware vSphere is not supported <quote>natively</quote> by
       &cloud;&mdash;it rather delegates requests to an existing vCenter. It
       requires preparations at the vCenter and post install adjustments of
       the &compnode;. See <xref linkend="app.deploy.vmware"/> for
       instructions.  <guimenu>nova-multi-compute-vmware</guimenu> can only be
       deployed on a single &compnode;.
      </para>
     </important>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>
    The &o_comp; &Barcl;: Node Deployment Example with Three KVM Nodes
   </title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_nova_node_deployment.png" width="75%"
		format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_nova_node_deployment.png" width="75%"
		format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 id="sec.depl.ostack.nova.ha">
   <title>&haSetup; for &o_comp;</title>
   <para>
    Making &o_comp; highly available requires no special
    configuration&mdash;it is sufficient to deploy it on a cluster.
   </para>
   <para>
    As of &productname; &productnumber; making &compnode;s highly available is
    not supported. See <xref linkend="sec.depl.reg.ha.compute"/> for
    additional information.
   </para>
  </sect2>
  
 </sect1>
 <sect1 id="sec.depl.ostack.dash">
  <title>Deploying &o_dash; (&ostack; &dash;)</title>

  <para>
   The last service that needs to be deployed is &o_dash;, the &ostack;
   &dash;.  It provides a Web interface for users to start and stop &vmguest;s
   and for administrators to manage users, groups, roles, etc. &o_dash; should
   be installed on a &contrnode;. To make &o_dash; highly available, deploy it
   on a cluster.
  </para>

  <para>The following attributes can be configured:</para>

  <variablelist>
   <varlistentry>
    <term>Session Timeout</term>
    <listitem>
     <para>
      Timeout (in seconds) after which a user is been logged out
      automatically. The default value is set to 30 minutes (1800 seconds).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <guimenu>
      User Password Validation: Regular expression used for password validation
     </guimenu>
    </term>
    <listitem>
     <para>
      Specify a regular expression with which to check the password. The
      default expression (<literal>.{8,}</literal>) tests for a minimum length
      of 8 characters. The string you enter is interpreted as a Python regular
      expression (see <ulink
      url="http://docs.python.org/2.7/library/re.html#module-re"/> for a
      reference).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <guimenu>
      User Password Validation: Text to display if the password does not pass
      validation
     </guimenu>
    </term>
    <listitem>
     <para>
      Error message that will be displayed in case the password validation
      fails.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSL Support: Protocol</term>
    <listitem>
     <para>
      Choose whether to encrypt public communication (<guimenu>HTTPS</guimenu>)
      or not (<guimenu>HTTP</guimenu>). If choosing
      <guimenu>HTTPS</guimenu>, refer to <xref
      linkend="sec.depl.ostack.keystone.ssl"/> for configuration details.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_dash; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_nova_dashboard.png" width="75%"
		format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_nova_dashboard.png" width="75%"
		format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 id="sec.depl.ostack.dash.ha">
   <title>&haSetup; for &o_dash;</title>
   <para>
    Making &o_dash; highly available requires no special
    configuration&mdash;it is sufficient to deploy it on a cluster.
   </para>
  </sect2>

 </sect1>

 <sect1 id="sec.depl.ostack.ceilometer">
  <title>Deploying &o_meter;</title>

  <para>

   <remark condition="clarity">
    2013-10-04 - fs: Which software/billing solution can make use of the
    ceilometer data?
   </remark>
   &o_meter; collects CPU and networking data from &cloud;. This data can be
   used by a billing system to enable customer billing. Deploying &o_meter; is
   optional.
  </para>
  <para>
   For more information about &o_meter; refer to the &ostack; documentation
    at <ulink url="http://docs.openstack.org/developer/ceilometer/"/>.
  </para>

  <important>
   <title>&o_meter; Restrictions</title>
   <para>
    As of &productname; &productnumber; data measuring is only supported for
    &kvm; and &xen; &vmguest;s. Other hypervisors and &cloud; features such as
    object or block storage will not be measured.
   </para>
  </important>

  <para>
   The following attributes can be configured for &o_meter;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Use MongoDB instead of standard database</guimenu></term>
    <listitem>
     <para>
      &o_meter; collects a huge amount of data, which is written to a
      database. In a production system it is recommended to use a separate
      database for &o_meter; rather than the standard database that is also
      used by the other &cloud; services. MongoDB is optimized to write a lot
      of data. As of &productname; &productnumber; MongoDB is only included as
      a technology preview and not supported.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   The &o_meter; service consists of four different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>ceilometer-server</guimenu></term>
    <listitem>
     <para>
      The &o_meter; API server role. This role needs to be deployed on a
      &contrnode;. &o_meter; collects approximately 200 bytes of data per hour
      and &vmguest;. Unless you have a very huge number of &vmguest;s, there
      is no need to install it on a dedicated node.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>ceilometer-cagent</term>
    <listitem>
     <para>
      The central agent listens to the message bus to collect data. It needs
      to be deployed on a &contrnode;. It can be deployed on the same node as
      <guimenu>ceilometer-server</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>ceilometer-agent</term>
    <listitem>
     <para>
      The compute agents collect data from the compute nodes. They have to be
      deployed on all &kvm; and &xen; compute nodes in your cloud (other
      hypervisors are currently not supported).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>ceilometer-swift-proxy-middleware</term>
    <listitem>
     <para>
      An agent collecting data from the &swift; nodes. This role needs to be
      deployed on the same node as swift-proxy.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_meter; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_ceilometer.png" width="75%"
		format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_ceilometer.png" width="75%"
		format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 id="sec.depl.ostack.ceilometer.ha">
   <title>&haSetup; for &o_meter;</title>
   <para>
    Making &o_meter; highly available requires no special
    configuration&mdash;it is sufficient to deploy the roles
    <guimenu>ceilometer-server</guimenu> and
    <guimenu>ceilometer-cagent</guimenu>on a cluster.
   </para>
  </sect2>

 </sect1>

 <sect1 id="sec.depl.ostack.heat">
  <title>Deploying &o_orch;</title>
  <para>
   &o_orch; is a template-based orchestration engine that enables you to, for
   example, start workloads requiring multiple servers or to automatically
   restart &vmguest;s if needed. It also brings auto-scaling to &cloud; by
   automatically starting additional &vmguest;s if certain criteria are met.
   For more information about &o_orch; refer to the &ostack; documentation
    at <ulink url="http://docs.openstack.org/developer/heat/"/>.
  </para>
  <para>
   &o_orch; should be deployed on a &contrnode;. To make &o_orch; highly
   available, deploy it on a cluster.
  </para>
  <para>
   The following attributes can be configured for &o_orch;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Setup Verbose Logging</guimenu></term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_orch; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_heat.png" width="75%"
		format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_heat.png" width="75%"
		format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 id="sec.depl.ostack.heat.ha">
   <title>&haSetup; for &o_orch;</title>
   <para>
    Making &o_orch; highly available requires no special
    configuration&mdash;it is sufficient to deploy it on a cluster.
   </para>
  </sect2>

 </sect1>

 <sect1 id="sec.depl.ostack.final">
  <title>How to Proceed</title>

  <para>
   With a successful deployment of the &ostack; &dash;, the &productname;
   installation is finished. In order to be able to test your setup by
   starting an &vmguest; one last step remains to be done&mdash;uploading an
   image to the &o_img; service. Refer to <xref linkend="sec.adm.cli.img"/>
   for instructions. Images for &cloud; can be built in SUSE
   Studio&mdash;see this blog post for details:
   <ulink
   url="http://blog.susestudio.com/2012/10/kvm-build-format-suse-cloud-support.html"/>.
  </para>
  <para>
   Now you can hand over to the cloud administrator to set up users, roles,
   flavors, etc.&mdash; refer to the <xref linkend="book.cloud.admin"/> for
   details. The default credentials for the &ostack; &dash; are user name
   <literal>admin</literal> and password <literal>crowbar</literal>.
  </para>
 </sect1>
</chapter>
