<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet
 href="urn:x-daps:xslt:profiling:novdoc-profile.xsl" 
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "IGNORE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>

<chapter id="cha.depl.ostack">
 <title>Deploying the &ostack; Services</title>
 <para>
  Once the nodes are installed, and configured you can start deploying the
  &ostack; services in order to finalize the installation. The services need
  to be deployed in a given order, because they depend on one
  another. Deployment is done from the &crow; Web interface through recipes,
  so called <quote>&barcl;</quote>.
 </para>
 <para>
  The services controlling the cloud (including storage management and control
  services) need to be installed on the &contrnode;. . However, you may
  <emphasis>not</emphasis> use your &contrnode; as a compute or storage
  host. Here is a list with services that may <emphasis>not</emphasis> be
  installed on the &contrnode;: <guimenu>Swift-storage</guimenu>,
  <guimenu>&ceph;-store</guimenu>,
  <guimenu>Nova-multi-compute</guimenu>. These services need to be installed
  on dedicated nodes.
 </para>
 <para>
  The &ostack; services need to be deployed in the following order. For
  general instructions on how to edit and deploy &barcl; refer to <xref
  linkend="sec.depl.ostack.barclamps"/>. Deploying &swift; and &ceph; is
  optional, all other services must be deployed.
 </para>
 <orderedlist>
  <listitem>
   <!-- Database -->
   <para>
    <xref linkend="sec.depl.ostack.db" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
   <!-- Keystone -->
   <para>
    <xref linkend="sec.depl.ostack.keystone" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
   <!-- Swift -->
   <para>
    <xref linkend="sec.depl.ostack.swift" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
   <!-- Ceph -->
   <para>
    <xref linkend="sec.depl.ostack.ceph" xrefstyle="select:title nopage"/>
   </para>
   <important>
    <title>&ceph; not Supported</title>
    <para>
     As of &productname; &productnumber; &ceph; is not officially
     supported but rather included as a technical preview, so using Nova
     Volume instead is recommended.
    </para>
   </important>
  </listitem>
  <listitem>
   <!-- Glance -->
   <para>
    <xref linkend="sec.depl.ostack.glance" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <!--
  <listitem>
   -->
   <!-- Nova -->
<!--
   <para>
    <xref linkend="sec.depl.ostack.nova" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
   -->
   <!--<guimenu>Nova Dashboard</guimenu>--> 
<!--
   <para>
    <xref linkend="sec.depl.dash.dash" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  -->
 </orderedlist>  
 <sect1 id="sec.depl.ostack.barclamps">
  <title>&barcl;</title>
  <para>
   The &ostack; services are automatically installed on the nodes by using
   so-called &barcl;s&mdash;a set of recipes, templates and installation
   instructions. All existing &barcl;s can be accessed from the &crow; Web
   interface by clicking on <guimenu>Barclamps</guimenu>. To edit a &barcl;
   proceed as follows:
  </para>
  <procedure>
   <step>
    <para>
     Open a browser and point it to the &crow; Web interface available at port
     <literal>3000</literal> of the &admserv;, for example <ulink
     url="http://192.168.124.10:3000/"/>. Log in as user <systemitem
     class="username">crowbar</systemitem>. The password defaults to
     <literal>crowbar</literal>, if you have not changed it.
    </para>
    <para>
     Click <guimenu>Barclamps</guimenu> to open the <guimenu>All
     Barclamps</guimenu> menu. Alternatively you may filter the list to
     <guimenu>Crowbar</guimenu> or <guimenu>&ostack;</guimenu> &barcl;s by
     choosing the respective option from <guimenu>Barclamps</guimenu>. The
     <guimenu>Crowbar</guimenu> &barcl;s contain general recipes for setting
     up and configuring all nodes, while the <guimenu>&ostack;</guimenu> are
     dedicated to &ostack; service deployment and configuration.
    </para>
   </step>
   <step>
    <para>
     Click a &barcl;'s name. You can either <guimenu>Create</guimenu>
     a proposal or <guimenu>Edit</guimenu> an existing one.
    </para>
    <para>
     When creating a new proposal, give it a meaningful name and
     description. You can create several proposals, for example, for testing
     purposes, bu only one at a time can be deployed.
    </para>
    <para>
     Most &ostack; &barcl;s consist of two sections: the
     <guimenu>Attributes</guimenu> section lets you change the configuration,
     the <guimenu>Node Deployment</guimenu> section lets you choose onto
     which nodes to deploy the &barcl;.
    </para>
   </step>
   <step>
    <para>
     To edit the <guimenu>Attributes</guimenu> section change the values via
     the Web form. Alternatively you can directly edit the configuration file
     by clicking <guimenu>Raw</guimenu>.
    </para>
    <warning>
     <title>Raw Mode</title>
     <para>
      Only use the <guimenu>Raw</guimenu> mode in case in case an option
      cannot be changed via Web form. Raw mode does not perform any syntax
      checks.
     </para>
     <para>
      If you switch between <guimenu>Raw</guimenu> mode and Web form
      (<guimenu>Custom</guimenu> mode), make sure to <guimenu>Save</guimenu>
      your changes before switching, otherwise they will be lost.
     </para>
    </warning>
    <para>
     In the <guimenu>Node Deployment</guimenu> section of the &ostack;
     &barcl; you can drag and drop nodes from the <guimenu>Available
     Nodes</guimenu> column to the desired role. You need to drop the node
     onto the role name. Do <emphasis>not</emphasis> drop a node onto the
     input field, this is rather used to filter the list of
     <guimenu>Available Nodes</guimenu>!
    </para>
    <para>
     One or more nodes are usually automatically pre-selected for available
     roles. If this pre-selection does not meet your requirements, remove it
     <emphasis>before</emphasis> dragging new nodes to the role. To remove a
     node from a role, click the respective <guimenu>Remove</guimenu> icon.
    </para>
   </step>
   <step>
    <para>
     To save and deploy your edits, click <guimenu>Apply</guimenu>. To just
     save your changes without deploying them, click
     <guimenu>Save</guimenu>. To remove the complete proposal, click
     <guimenu>Delete</guimenu>. A proposal that already has been deployed can
     only be deleted manually, see <xref
     linkend="sec.depl.ostack.barclamps.delete"/> for details.
    </para>
    <para>
     If you deploy a proposal onto a node where a previous one is still
     active, the new proposal will overwrite the old one.
    </para>
    
    <note>
     <title>Always Wait Until a Proposal has been Deployed</title>
     <para>
      Deploying a proposal might take some time (up to several
      minutes). Always wait until you see the note <quote>Successfully applied
      the proposal</quote> before proceeding on to the next proposal.
     </para>
    </note>
    
   </step>
  </procedure>
  
  <warning>
   <title>&barcl; Deployment Failure</title>
   <para>
    In case the deployment of a &barcl; fails, make sure to fix the reason
    that has caused the failure and deploy the &barcl; again. Refer to the
    respective troubleshooting section at <xref
    linkend="sec.depl.trouble.faq.ostack"/> for help. A deployment
    failure may leave your node in an inconsistent state.
   </para>
  </warning>
  
  
  <sect2 id="sec.depl.ostack.barclamps.delete">
   <title>Deactivate a Proposal that Already has been Deployed</title>
   <para>
    To finally deactivate a proposal that already has been deployed, you
    first need to <guimenu>Deactivate</guimenu> it in the &crow; Web
    interface. Run the following commands as &rootuser; on the &admserv;
    afterwards: 
   </para>
   <screen>P_NAME="<replaceable>proposal_name_to_delete</replaceable>"
   SERVICE="<replaceable>service_name</replaceable>"
   crowbar $SERVICE proposal delete $P_NAME
   crowbar $SERVICE delete $P_NAME</screen>
   <para>
    <replaceable>proposal_name_to_delete</replaceable> needs to be replaced
    by the name of the proposal you want to delete.
    <replaceable>service_name</replaceable> needs to be replaced by one of
    the following strings representing the &ostack; services:
    <literal>ceph</literal>, <literal>database</literal>,
    <literal>glance</literal>, <literal>keystone</literal>,
    <literal>nova_dashboard</literal>, <literal>nova</literal>,
    <literal>swift</literal>. 
   </para>
  </sect2>
 </sect1>
 
 <sect1 id="sec.depl.ostack.db">
  <title>Deploying the Database</title>
  <para>
   The very first service that needs to be deployed is the
   <guimenu>Database</guimenu>. The database service is used by all other
   services. It must be installed on the &contrnode;.
  </para>
  <para>
   &cloud; only supports <guimenu>&postgres;</guimenu> as an
   <guimenu>SQL Engine</guimenu>, so this value must not be
   changed.
  </para>
 </sect1>  
    
 <sect1 id="sec.depl.ostack.keystone">
  <title>Deploying Keystone</title>
  <para>
   <guimenu>Keystone</guimenu> is another core component that is used by all
   other &ostack; services. It provides authentication and authorization
   services. <guimenu>Keystone</guimenu> needs to be installed on the
   &contrnode;. You can configure the following parameters of this &barcl;:
  </para>
  <variablelist>
   <varlistentry>
    <term><guimenu>SQL Engine</guimenu></term>
    <listitem>
     <para>
      Must be set to <guimenu>&postgres;/MySQL</guimenu>. This is the default.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>SQL Instance</guimenu></term>
    <listitem>
     <para>
      Name of the proposal you deployed in the previous step (see <xref
      linkend="sec.depl.ostack.db"/>). The default value should be
      correct. 
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Default Tenant</guimenu></term>
    <listitem>
     <para>
      Tenant for the users. Do not change the default value of
      <literal>openstack</literal>. 
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Regular User/Administrator Username/Password</term>
    <listitem>
     <para>
      Username and password for the regular user and the administrator. Both
      accounts can be used to log into the &cloud; &dash; to manage Keystone
      users and access.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Administrator Token (long-lived)</guimenu></term>
    <listitem>
      <para>
       The permanent administrator token (random string).
      </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Administrator Token Expiration</guimenu></term>
    <listitem>
     <para>
      Expiration Date for the administrator token. Must be entered in the
      form
      <replaceable>YYYY-MM-DD</replaceable>T<replaceable>HH:MM</replaceable>,
      e.g. 2015-03-31T12:00.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Security Attributes</guimenu></term>
    <listitem>
     <para>
      When sticking with the default value <literal>HTTP</literal>, public
      communication will not be encrypted. Choose <literal>HTTPS</literal> to
      use SSL for encryption and specify the path to the certificate
      files. Note that you need to create and copy the certificate prior to
      deploying Keystone, see <xref linkend="sec.depl.inst.nodes.post.ssl"/>
      for instructions.
      </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 
 <sect1 id="sec.depl.ostack.swift">
  <title>Deploying &swift; (optional)</title>
  <para>
   &swift; adds an object storage service to &cloud; that lets you store
   single files such as images or snapshots. It offers high data security by
   storing the data redundant on a pool of &stornode;s&mdash;therefore &swift;
   needs to be installed on at least two dedicated nodes.
  </para>
  <para>
   It is recommended to not change the defaults in the &barcl; proposal,
   unless you exactly know what you are doing. However you should change the
   <guimenu>Cluster Admin Password</guimenu>. If you plan to change the
   <guimenu>Zone</guimenu> value, it is important to know that you need at
   least as many &stornode;s as <guimenu>Zones</guimenu>. 
  </para>
  <para>
   The &swift; service consists of three different roles:
  </para>
  <variablelist>
   <varlistentry>
    <term><guimenu>Swift-ring-compute</guimenu></term>
    <listitem>
     <para>
      The ring maintains the information about the location of objects,
      replicas, and devices. It can be compared to an index, that is used by
      various &ostack; services to look up the physical location of objects.
      <guimenu>Swift-ring-compute</guimenu> must only be installed on a single
      node; it is recommended to use the the &contrnode;.
     </para>
    </listitem>
    </varlistentry>
    <varlistentry>
     <term><guimenu>Swift-proxy-acct</guimenu></term>
     <listitem>
      <para>
       The Swift proxy server takes care of routing requests to
       Swift. Installing a single instance of
       <guimenu>Swift-proxy-acct</guimenu> on the &contrnode; is recommended. 
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><guimenu>Swift-storage</guimenu></term>
     <listitem>
      <para>
       The virtual object storage service. Install this role on all dedicated
       &swift; &stornode;s (at least two), but not on any other node.
      </para>
      <warning>
       <title>Swift-storage Needs Dedicated Machines</title>
       <para>
        Never install the Swift-storage service on a node that runs other
        &ostack; services.
       </para>
      </warning>
     </listitem>
    </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.depl.ostack.ceph">
  <title>Deploying &ceph; (optional, unsupported)</title>
  <para>
   &ceph; adds a redundant block storage service to &cloud;.  It lets you
   store persistent devices that can be mounted from &vmguest;s. It offers
   high data security by storing the data redundant on a pool of
   &stornode;s&mdash;therefore &ceph; needs to be installed on at least two
   dedicated nodes. 
  </para>
<!-- fs 2012-09-25: Preserve for 2.0
Block storage can alternatively be provided by Nova Volume, but Nova Volume
offers neither redundancy nor distributed storage (Nova Volume solely runs on
the &contrnode;). When &ceph; is deployed, Nova Volume automatically makes use
of it.
-->
  <important>
   <title>&ceph; not Supported</title>
   <para>
    As of &productname; &productnumber; &ceph; is not officially
    supported but rather included as a technical preview, so using Nova
    Volume instead is recommended.
   </para>
  </important>
  <para>
   The &ceph; &barcl; only has one configuration option: telling &ceph; which
   devices to use on the nodes. Edit the &barcl; in <guimenu>Raw</guimenu> and
   search for the following the lines
  </para>
  <screen>  "devices": [
   
  ],</screen>
  <para>
    Add a comma-separated list of devices that should be used by &ceph;. For
    example:
  </para>
  <screen>  "devices": [
    "/dev/sdb", "/dev/sdc", "/dev/sdd"
  ],</screen>
  
  <important>
   <title>Devices</title>
   <para>
    Not all of the devices used for &ceph; need to exist on all nodes. All
    devices from a node matching the list will be used. They must
    <emphasis>not</emphasis> be mounted prior to deploying &ceph;. Any data
    stored on these devices will be lost.
   </para>
  </important>

  <para>
   The &ceph; service consists of three different roles:
  </para>
  <variablelist>
   <varlistentry>
    <term>><guimenu>Ceph-mon-master</guimenu></term>
    <listitem>
     <para>
      Master cluster monitor daemon for the &ceph; distributed file
      system. <guimenu>Ceph-mon-master</guimenu> must only be installed on a
      single node; it is recommended to use the the &contrnode;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Ceph-mon</guimenu></term>
    <listitem>
     <para>
      Cluster monitor daemon for the &ceph; distributed file
      system. <guimenu>Ceph-mon</guimenu> needs to be installed on two or four
      &stornode;s.
     </para>
     <important>
      <title>Number of &ceph; Monitor Nodes</title>
      <para>
       In addition to the node running the &ceph;-mon-master service an
       additional two or four nodes also need to run the
       <guimenu>&ceph;-mon</guimenu> service. The sum of the
       <guimenu>&ceph;-mon-master</guimenu> and the
       <guimenu>&ceph;-mon</guimenu> nodes must always be an odd number
       (either three or five). 
      </para>
      <para>
       Nodes running <guimenu>Ceph-mon</guimenu> cannot be deleted or
       temporarily be disabled.
      </para>
     </important>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Ceph-store</guimenu></term>
    <listitem>
     <para>
      The virtual block storage service. Install this role on all dedicated
       &ceph; &stornode;s (at least two), but not on any other node.
     </para>
     <warning>
      <title><guimenu>Ceph</guimenu>-store Needs Dedicated Machines</title>
      <para>
       Never deploy <guimenu>Ceph-store</guimenu> on a node that runs other
       non-&ceph; &ostack; services. The only service that may be deployed
       together with it is <guimenu>Ceph-mon</guimenu>.
      </para>
     </warning> 
    </listitem>
   </varlistentry>
  </variablelist>
  <para>
   Deploying &ceph; requires to perform the steps in a given order:
  </para>
  <procedure>
   <step>
    <para>
     Edit the &barcl; proposal to specify the devices to be used by &ceph; as
     described above.
    </para>
   </step>
   <step>
    <para>
     Drag and drop a node (for example, the &contrnode;) to the
     <guimenu>&ceph;-mon-master</guimenu> role.  role.
    </para>
   </step>
   <step>
    <para>
     Drag and drop two or four nodes to the <guimenu>&ceph;-mon</guimenu>
     role. Note that the maximum number of <guimenu>&ceph;-mon</guimenu> nodes
     cannot exceed four and that the sum of
     <guimenu>&ceph;-mon-master</guimenu> and <guimenu>&ceph;-mon</guimenu>
     nodes must be odd.
    </para>
   </step>
   <step>
    <para>
     Drag and drop all dedicated &ceph; &stornode;s to the
     <guimenu>&ceph;-store</guimenu> (at least two). You may also use the
     nodes with the <guimenu>&ceph;-mon</guimenu> roles, but not the
     <guimenu>&ceph;-mon-master</guimenu> node (you can add that one later).
    </para>
   </step>
   <step>
    <para>
     Click <guimenu>Apply</guimenu> to deploy your proposal. This can take some
     time.  
    </para>
   </step>
   <step>
    <para>
     If you also want to use the <guimenu>&ceph;-mon-master</guimenu> as a
     &stornode;, drag and drop it to the <guimenu>&ceph;-store</guimenu> role
     and click <guimenu>Apply</guimenu> again. Note that it is not recommended
     to use the &contrnode; for non-management purposes such as storage or
     compute.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 id="sec.depl.ostack.glance">
  <title>Deploying Glance</title>
  <para>  
   Glance provides discovery, registration, and delivery services for virtual
   disk images. An image is needed to start a &vmguest;&mdash;it is its
   pre-installed root-partition. All images you want to use in your cloud to
   boot &vmguest;s from, are provided by Glance.
  </para>
  <para>
   Glance should be deployed onto the &contrnode;. There are a lot of options
   to configure Glance. The most important ones are explained below&mdash;for
   a complete reference refer to <ulink
   url="http://github.com/dellcloudedge/crowbar/wiki/Glance--barclamp"/>.
  </para>
  <variablelist>
   <varlistentry>
    <term><guimenu>Image Store Directory</guimenu></term>
    <listitem>
     <para>
      Directory in which all images uploaded to Glance are stored. If you want
      to put the images onto a separate partition or volume, you need to mount
      this partition or volume on the &contrnode; prior to deploying the
      Glance proposal (see <xref
      linkend="sec.depl.inst.nodes.post.contrnode"/>). Specify the mount point
      of the partition or volume here.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Security Attributes</guimenu></term>
    <listitem>
     <para>
      When sticking with the default value <literal>HTTP</literal>, public
      communication will not be encrypted. Choose <literal>HTTPS</literal> to
      use SSL for encryption and specify the path to the certificate
      files. Note that you need to create and copy the certificate prior to
      deploying Glance, see <xref linkend="sec.depl.inst.nodes.post.ssl"/> for
      instructions.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>API/Registry <guimenu>Bind to All Addresses</guimenu></term>
    <listitem>
     <para>
      Set these two options to <literal>true</literal> to enable image uploads
      to Glance.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Caching</guimenu></term>
    <listitem>
     <para>
      Enable and configure image caching in this section. By default image
      caching is disabled. Learn more about Glance's caching feature at <ulink
      url="http://docs.openstack.org/developer/glance/cache.html"/>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
  
 <sect1 id="sec.depl.ostack.nova">
  <title>Deploying Nova</title>
  <para>
   Nova provides key services for managing the &cloud;, sets up the
   &compnode;s and provides a block storage service that either makes use of
   &ceph; (if deployed) or uses local disks. There are a lot of options to
   configure Nova. The most important ones are explained below&mdash;for a
   complete reference refer to <ulink
   url="https://github.com/dellcloudedge/crowbar/wiki/Nova--barclamp"/>. You
   will also find details about Nova's network modes on that page.
  </para>
  <variablelist>
   <varlistentry>
    <term><guimenu>Hypervisor</guimenu></term>
    <listitem>
     <para>
      Choose between the <guimenu>&kvm;</guimenu> and <guimenu>&xen;</guimenu>
      hypervisors (other available options are currently not supported by
      &suse;). As of &productname; &productnumber; choosing more than one
      hypervisor is not supported.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Choose disks for nova-volume storage volume group</term>
    <listitem>
     <para>
      In case you have not deployed &ceph;, Nova Volume will use local disks
      to provide block storage. Specify which disks to use with this
      option. If &ceph; was already deployed, it is used automatically and you
      will not be able to choose any disks here.
     </para>
     <note>
      <title>Nova Volume</title>
      <para>
       Nova Volume only runs on the host onto which
       <guimenu>Nova-multi-controller</guimenu> is deployed. Make sure it is
       equipped with enough spare disks. See <xref
       linkend="sec.depl.req.storage.services"/> for more information.
      </para>
     </note>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Security Attributes</guimenu></term>
    <listitem>
     <para>
      When sticking with the default value <literal>HTTP</literal>, public
      communication will not be encrypted. Choose <literal>HTTPS</literal> to
      use SSL for encryption and specify the path to the certificate
      files. Note that you need to create and copy the certificate prior to
      deploying Nova, see <xref linkend="sec.depl.inst.nodes.post.ssl"/>
      for instructions.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>noVNC Security Attributes</term>
    <listitem>
     <para>
      After having started a &vmguest; you can display its VNC console in the
      Nova &dash; via browser using the <literal>noVNC</literal>
      implementation. By default this connection is not encrypted and can
      potentially be eavesdropped. To encrypt it, you can make use of SSL by
      setting <guimenu>noVNC via SSL</guimenu> to <literal>true</literal>. If
      you do not specify any additional certificates, the same ones as for
      Nova will be used.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <para>
   The Nova service consists of two different roles:
  </para>
  <variablelist>
   <varlistentry>
    <term><guimenu>Nova-multi-controller</guimenu></term>
    <listitem>
     <para>
      Distributing and scheduling the &vmguest;s is managed by the
      <guimenu>Nova-multi-controller</guimenu>. It also provides networking
      and messaging services. <guimenu>Nova-multi-controller</guimenu> needs
      to be installed on the &contrnode;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Nova-multi-compute</guimenu></term>
    <listitem>
     <para>
      Provides the hypervisor (&kvm; or &xen;) and tools needed to manage the
      &vmguest;s. <guimenu>Nova-multi-compute</guimenu> needs to be installed
      on every &compnode;. The &compnode;s are the <quote>workhorses</quote>
      of the cloud&mdash;each &vmguest;s is started on a &compnode;.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.depl.ostack.dash">
  <title>Deploying the Nova Dashboard</title>
  <para>     
   The last service that needs to be deployed is the Nova Dashboard. It
   provides a Web interface for users to start and stop &vmguest;s and for
   administrators to manage users, groups, roles, etc. Nova Dashboard should
   be installed on the &contrnode;. The following attributes can be
   configured:
  </para>
  <variablelist>
   <varlistentry>
    <term><guimenu>SQL Engine</guimenu></term>
    <listitem>
     <para>
      Must be set to <guimenu>&postgres;/MySQL</guimenu>. This is the default.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SQL/Keystone Instance</term>
    <listitem>
     <para>
      Name of the proposal for <guimenu>Database</guimenu> and
      <guimenu>Keystone</guimenu> you deployed in the previous steps. The
      default value should be correct.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Disable SSL Certification Verification</guimenu></term>
    <listitem>
     <para>
      Usually SSL certificates are checked whether they have been signed by a
      trusted organization. Use this option to turn off checks. Useful in
      testing environments when using self-signed certificates. In production
      environments you should always use signed certificates and set this
      option to <guimenu>false</guimenu> (default).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Apache Attributes</guimenu></term>
    <listitem>
     <para>
      When sticking with the default value <guimenu>HTTP</guimenu> equals
      <literal>true</literal>, public communication will not be encrypted. Set
      <literal>HTTPS</literal> to <literal>true</literal> to use SSL for
      encryption and specify the path to the certificate files. Note that you
      need to create and copy the certificate prior to deploying Nova
      Dashboard, see <xref linkend="sec.depl.inst.nodes.post.ssl"/> for
      instructions.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
</chapter>
