<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet
 href="urn:x-daps:xslt:profiling:novdoc-profile.xsl" 
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "IGNORE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.depl.inst">
 <title>Installation Overview</title>
 <abstract>
  <para>
   Before deploying &productname;, there are a few requirements to be
   met and security considerations to be made. Make sure to thoroughly read
   this chapter&mdash;some decisions need to be made
   <emphasis>before</emphasis> deploying &cloud;, since you cannot change them
   afterwards.
  </para>
 </abstract>
 <sect1 id="sec.depl.inst.requires">
  <title>Requirements</title>
  <para>
   While deploying additional &compnode;s and &stornode;s to a running &cloud; 
   is easy<!-- (see FIXME)</para>-->, it is more difficult to update the
   administrative part of the &cloud; infrastructure (the &admserv; and the
   &contrnode;) on the fly. 
  </para>
  <important>
   <title>Changing the Network Setup</title>
   <para>Changing the network setup is not possible without re-deploying the 
   complete setup. Therefore it is essential to thoroughly plan your cloud setup
   before deploying it.</para>
  </important>
  <sect2 id="sec.depl.inst.requires.hardware">
   <title>Hardware Requirements</title>
   <para>
    Precise hardware requirements can only be listed for the &admserv; and the
    &ostack; &contrnode;. The requirements of the &ostack; compute and
    &stornode;s depends on the number of concurrent &vmguest;s and their
    virtual hardware equipment.
   </para>
  <important>
   <title>Physical Machines and Architecture</title>
   <para>
    All &cloud; nodes need to be physical machines. Although the &admserv; and
    the &contrnode; can be virtualized in test environments, this is not
    supported for production systems.
    </para>
    <para>
    &cloud; currently only runs on <literal>x86_64</literal> hardware.
   </para>
  </important>
  <sect3 id="sec.depl.inst.requires.hardware.admserv">
  <title>&admserv;</title>
  <itemizedlist>
   <listitem>
     <para>
      Architecture: x86_64
     </para>
    </listitem>
    <listitem>
     <para>
      <remark condition="clarity">
       2012-08-01 - fs: Recommendation OK?
      </remark>
      RAM: at least 2 GB, 4 GB recommended
     </para>
    </listitem>
    <listitem>
     <para>
      Hard disk: at least 40 GB. It is recommended to put 
      <filename>/srv</filename> on a separate partition with at least 25 GB space.
     </para>
    </listitem>
    <listitem>
     <para>
      Number of network cards: 1 or more, depending on your network topology
     </para>
    </listitem>
   </itemizedlist>
   </sect3>
   <sect3 id="sec.depl.inst.requires.hardware.contrnode">
   <title>&contrnode;</title>
   <itemizedlist>
    <listitem>
     <para>
      Architecture: x86_64
     </para>
    </listitem>
    <listitem>
     <para>
      <remark condition="clarity">
       2012-08-06 - fs: Recommendation OK?
      </remark>
      RAM: at least 1 GB, 2 GB recommended
     </para>
    </listitem>
    <listitem>
     <para>
      <remark condition="clarity">
       2012-08-10 - fs: Storing images on the swift nodes is not supported in
                        &productname; 1.0
      </remark>
      Hard disk: 4 GB of disk space for the operating system and the &ostack; 
      services. Additionally, enough hard disk space to host all images you want 
      to launch (e.g. an LVM volume). The images you can launch for starting 
      &vmguest;s in the cloud are also stored on the hard disk of the
      control node (by default under <filename>/var/lib/glance/images</filename>).   
     </para>
    </listitem>
    <listitem>
     <para>
      Number of network cards: 1 or more, depending on your network topology
     </para>
    </listitem>
   </itemizedlist>
   </sect3>
   <sect3 id="sec.depl.inst.requires.hardware.compnode">
   <title>&compnode;</title>
   <para>
    The &compnode;s need to be equipped with a sufficient amount of RAM and
    CPUs, matching the numbers required by the maximum number of &vmguest;s
    running concurrently. &vmguest; started in &cloud; cannot share resources
    from several physical nodes, but only uses the resources of the
    node on which they are started. So if you offer a flavor (see <xref
    linkend="gloss.flavor"/> for a definition) with 8 CPUs and 12 GB RAM at
    least one of your nodes should be able to provide these resources.
   </para>
   </sect3>
    <sect3 id="sec.depl.inst.requires.hardware.stornode">
   <title>&stornode;</title>
   <para>
    The &stornode;s are sufficiently equipped with a single CPU and one or two
    GB of RAM. Also see <xref linkend="sec.depl.inst.requires.storage"/>.
   </para>
   </sect3>
  </sect2>

  <sect2 id="sec.depl.inst.requires.storage">
   <title>Storage Requirements</title>
    <para>  
    <remark condition="clarity">
     2012-08-10 - fs: Todo: SAN/iSCSI configuration
    </remark>
    This section describes the storage requirements for the Compute and
    &stornode;s. For storage requirements of the &admserv; and &contrnode;, see
    <xref
    linkend="sec.depl.inst.requires.hardware.admserv"/> and <xref
    linkend="sec.depl.inst.requires.hardware.contrnode"/>, respectively. 
   </para>
   
   <sect3 id="sec.depl.inst.requires.storage.compute">
    <title>&compnode;s</title>
    <para>
     Each &vmguest; is started with at least one disk&mdash;a copy of the
     image from which it has been started. Depending on the flavor you start,
     the &vmguest; may also have a second, so-called <quote>ephemeral</quote>
     disk. The size of the root image depends on the image itself,
     while ephemeral disks are always created as sparse image files.
    </para>
    <para>
     Both disks, root images and ephemeral disk, are directly bound to the
     &vmguest; and are deleted when the &vmguest; is terminated. Therefore
     these disks are bound to the &compnode; on which the &vmguest; has been
     started. The disks are created under <filename>/var/lib/nova</filename>
     on the &compnode;. Therefore your &compnode;s should be equipped with
     enough disk space to store the root images and ephemeral disks.
    </para>
    <note>
     <title>Persistent Storage</title>
     <para>
      &ceph; or Nova Volume provide persistent block devices that can be mounted 
      after the &vmguest; has been started are provided. Snapshots of instances
      or volumes are also stored within &ceph; or Nova Volume.
     </para>
    </note>
   </sect3>
  
   <sect3 id="sec.depl.inst.requires.storage.store">
    <title>Storage Nodes</title>
    <para>
     The block-storage service &ceph; and the object storage service Swift need
     to be deployed onto dedicated nodes&mdash;it is not possible to mix these
     services. Each storage service requires at least two machines (more are
     recommended), otherwise it would not be possible to store the data
     redundant.
    </para>
    <para>
     <remark condition="clarity">
      2012-08-17 - fs: @devs: How to make sure &sls; is installed on the disk
      the admin has chosen for this task? If I have a machine with a 100GB
      disk for OS installation and several 1 TB disks, how to make sure the OS
      is installed on the 100 GB disk?
     </remark>
     Each &ceph;/Swift &stornode; needs at least two hard disks. The first one
     (<filename>/dev/sda</filename>) will be used for the operating system
     installation, while the others can be used for storage purposes. While
     you can configure which devices &ceph; uses for storage, Swift always uses
     all devices.
    </para>
    <para>
     If you choose to use Nova Volume rather than &ceph; for storage, the
     &contrnode; needs to be equipped with sufficient disk space on spare
     disks. Nova volume does not provide redundancy and only runs on the
     &contrnode;.
    </para>
   </sect3>
  </sect2>
  
  <sect2 id="sec.depl.inst.requires.network">
   <title>Network Requirements</title>
   <para>
    The network configuration on the nodes in the &cloud; network is
    entirely controlled by &crow;. Any network configuration not done with
    &crow; (e.g. with &yast;) will automatically be overwritten. Once the
    cloud is deployed, network settings cannot be changed anymore!
    </para>
    <para>
    By default the following networks will be created within the &cloud;:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <systemitem class="etheraddress">192.168.124/24</systemitem> (admin
      network to access the &admserv; and the &ostack; nodes)
     </para>
    </listitem>
    <listitem>
     <para>
      <systemitem class="etheraddress">192.168.122/24</systemitem> (VLAN,
      public network to access the &dash; and the &vmguest;s)
     </para>
    </listitem>
    <listitem>
     <para>
      <systemitem class="etheraddress">192.168.123/24</systemitem> (VLAN,
      network for inter-&vmguest; communication, internal only)
     </para>
    </listitem>
    <listitem>
     <para>
      <systemitem class="etheraddress">192.168.125/24</systemitem> (VLAN,
      network for &store; (Swift and &ceph;), internal only)
     </para>
    </listitem>
   </itemizedlist>
   <!--
   <para>
    A more complex network setup is discussed at <xref
    linkend="app.deploy.network"/>. If you need to change the default IP
    addresses listed above, you need to adjust the network configuration
    templates <emphasis>prior</emphasis> to running the &cloud; setup
    script. See <xref linkend="sec.depl.admin_server.install.os.crowbar"/> for
    instructions.
   </para>
   -->
   <para>
    Any additional network card (<systemitem
    class="resource">eth1</systemitem>+) on the &admserv; that is used to
    access other networks, must have a static IP address&mdash;DHCP is
    currently not supported. These interfaces will be shut down up 
    <remark>taroth 2012-08-17: hm, seems to me either "down" or "up" is right,
    from the next sentence it probably must be "down" here - DEVs, your call :)
    </remark>by &crow; during the &cloud; setup process and will only be set up
    <emphasis>after</emphasis> the &admserv; installation has been
    finished. As a consequence, external repositories (if configured at the
    time you run the &cloud; installation script) need to be accessible from
    the first network card (<systemitem class="resource">eth0</systemitem>).
   </para>
   <para>
    If you install the &smt; Add-On product you need to be able to access
    the Internet from one of the network interfaces on the &admserv;.
   </para>
   <important>
    <title>Network Requirements Summary</title>
    <itemizedlist>
     <listitem>
      <para>
       Network setup is completely controlled by &crow;
      </para>
     </listitem>
     <listitem>
      <para>
       Network setup cannot be changed once &cloud; is deployed
      </para>
     </listitem>
     <listitem>
      <para>
       NIC used for <systemitem class="resource">eth0</systemitem> needs to
       support teaming
      </para>
     </listitem>
     <listitem>
      <para>
       external repositories need to be accessible via <systemitem
       class="resource">eth0</systemitem>
      </para>
     </listitem>
     <listitem>
      <para>
       additional network interfaces <systemitem
       class="resource">eth1</systemitem>+ need to be have a static IP address
      </para>
     </listitem>
     <listitem>
      <para>
       If the &smt; server is installed, Internet access is required
      </para>
     </listitem>
    </itemizedlist>
   </important>
  </sect2>
  
  <sect2 id="sec.depl.inst.requires.software">
   <title>Software Requirements</title>
   <para>
    The following software requirements need to be met in order to install
    &cloud;:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &sls; 11 SP2 installation media (included in the &cloud; &admserv;
      subscription)   
     </para>
    </listitem>
    <listitem>
     <para>
      Access to the &sls; 11 SP2 Update repositories (either by registering
      &sls; 11 SP2 or via an existing &smt; server).
     </para>
    </listitem>
    <listitem>
     <para>
      &smt; installation media. A free download is available on <ulink
      url="http://www.novell.com/linux/smt/"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      &cloud; installation media.
     </para>
    </listitem>
    <listitem>
     <para>
      A &suse;/&novell; account (for product registration and &smt; setup). If
      you do not already have one, go to <ulink
      url="http://www.suse.com/login"/> to create it. 
     </para>
    </listitem>
   </itemizedlist>
   
   <sect3 id="sec.depl.inst.requires.software.repos">
    <title>Access to Update Repositories</title>
    
    <remark condition="clarity">
     2012-08-08 - fs: Other alternatives for update repositories?
     Both remote alternatives require to adjust autoyast.xml.erb
    </remark>
    
    <para>
     In order to keep the operating system and the &cloud; software itself
     up-to-date on the nodes, the appropriate update repositories need to be
     accessible from all nodes. There are several possibilities to achieve that:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Setting up an &smt; server on the &admserv;.
      </para>
     </listitem>
     <listitem>
      <para>
       Using the repositories from an an existing &smt; server. This requires
       that the &smt; server can be accessed from all &cloud; nodes and that
       it it subscribed to the following channels: &sls; 11 SP2 and &cloud; update
       channels.
      </para>
     </listitem>
     <listitem>
      <para>
       Exporting the repositories from an existing SMT server via NFS and mount
       them on the &admserv;.
      </para>
     </listitem>
     <listitem>
      <para>
      Synchronizing the repositories from an existing SMT server to the &admserv;
      with <command>rsync</command>.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>
 </sect1>

<!--
 <sect1 id="sec.depl.arch.security">
  <title>Security Considerations</title>

  <para>FIXME</para>
  <screen>
   * https://github.com/SUSE/cloud/wiki/Common-issues
   * https://github.com/SUSE/cloud/wiki/Nova-compute
   * enabling SSL

    Which communication will be encrypted?

   
     Needs a certificate
      -> point to SLES Apache doc for HowTo
      -> sufficient to put it on Admin node in /etc/apache2/ssl.*/

Passwords:

* SMT DB Password (change via yast)
* MySQL Root Password (MySQL is used for SMT)
* Crowbar Password (change cia yast)
* Chef WebUI Password:
  - initially change via yast (before install-chef-suse run)
  - in the running system change via
    * Login to the Crowbar Web UI on port 3000:
      * Click on Barclamps in the top right corner of the Web UI.
      * Click on the arrow to the left of the Crowbar barclamp to expand it
      * Click the Edit button to edit the Default proposal.
      * Click on Raw to the right of Attributes.
      * Scroll through the attributes until you see something that looks like:

    "users": {
      "machine-install": {
        "password": "0e7b02a8d2086009c1ff00cc26a827d981306cbd420b1862063e6df8534e0f6a13e45100b9874d0a3fae5962c9ec2de12c0525b8c33685e8ee30406c4eee7133"
      },
      "crowbar": {
        "password": "crowbar"
      }
    },

    * Change the password.
    * Click Apply and confirm you want to apply the changes.
 - nodes root-Password (change in autoyast.xml.erb -> how? hashed, clear text?)

* Keystone
  - "Regular User" Name/Password
  - "Administrator" Name/Password
  Change via Keystone Barclamp

* Glance
  - Service User: Glance User in Keystone
  - Service Password: PW for Service User
  Change via Glance Barclamp

* Swift
  Cluster Admin Password
  Change via Swift Barclamp
    


   
  </screen>
-->
<!-- SSL support:

* vuntz tries to reinstall everything from scratch, with ssl by default
<Magic> vuntz: Would you do me a favor and document (in short notes) what is needed in the different stages of the installation to support SSL? That would make documenting it much easier for me.
<vuntz> Magic: I assume it's just chosing the right options for the keystone, glance, nova and nova_dashboard proposals
<vuntz> choosing
<bmwiedemann> yes. clicking "HTTPS" dropdowns in crowbar-dashboard
<bmwiedemann> the hard part is generating trusted key+crt files
<bmwiedemann> as the autogenerated ones are just self-signed
<Magic> Would the following be sufficient as a lilnk to point to when it comes to generating certificates? http://docserv.suse.de/documents/SLES/SLES-admin/htmlsingle/#sec.apache2.ssl
<Magic> (Much of it is about creating a self-signed certificate, I know, but there also is a section on getting an officially signed one)
<Magic> Do the certificates for Cloud have to be placed in the default location ( /etc/apache2/ssl.crt/) or somewhere special?
<vuntz> Magic: I assume it's enough, yes
<iartarisi> ping thomas / thomas1
<vuntz> Magic: iirc, we have options in the barclamps to tell the path to the certificates

-->


 <sect1 id="sec.depl.inst.installation">
  <title>Installation Process</title>

  <para>
   Deploying and installing &productname; is a multi-step process, starting
   by deploying a basic &sls; installation onto the &admserv;. Now the
   &admserv; needs to be properly configured before the &cloud; packages are
   installed. After the &crow; network templates have been adapted to the
   machine's network configuration, run the installation script for &cloud;.
   It will install and configure &chef; and use it to complete the
   installation of &crow; and all required &barcl;s on the &admserv;.
   <!--taroth 2012-08-17: fs, I would phrase some of the sentence before in 
   active voice to make clear what the operator needs to do and what is
   done automatically-->
  </para>

  <para>
   Once the &admserv; is installed and configured, the Controller, Compute
   and Storage Nodes can be deployed. The complete node deployment is done
   automatically via &crow; and &chef; from the &admserv;. All you need
   to do is to PXE boot the nodes and to distribute the &ostack; services
   onto them.
  </para>

  <procedure>
   <title>High Level Overview of the &cloud; Installation</title>
   <!--taroth 2012-08-17: fs, if you have separate sections or procedures for
   the steps, use xrefs instead?-->
   <step>
    <para>
     Install &sls; <remark>taroth 2012-08-17: fs, mention 11 SP2?</remark>
     on the &admserv; with the Add-On products &smtool; (&smt;,
     optional) and &cloud;.
    </para>
    <substeps>
     <step>
      <para>
       If &smt; has been installed on the &admserv;, mirror the &sls;
       repositories.
      </para>
     </step>
     <step>
      <para>
       Make sure all repositories needed for deploying the nodes are locally
       available on the &admserv;.
      </para>
     </step>
     <step>
      <para>
       Run the &chef; installation script which will finalize the &cloud;
       installation on the &admserv;.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     PXE boot all nodes onto which the &ostack; components should be
     deployed and allocate them in the &crow; Web interface to start the automatic
     &sls; installation.
    </para>
   </step>
   <step>
    <para>
     Configure and deploy the &ostack; services via the &crow; Web interface or
     command line tools.
    </para>
   </step>
   <step>
    <para>
     When all &ostack; services are up and running, &cloud; is ready. Upload
     images and start deploying virtual machines.<!--taroth 2012-08-17: 
    fs, as uploading images and deploying VMs is usually not done by the cloud 
    operator, perhaps refer to cloud admin here--> 
    </para>
   </step>
  </procedure>
 </sect1>
</chapter>
