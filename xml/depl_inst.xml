<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet
 href="urn:x-daps:xslt:profiling:novdoc-profile.xsl" 
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "IGNORE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.depl.inst">
 <title>Installation Overview</title>
 <abstract>
  <para>
   FIXME
  </para>
 </abstract>
 <sect1 id="sec.depl.inst.requires">
  <title>Requirements</title>
  <para>
   While it is easy to deploy additional &compnode;s and &stornode;s to
   &cloud; while it is running, it is more difficult to update the
   administrative part of the &cloud; infrastructure (the &admserv; and the
   &contrnode;) on the fly. Whats more, changing the network setup is not
   possible without re-deploying the complete setup. Therefore it is essential
   to thoroughly plan your cloud setup. 
  </para>
  <sect2 id="sec.depl.inst.requires.hardware">
   <title>Hardware Requirements</title>
   <para>
    Precise hardware requirements can only be listed for the &admserv; and the
    &ostack; controller node. The requirements of the &ostack; compute and
    storage nodes depends on the number of concurrent &vmguest;s and their
    virtual hardware equipment.
   </para>
   <para>
    The &compnode;s need to be equipped with a sufficient amount of RAM and
    CPUs, matching the numbers required by the maximum number of &vmguest;s
    running concurrently. &vmguest; started in &cloud; cannot share resources
    from several physical nodes, but can rather only use the resources of the
    node on which they are started. So if you offer a flavor (see <xref
    linkend="gloss.flavor"/> for a definition) with 8 CPUs and 12 GB RAM at
    least one of your nodes should be able to provide these resources.
   </para>
   <para>
    The &stornode;s are sufficiently equipped with a single CPU and one or two
    GB of RAM. Also see <xref linkend="sec.depl.inst.requires.storage"/>.
   </para>
   <para>
    All &cloud; nodes need to be physical machine. Although the &admserv; and
    the &contrnode; can be virtualized in test environments, this is not
    supported for production systems.
   </para>
   <itemizedlist id="li.depl.inst.requires.hardware.admserv">
    <title>Hardware Requirements for &admserv;</title>
    <listitem>
     <para>
      Architecture: x86_64
     </para>
    </listitem>
    <listitem>
     <para>
      <remark condition="clarity">
       2012-08-01 - fs: Recommendation OK?
      </remark>
      RAM: at least 2 GB, 4 GB recommended
     </para>
    </listitem>
    <listitem>
     <para>
      Harddisk: at least 40 GB
     </para>
    </listitem>
    <listitem>
     <para>
      <remark condition="clarity">
       2012-08-01 - fs: Please review
      </remark>
      Number of network cards: 2
     </para>
     <important>
      <title>Teaming support</title>
      <para>
       <remark condition="clarity">
        2012-08-01 - fs: Is this correct?
       </remark>
       The network card used as <systemitem
       class="resource">eth0</systemitem> needs to support <literal>NIC
       Teaming</literal>. If you are deploying the &admserv; on a virtual
       machine, be aware that fully virtualized NICs may not support teaming.
      </para>
     </important>
    </listitem>
   </itemizedlist>
   <itemizedlist id="li.depl.inst.requires.hardware.contrnode">
    <title>Hardware Requirements for &contrnode;</title>
    <listitem>
     <para>
      Architecture: x86_64
     </para>
    </listitem>
    <listitem>
     <para>
      <remark condition="clarity">
       2012-08-06 - fs: Recommendation OK?
      </remark>
      RAM: at least 1 GB, 2 GB recommended
     </para>
    </listitem>
    <listitem>
     <para>
      <remark condition="clarity">
       2012-08-06 - fs: Can the images also be stored on the storage nodes
       when swift is enabled?
      </remark>
      Harddisk: the operating system and the &ostack; services require no more
      than 4 GB of disk space. However, the images you can launch in order to
      start &vmguest;s in the cloud are also stored on the hard disk of the
      control node (by default under
      <filename>/var/lib/glance/images</filename>). Make sure to equip the
      &contrnode; with enough hard disk space to host all images you want to
      launch.  
     </para>
    </listitem>
    <listitem>
     <para>
      <remark condition="clarity">
       2012-08-01 - fs: Please review
      </remark>
      Number of network cards: 2
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 id="sec.depl.inst.requires.storage">
   <title>Storage Requirements</title>
   <para/>
   
   <remark condition="clarity">
    2012-08-06 - fs: How to set up nova volume?
   </remark>
   
   <screen>
    - admin node: smt repodata: separate partition/LVM 25 GB
    - control node:
      * separate partition/LVM for glance images (/var/lib/glance/images)
    - compute nodes:
      * space for ephemeral disks (always bound to the machine where the VM
        was started)
      * space for images copied from glance. Only applies to VMs with no root
        disk (minimal flavor)
    - storage nodes: ??
    - nova volume: ??
   </screen>
  </sect2>
  
  <sect2 id="sec.depl.inst.requires.network">
   <title>Network Requirements</title>
   <para>
    The network configuration on the nodes in the &cloud; network is
    entirely controlled by &crow;. Any network configuration not done with
    &crow; (e.g. with &yast;) will automatically be overwritten. Once the
    cloud is deployed, network settings cannot be changed anymore! 
    By default the following networks will be created within the &cloud;:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <systemitem class="etheraddress">192.168.124/24</systemitem> (admin
      network to access the &admserv; and the &ostack; nodes)
     </para>
    </listitem>
    <listitem>
     <para>
      <systemitem class="etheraddress">192.168.122/24</systemitem> (vLAN,
      public network to access the &dash; and the &vmguest;s)
     </para>
    </listitem>
    <listitem>
     <para>
      <systemitem class="etheraddress">192.168.123/24</systemitem> (vLAN,
      network for inter-&vmguest; communication, internal only)
     </para>
    </listitem>
    <listitem>
     <para>
      <systemitem class="etheraddress">192.168.125/24</systemitem> (vLAN,
      network for &store; (Swift), internal only)
     </para>
    </listitem>
   </itemizedlist>
   <para>
    If these default IP ranges do not met your requirements and/or if you
    follow the network proposal described at FIXME, you need to adjust the
    network configuration templates <emphasis>prior</emphasis> to running
    the &cloud; setup script. See FIXME for instructions.
   </para>
   <para>
    Any additional network card (<systemitem
    class="resource">eth1</systemitem>+) on the &admserv; that is used to
    access other networks, must have a static IP address&mdash;DHCP is
    currently not supported. These interfaces will be shut down up by &crow;
    during the &cloud; setup process and will only be set up
    <emphasis>after</emphasis> the &admserv; installation has been
    finished. As a consequence, external repositories (if configured at the
    time you run the &cloud; installation script) need to be accessible from
    the first network card (<systemitem class="resource">eth0</systemitem>).
   </para>
   <para>
    If you install the &smt; Add-On product you need to be able to access
    the internet from one of the network interfaces on the &admserv;.
   </para>
   <important>
    <title>Network Requirements Summary</title>
    <itemizedlist>
     <listitem>
      <para>
       Network setup is completely controlled by &crow;
      </para>
     </listitem>
     <listitem>
      <para>
       Network setup cannot be changed once &cloud; is deployed
      </para>
     </listitem>
     <listitem>
      <para>
       NIC used for <systemitem class="resource">eth0</systemitem> needs to
       support teaming
      </para>
     </listitem>
     <listitem>
      <para>
       external repositories need to be accessible via <systemitem
       class="resource">eth0</systemitem>
      </para>
     </listitem>
     <listitem>
      <para>
       additional network interfaces <systemitem
       class="resource">eth1</systemitem>+ need to be have a static IP address
      </para>
     </listitem>
     <listitem>
      <para>
       If the &smt; server is installed, internet access is required
      </para>
     </listitem>
    </itemizedlist>
   </important>
  </sect2>
  
  <sect2 id="sec.depl.inst.requires.software">
   <title>Software Requirements</title>
   <para>
    The following software requirements need to be met in order to install
    &cloud;:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &sls; 11 SP2 installation media (included in the &cloud; &admserv;
      subscription)   
     </para>
    </listitem>
    <listitem>
     <para>
      Access to the &sls; 11 SP2 Update repositories (either by registering
      &sls; 11 SP2 or via an existing &smt; server)
     </para>
    </listitem>
    <listitem>
     <para>
      &smt; installation media; free download available on <ulink
      url="http://www.novell.com/linux/smt/"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      &cloud; installation media
     </para>
    </listitem>
    <listitem>
     <para>
      A &suse;/&novell; account (for product registration and &smt; setup). If
      you do not already have one, go to <ulink
      url="http://www.suse.com/login"/> to create it. 
     </para>
    </listitem>
   </itemizedlist>
   
   <sect3 id="sec.depl.inst.requires.software.repos">
    <title>Access to Update Repositories</title>
    
    <remark condition="clarity">
     2012-08-08 - fs: Other alternatives?
     Both remote alternatives require to adjust autoyast.xml.erb
    </remark>
    
    <para>
     In order to keep the operating system and the &cloud; software itself
     up-to-date on the nodes, the appropriate update repositories need to be
     accessible from all nodes. There are several possibilities to achieve that:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Setting up an &smt; server on the &admserv;.
      </para>
     </listitem>
     <listitem>
      <para>
       Use the repositories from an an existing &smt; server. This requires
       that the &smt; server can be accessed from all &cloud; nodes and that
       it it subscribed to the &sls; 11 SP2 as well as the &cloud; update
       channels.
      </para>
     </listitem>
     <listitem>
      <para>
       Export the directories mirroring the repositories via NFS for example
       and mount them on the nodes. This requires that the exported directories
       can be mounted from all &cloud; nodes.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   
  </sect2>
 </sect1>
 <sect1 id="sec.depl.inst.installation">
  <title>Installation Process</title>

  <para>
   Deploying and installing &productname; is a multi-step process, starting
   by deploying a basic &sls; installation onto the &admserv;. Now the
   &admserv; needs to be properly configured before the &cloud; packages are
   installed. After the &crow; network templates have been adapted to the
   machine's network configuration, run the installation script for &cloud;.
   It will install and configure &chef; and use it to complete the
   installation of &crow; and all required &barcl; on the &admserv;.
  </para>

  <para>
   Once the &admserv; is installed and configured, the Controller, Compute
   and Storage Nodes can be deployed. The complete node deployment is done
   automatically via &crow; and &chef; from the admin server. All you need
   to do is to PXE boot the nodes and to distribute the &ostack; services
   onto them.
  </para>

  <procedure>
   <title>High Level Overview of the &cloud; Installation</title>
   <step>
    <para>
     Install &sls; on the &admserv; with the Add-On products &smtool; (&smt;)
     and &cloud;.
    </para>
    <substeps>
     <step>
      <para>
       If &smt; has been installed on the &admserv;, mirror the &sls;
       repositories.
      </para>
     </step>
     <step>
      <para>
       Make all repositories needed for deploying the nodes locally
       available on the &admserv;.
      </para>
     </step>
     <step>
      <para>
       Run the &chef; installation script which will finalize the &cloud;
       installation on the &admserv;.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     PXE boot all nodes onto which the &ostack; components should be
     deployed and allocate them in the &crow; web UI to start the automatic
     &sls; installation.
    </para>
   </step>
   <step>
    <para>
     Configure and deploy the &ostack; services.
    </para>
   </step>
   <step>
    <para>
     When all &ostack; services are up and running, &cloud; is ready. Upload
     and image and start deploying virtual machines.
    </para>
   </step>
  </procedure>
 </sect1>
</chapter>
