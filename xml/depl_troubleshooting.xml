<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet
 href="urn:x-daps:xslt:profiling:novdoc-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "IGNORE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.depl.trouble">
 <title>Troubleshooting and Support</title>
  <para>
   Find solutions for the most common pitfalls and technical details on how to
   create a support request for &productname; here.
  </para>
  
 <sect1 id="sec.depl.trouble.faq">
  <title>FAQ</title>

  <para>
   If your problem is not mentioned here, checking the log files on either the
   &admserv; or the &ostack; nodes may help. A list of log files is available
   at <xref linkend="cha.deploy.logs"/>.
  </para>

  <qandaset defaultlabel="qanda">
   <qandadiv id="sec.depl.trouble.faq.admin">
    <title>Admin Node Deployment</title>
    <qandaentry>
     <question>
      <para>
       What to do when <command>install-suse-cloud</command> fails?
      </para>
     </question>
     <answer>
      <para>
       Please check the script's log file at
       <filename>/var/log/crowbar/install.log</filename> for error messages.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
<!-- bnc #782337 -->
       What to do if <command>install-suse-cloud</command> fails while
       deploying the IPMI/BMC network?
      </para>
     </question>
     <answer>
      <para>
       As of &productname; &productnumber;, it is assumed that each machine
       can be accessed directly via IPMI/BMC. However, this is not the case on
       certain blade hardware, where several nodes are accessed via a common
       adapter. Such a hardware setup causes an error on deploying the
       IPMI/BMC network. You need to disable the IPMI deployment running the
       following command:
      </para>
<screen>/opt/dell/bin/json-edit -r -a "attributes.ipmi.bmc_enable" \
-v "false" /opt/dell/chef/data_bags/crowbar/bc-template-ipmi.json</screen>
      <para>
       Re-run <command>install-suse-cloud</command> after having disabled
       the IPMI deployment.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       Why am I not able to reach the &admserv; from outside the admin
       network via the bastion network?
      </para>
     </question>
     <answer>
      <para>
       If <command>route <option>-n</option></command> shows no gateway for
       the bastion network, make sure the value for the bastion network's
       <literal>"router_pref":</literal> entry in
       <filename>/etc/crowbar/network.json</filename> is set to a
       <emphasis>lower</emphasis> value than the
       <literal>"router_pref":</literal> entry for the admin network.
      </para>
      <para>
       If the router preference is set correctly, <command>route
       <option>-n</option></command> shows a gateway for the bastion
       network. In case the &admserv; is still not accessible via its admin
       network address (e.g. <systemitem
       class="ipaddress">192.168.124.10</systemitem>), you need to disable
       route verification (<literal>rp_filter</literal>). Do so by running the
       following command on the &admserv;:
      </para>
      <screen>echo 0 > /proc/sys/net/ipv4/conf/all/rp_filter</screen>
      <para>
       If this setting solves the problem, make it permanent by editing
       <filename>/etc/sysctl.conf</filename> and setting the value for
       <literal>net.ipv4.conf.all.rp_filter</literal> to <literal>0</literal>.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       Can I change the hostname of the &admserv;?
      </para>
     </question>
     <answer>
      <para>
       No, once you have run <command>install-suse-cloud</command> you
       cannot change the hostname anymore. Services like &crow;, &chef;, and
       the RabbitMQ will fail when having changed the hostname.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       What to do when browsing the &chef; Web UI gives a <literal>Tampered
       with cookie</literal> error?
      </para>
     </question>
     <answer>
      <para>
       You probably have an old cookie in your browser from a previous
       &chef; installation on the same IP. Remove the cookie named
       <literal>_chef_server_session_id</literal> and try again.
      </para>
     </answer>
    </qandaentry>
    <qandaentry id="q.depl.trouble.faq.admin.custom_repos">
     <question>
      <para>
       How to make custom software repositories from an external server
       available for the nodes?
      </para>
     </question>
     <answer>
      <para>
       Custom repositories need to be added manually to
       <filename>/etc/crowbar/provisioner.json</filename> (it is currently not
       possible to use the &yast; &crow; module for this purpose). To add a
       repository called <literal>my_custom_repo_1.0</literal>, from an &smt; server, run the following
       command:
      </para>
      <screen>/opt/dell/bin/json-edit \
-a "attributes.provisioner.suse.autoyast.repos.<replaceable>my_custom_repo_1\.0</replaceable>.url \
-v "http://<replaceable>smt.&exampledomain;</replaceable>/repo/\$RCE/<replaceable>my_repos</replaceable>/x86_64/" \
/etc/crowbar/provisioner.json
</screen>
      <para>
       Note that you need to escape <literal>.</literal> (period) in
       repository names like in <replaceable>my_custom_repo_1\.0</replaceable>!
      </para>
      <para>
       Access errors to a repository are silently ignored by default. To
       ensure that you get notified of these errors, set the <literal>Ask On
       Error</literal> flag for the repository by running the following
       command:
      </para>
      <screen>/opt/dell/bin/json-edit -r \
-a "attributes.provisioner.suse.autoyast.repos.my_custom_repo_1\.0.ask_on_error" \
-v "true" /etc/crowbar/provisioner.json</screen>
     </answer>
    </qandaentry>
   </qandadiv>
   <qandadiv id="sec.depl.trouble.faq.ostack">
    <title>&ostack; Node Deployment</title>
    <qandaentry id="var.depl.trouble.faq.ostack.login">
     <question>
      <para>
       How can I log in to a node as &rootuser;?
      </para>
     </question>
     <answer>
      <para>
       By default you cannot directly log in to a node as &rootuser;,
       because the nodes were set up without a &rootuser; password. You can
       only log in via SSH from the &admserv;. You should be able to log in
       to a node with
       <command>ssh&nbsp;root@<replaceable>NAME</replaceable></command>
       where <replaceable>NAME</replaceable> is the name (alias) of the
       node.
      </para>
      <para>
       If name resolution does not work, go to the &crow; Web interface and
       open the <guimenu>Node Dashboard</guimenu>. Click on the name of the
       node and look for its <guimenu>admin (eth0)</guimenu> <guimenu>IP
       Address</guimenu>. Log in to that IP address via SSH as user
       &rootuser;.
      </para>
     </answer>
    </qandaentry>
<!-- fs 2013-10-14: Needs clarification
    <qandaentry>
     <question>
      <para>
       How to change the default disk used for operating system installation?
      </para>
     </question>
     <answer>
      <para/>
     </answer>
    </qandaentry>
-->
    <qandaentry>
     <question>
      <para>
       What to do if a node refuses to boot or boots into a previous
       installation?
      </para>
     </question>
     <answer>
      <para>
       Make sure to change the boot order in the BIOS of the node, so that
       the first boot option is to boot from the network/PXE boot.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       What to do if a node hangs during hardware discovery after the very
       first PXE boot into the <quote>SLEShammer</quote> image?
      </para>
     </question>
     <answer>
      <para>
       The &rootuser; login is enabled at a very early state in discovery
       mode, so chances are high that you can log in for debugging purposes
       as described in <xref linkend="var.depl.trouble.faq.ostack.login"/>.
       If logging in as &rootuser; does not work, you need to set the
       &rootuser; password manually:
      </para>
      <orderedlist>
       <listitem>
        <para>
         Create a directory on the &admserv; named
         <filename>/updates/discovering-pre</filename>
        </para>
<screen>mkdir /updates/discovering-pre</screen>
       </listitem>
       <listitem>
        <para>
         Create a hook script <filename>setpw.hook</filename> in the
         directory created in the previous step:
        </para>
<screen>cat > /updates/discovering-pre/setpw.hook &lt;&lt;EOF
#!/bin/sh
echo "linux" | passwd --stdin root
EOF</screen>
       </listitem>
       <listitem>
        <para>
         Make the script executable:
        </para>
<screen>chmod a+x  /updates/discovering-pre/setpw.hook</screen>
       </listitem>
      </orderedlist>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       What to do when a deployed node fails to PXE boot with the following
       error message: <literal>Could not find kernel image:
       ../suse-11.3/install/boot/x86_64/loader/linux</literal>?
      </para>
     </question>
     <answer>
      <para>
       The installation repository at
       <filename>/srv/tftpboot/suse-11.3/install</filename> on the &admserv;
       has not been set up correctly to contain the &sls; 11 SP3
       installation media. Please review the instructions at
       <xref
       linkend="sec.depl.inst.admserv.post.local_repos"/>.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       Why does my deployed node hang at <literal>Unpacking
       initramfs</literal> during PXE boot?
      </para>
     </question>
     <answer>
      <para>
       The node probably does not have enough RAM. You need at least 2 GB
       RAM.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       What to do if a node hangs at <literal>Executing AutoYast script:
       /var/adm/autoinstall/init.d/crowbar_join</literal> after the
       installation has been finished?
      </para>
     </question>
     <answer>
      <para>
       Be patient&mdash;the &ay; script may take a while to finish. If it
       really hangs, log in to the node as &rootuser; (see
       <xref
       linkend="var.depl.trouble.faq.ostack.login"/> for
       details). Check the log files at
       <filename>/var/log/crowbar/crowbar_join/*</filename> for errors.
      </para>
      <para>
       If the node is in a state where login in from the &admserv; is not
       possible, you need to create a &rootuser; password for it as
       described in <xref linkend=" var.depl.inst.nodes.prep.root_login"/>.
       Now re-install the node by going to the node on the &crow; Web
       interface and clicking <guimenu>Reinstall</guimenu>. After having
       been re-installed, the node will hang again, but now you will be able
       to log in and check the log files to find the cause.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       Where to find more information when applying a &barcl; proposal
       fails?
      </para>
     </question>
     <answer>
      <para>
       Check the &chef; client logs on the &admserv; located at
       <filename>/var/log/crowbar/chef-client/d*.log</filename>. Further
       information is available from the &chef; client logs located on the
       node(s) affected by the proposal
       (<filename>/var/log/chef/client.log</filename>), and also from the
       logs of the service that failed to be deployed. Additional
       information may be gained from the &crow; Web UI logs on the
       &admserv;. For a list of log file locations refer to
       <xref linkend="cha.deploy.logs"/>.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       I have installed a new hard disk on a node that was already deployed.
       Why is it ignored by &crow;?
      </para>
     </question>
     <answer>
      <para>
       When adding a new hard disk to a node that has already been deployed,
       it can take up to 15 minutes before the new disk is detected.
      </para>
     </answer>
    </qandaentry>
   </qandadiv>
  </qandaset>
 </sect1>
 <sect1 id="sec.depl.trouble.support">
  <title>Support</title>

  <para>
   <remark condition="clarity">
    2012-09-07 - fs: Do we have some official text on cloud and support we
    could use here?
   </remark>
   Before contacting support to help you with a problem on &cloud;, it is
   strongly recommended that you gather as much information about your
   system and the problem as possible. For this purpose, &productname; ships
   with a tool called <command>supportconfig</command>. It gathers system
   information such as the current kernel version being used, the hardware,
   RPM database, partitions, and other items.
   <command>supportconfig</command> also collects the most important log
   files, making it easier for the supporters to identify and solve your
   problem.
  </para>

  <para>
   It is recommended to always run <command>supportconfig</command> on the
   &admserv; as well as on the &contrnode;. If a &compnode; or a &stornode;
   is part of the problem, run <command>supportconfig</command> on the
   affected node as well. For details on how to run
   <command>supportconfig</command>, please refer to
   <ulink
   url="http://www.suse.com/documentation/sles11/book_sle_admin/data/cha_adm_support.html"/>.
  </para>
  <sect2 id="sec.depl.trouble.support.ptf">
   <title>
    Applying PTFs (Program Temporary Fixes) Provided by the &suse; L3 Support
   </title>
   <para>
    Under certain circumstances, the &suse; support may provide temporary
    fixes, the so-called PTFs, to customers with an L3 support contract. These
    PTFs are provided as RPM packages. To make them available on all nodes in
    &cloud;, proceed as follows.
   </para>
   <procedure>
    <step>
     <para>
      Download the packages to a temporary location on the &admserv;.
     </para>
    </step>
    <step>
     <para>
      Move <quote>noarch</quote> packages (<filename>*.noarch.rpm</filename>)
      from the download location to
      <filename>/srv/tftpboot/repos/Cloud-PTF/rpm/noarch</filename> on the
      &admserv;.
     </para>
    </step>
    <step>
     <para>
      Move <quote>x86_64</quote> packages (<filename>*.x86_64.rpm</filename>)
      from the download location to
      <filename>/srv/tftpboot/repos/Cloud-PTF/rpm/x86_64</filename> on the
      &admserv;.
     </para>
    </step>
    <step>
     <para>
      Create or update the repository metadata:
     </para>
     <screen>createrepo /srv/tftpboot/repos/Cloud-PTF</screen>
    </step>
    <step>
     <para>
      Sign the repository metadata:
     </para>
     <screen>gpg -a --detach-sign /srv/tftpboot/repos/Cloud-PTF/repodata/repomd.xml</screen>
     <para>
      In case you are asked to overwrite the file, answer
      <guimenu>Yes</guimenu>.
     </para>
    </step>
    <step>
     <para>
      The repository is now set up and is available to all nodes in &cloud;
      except for the &admserv;. In case the PTF also contains packages to be
      installed on the &admserv;, you need to make the repository available on
      the &admserv; as well:
     </para>
     <screen>zypper ar -f /srv/tftpboot/repos/Cloud-PTF Cloud-PTF</screen>
    </step>
    <step>
     <para>
      To deploy the updates, proceed as described in <xref
      linkend="sec.depl.inst.nodes.post.updater"/>. Alternatively, run
      <command>zypper up</command> manually on each node.
     </para>
    </step>
   </procedure>
  </sect2>
 </sect1>
</chapter>
