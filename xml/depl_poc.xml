<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE appendix 
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<appendix xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.deploy.poc">
 <title>&cloud; Deployment Guide Questionnaire</title>
 <info>
<dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
    <dm:maintainer>fs</dm:maintainer>
    <dm:status>editing</dm:status>
    <dm:deadline/>
    <dm:priority/>
    <dm:translation>no</dm:translation>
    <dm:languages/>
</dm:docmanager>
</info>

<sect1 xml:id="sec.depl.poc.scope">
  <title>Document Scope</title>
  <para>
    This document will help to optimally prepare SUSE and their prospective customers for a Proof of Concept (PoC) deployment of &cloud;. Please note that this document does not replace any parts of the official documentation. This document is an addition to the SUSE OpenStack Deployment Guide (presently at
    <link xlink:href="https://www.suse.com/documentation/suse-openstack-cloud-7"/>) that provides specific details for the Proof of Concept implementation.
  </para>
</sect1>
<sect1 xml:id="sec.depl.poc.features">
  <title>&cloud; Key Features</title>
  <para>
    The latest version 7 of &cloud; supports all OpenStack Newton release components for best-in-class capabilities to deploy an open source private cloud.
  </para>
  <itemizedlist mark="bullet" spacing="normal">
    <listitem>
      <para>
	<emphasis>Installation Framework:</emphasis> Integration with the Crowbar project speeds up and simplifies installation and administration of your physical cloud infrastructure.
      </para>
    </listitem>
    <listitem>
      <para>
	<emphasis>Mixed Hypervisor Support:</emphasis> Enhanced virtualization management through support for multi-hypervisor environments that use KVM, Xen, Microsoft Hyper-V, VMware vSphere and IBM z/VM.
      </para>
    </listitem>
    <listitem>
      <para>
	<emphasis>High Availability:</emphasis> Automated deployment and configuration of control plane clusters. Ensures continuous access to business services and delivery of enterprise-grade SLAs.
      </para>
    </listitem>
    <listitem>
      <para>
	<emphasis>High availability for KVM and Xen Compute Nodes and Workloads:</emphasis> Enhanced support for critical workloads not designed for cloud architectures.
      </para>
    </listitem>
    <listitem>
      <para>
	<emphasis>Ceph:</emphasis> Integration with SUSE Enterprise Storage provides a streamlined deployment of a single solution for distributed block, object, and virtual machine image storage.
      </para>
    </listitem>
    <listitem>
      <para>
	<emphasis>Docker Support:</emphasis> Build and run innovative containerized application through Magum integration.
      </para>
    </listitem>
    <listitem>
      <para>
	<emphasis>Scalability:</emphasis> Cloud control system designed to grow with your demands.
      </para>
    </listitem>
    <listitem>
      <para>
	<emphasis>Open APIs:</emphasis> Using the standard APIs, customers can enhance and integrate OpenStack with third-party software.
      </para>
    </listitem>
    <listitem>
      <para>
	<emphasis>Block Storage Plug-Ins:</emphasis> Broad choice of from storage vendors such as EMC, NetApp and others.
      </para>
    </listitem>
    <listitem>
      <para>
	<emphasis>Networking Plug-Ins:</emphasis> Whilst natively supporting Open Source SDN’s via Open vSwitch harnessing the power of DPDK in &sls;  12 SP2 or third-party tools from Cisco, Midokura, Infoblox, Nuage Networks, PLUMgrid and even VLAN bridging solutions for flexibility.
      </para>
    </listitem>
    <listitem>
      <para>
	<emphasis>Award-winning Support:</emphasis> Backed by 24X7 worldwide-technical support.
      </para>
    </listitem>
    <listitem>
      <para>
	<emphasis>Full Integration into SUSE Update Processes:</emphasis> Easily maintain and patch cloud deployments.
      </para>
    </listitem>
    <listitem>
      <para>
	<emphasis>Non-disruptive Upgrade Capabilities:</emphasis> Ease migration to future SUSE OpenStack Cloud releases
      </para>
    </listitem>
  </itemizedlist>
</sect1>

<sect1 xml:id="sec.depl.poc.components">
    <title>Main Components<remark>dpopov 2016-11-11: Proof-of-Concept Deployment Guidelines</remark>
    POC Guide</title>
    <para>
      Components for setting up and managing SUSE OpenStack cloud include:
    </para>

    <para>
      <emphasis>Administration Server:</emphasis> The Administration Server provides all services needed to manage and deploy all other nodes in
the cloud. Most of these services are provided by the Crowbar tool that automates in conjunction with Chef all the required installation and configuration tasks. Among the services provided by the server are DHCP, DNS, NTP, PXE, TFTP.
    </para>

    <para>
      The Administration Server also hosts the software repositories for SUSE Linux Enterprise Server and SUSE OpenStack Cloud, since they are needed for node deployment. Optionally (if no other sources for the software repositories are available) it can also host the Subscription Management Tool (SMT), providing up-to-date repositories with updates and patches for all nodes.
    </para>

    <para>
      Control Nodes: The Control Node(s) hosts all OpenStack services needed to orchestrate virtual machines deployed on the Compute Nodes in the SUSE OpenStack Cloud. OpenStack on SUSE OpenStack Cloud uses a PostgreSQL database, which is also hosted on the Control Node(s). The following OpenStack components—if deployed—run on the Control Node(s):
    </para>

    <itemizedlist>
      <listitem>
	<para>PostgreSQL</para>
      </listitem>
      <listitem>
	<para>Image (Glance)</para>
      </listitem>
      <listitem>
	<para>Identity (Keystone)</para>
      </listitem>
      <listitem>
	<para>Networking (Neutron)</para>
      </listitem>
      <listitem>
	<para>Block Storage (Cinder)</para>
      </listitem>
      <listitem>
	<para>Shared Storage (Manila)</para>
      </listitem>
      <listitem>
	<para>OpenStack Dashboard</para>
      </listitem>
      <listitem>
	<para>Keystone</para>
      </listitem>
     <listitem>
	<para>Pacemaker</para>
      </listitem>
     <listitem>
	<para>Nova controller</para>
      </listitem>
     <listitem>
	<para>Message broker</para>
      </listitem>
     <listitem>
	<para>Swift proxy server</para>
      </listitem>
     <listitem>
	<para>Hawk monitor</para>
      </listitem>
     <listitem>
	<para>Heat an orchestration engine</para>
      </listitem>
     <listitem>
	<para>Ceilometer server and agents</para>
      </listitem>
     <listitem>
	<para>Trove a Database-as-a-Service</para>
      </listitem>
    </itemizedlist>

    <para>
      Being a central point in the SUSE OpenStack Cloud architecture that runs a lot of services, a single Control Node can quickly become a performance bottleneck, especially in large SUSE OpenStack Cloud deployments. It is possible to distribute the services listed above on more than one Control Node, up to a setup where each service runs on its own node.
    </para>

    <para>
      <emphasis>Compute Nodes:</emphasis> The Compute Nodes are the pool of machines on which the instances are running. These machines need to be equipped with a sufficient number of CPUs and enough RAM to start several instances. They also need to provide sufficient hard disk space.  The Control Node effectively distributes instances within the pool of Compute Nodes and provides the necessary network resources. The OpenStack service Compute (Nova) runs on the Compute Nodes and provides means for setting up, starting, and stopping virtual machines. SUSE OpenStack Cloud supports several hypervisors such as Hyper-V, KVM, VMware vSphere, Xen and Docker. Each image that can be started with an instance is bound to one hypervisor. Each Compute Node can only run one hypervisor at a time. For this PoC SUSE recommends to leverage KVM  as hypervisor of choice.
    </para> 

      <para>
	Optionally Storage Nodes: The Storage Nodes are the pool of machines providing object or block storage. Object storage supports several different backends.
      </para>
</sect1>
<sect1 xml:id="sec.depl.poc.objectives">
  <title>Proof of Concept Deployment Objectives</title>
  <para>
    {Every customers requirements in this will be their own, but take care to define 3-5 clearly defined objectives that are provable, measurable and have a specific time scale in which proof is required.  Flexibility in these objectives is encouraged but only through full agreement of both parties and a re-definition of these goals. Please note that is best to add an extension of proof requirements in this document so it remains a full record of the work completed.}
  </para>
  <sect2 xml:id="sec.depl.poc.preparations">
    <title>Proof of Concept Preparations</title>
    <para>
      This document is a summary of tasks and best-practices of &cloud; PoC. It is important that this document does not aim to replace any of the official documentations. This document is an addition to the publicly available Deployment Guide and shows specific details for the  preparations.
    </para>

    <para>
      Before deploying &cloud;, there are a few requirements to be met and considerations to be made. Some decisions need to be made <emphasis>before</emphasis> deploying SUSE OpenStack Cloud, since they <emphasis>cannot</emphasis> be changed afterward.
    </para>

    <para>
      This document provides preparation steps needed for the deployment of &cloud;. This includes software and hardware components for successful implementation.
    </para>
    <procedure>
      <title>Preparations Prior to PoC Deployment</title>
      <step>
	<para>
	  Hardware and VMs provided and setup
	</para>
      </step>
      <step>
	<para>
	  PXE boot from first NIC in BIOS enabled (legacy bios mode / no UEFI)
	</para>
      </step>
      <step>
	<para>
	  Hardware certified with SLES12 SP2
	</para>
      </step>
      <step>
	<para>
	  Booting from ISO works
	</para>
      </step>
      <step>
	<para>
	  All NICs are visible
	</para>
      </step>
      <step>
	<para>
	  Install sar/sysstat for performance troubleshooting
	</para>
      </step>
      <step>
	<para>
	  All needed subscription records are available. Depending on the size of the cloud to be implemented this includes:
	</para>
	<itemizedlist>
	  <listitem>
	    <para>
	      SUSE OpenStack Cloud  subscriptions
	    </para>
	  </listitem>
	  <listitem>
	    <para>
	      SUSE Linux Enteprise Server (SLAS)  subscriptions
	    </para>
	  </listitem>
	  <listitem>
	    <para>
	      SLES High Availability Extensions (HAE) subscriptions
	    </para>
	  </listitem>
	  <listitem>
	    <para>
	      Optionally: SUSE Enterprise Storage (SES) subscriptions
	    </para>
	  </listitem>
	</itemizedlist>
      </step>
      <step>
	<para>
	  All needed channels and updates are either locally or remotely available. Options to providing the repositories and channels are:
	</para>
	<itemizedlist>
	  <listitem>
	    <para>
	      Setup SMT server on the Administration Server  - this is an optional step
	    </para>
	  </listitem>
	  <listitem>
	    <para>
	      Use existing SMT server
	    </para>
	  </listitem>
	  <listitem>
	    <para>
	      Use of existing SUSE Manager
	    </para>
	  </listitem>
	</itemizedlist>
      </step>
      <step>
	<para>
	  Networking planed and wired according to Layout/Topology
	</para>
      </step>
      <step>
	<para>
	  If SUSE Enterprise Storage is to evaluated as well, all nodes should be installed, configured and appropriately tuned prior starting to install SUSE OpenStack Cloud 6. Storage services (Nova, Cinder, Glance, Cluster Stonith, …) needed by SUSE OpenStack Cloud 6 must be available and accessible.
	</para>
      </step>
      <step>
	<para>
	  <systemitem>Network.json</systemitem> is adjusted and customized to meet the needs - as this is the important tasks which need to be done in forehand and should be discussed in depth (see Network section for more details on Network.json and SUSE OpenStack Cloud 6 Deployment Guide)
	</para>
      </step>
    </procedure>
  </sect2>
</sect1>

<sect1 xml:id="sec.depl.poc.matrix">
  <title>Hardware and Software Matrix</title>
  <para>
    All machines are running SLES 12 SP1 Server Operating System and KVM or XEN Hypervisors must be running on bare-metal. The Hardware and software matrix used is detailed below. The Admin Node will (can) be deployed on a KVM / VMWare Virtual Machine.
  </para>

  <para>
This sizing recommendation includes admin node (bare-metal or VM), controller nodes, compute nodes to host all your OpenStack services, as well as the optional SES nodes.  Initiall this is based on real use cases and experiences made by SUSE Cloud-Team in the last 3 years. It also mentions the network equipments needed and including the minimum requirements on bandwidth.
  </para>

  <note>
    <para>
      Nonetheless these recommendations are just an example and by no means final. The final sizing decision depends on the real customer workloads and architecture which must be discussed in depth. The type and number of hardware components such as HDD, CPU, and RAM are also just placeholders for further discussion and evaluation depending on workloads.
     </para>
    </note>

    <table>
      <title>BoM/SUSE OpenStack Cloud Services</title>
      <tgroup cols="4">
	<colspec colnum="1" colname="1" colwidth="25*"/>
	<colspec colnum="2" colname="2" colwidth="25*"/>
	<colspec colnum="3" colname="3" colwidth="25*"/>
	<colspec colnum="4" colname="4" colwidth="25*"/>
	<thead>
	  <row>
	    <entry><para>Unit Nr.</para></entry>
	    <entry><para>Function</para></entry>
	    <entry><para>Configuration</para></entry>
	    <entry><para>OpenStack Component</para></entry>
	   </row>
	  </thead>
	  <tbody>
	    <row>
	      <entry><para>3x</para></entry>
	      <entry><para>Compute Nodes</para></entry>
	      <entry><para>2 x HDD</para>
	      <para>2 x Quad Core Intel/AMD Proc.</para>
	      <para>256GB Memory</para>
	      <para>2 (4) x 10Gb Ethernet NICs</para></entry>
	      <entry><para>Nova-multi-compute</para>
	      <para>ML2 Agent</para>
	      <para>OVS Agent</para></entry>
	     </row>
	    <row>
	      <entry><para>1</para></entry>
	      <entry><para>Admin Node Or VM</para></entry>
	      <entry><para>2x HDD</para>
	      <para>1x Quad Core Intel/AMD Proc.</para>
	      <para>8GB Memory</para>
	      <para>2 (4) x 10Gb Ethernet NICs</para></entry>
	      <entry><para>Crowbar, tftpboot, PXE</para>
	      </entry>
	     </row>
	    <row>
	      <entry><para>2x</para></entry>
	      <entry><para>Control Node</para></entry>
	      <entry><para>2x HDD</para>
	      <para>2x Quad Core Intel/AMD Proc.</para>
	      <para>2 x 64GB Memory</para>
	      <para>2 (4) x 10Gb Ethernet NICs</para></entry>
	      <entry><para>Horizon</para>
	      <para>Rabbit MQ</para>
	      <para>Nova Nova multi-controller</para>
	      <para>Cinder</para>
	      <para>Glance</para>
	      <para>Heat</para>
	      <para>Ceilometer</para>
	      <para>Neutron-Server</para>
	      <para>ML2 Agent</para>
	      <para>Keystone</para>
	      <para>PostgreSQL DB</para>
	      <para>Neutron ML2 Plugin</para>
	      <para>L2/L3 Agents</para>
	      <para>DHCP Agent</para>
	      </entry>
	     </row>
	    <row>
	      <entry><para></para></entry>
	      <entry><para>CloudFoundry</para></entry>
	      <entry><para>48 vCPUs</para>
	      <para>256GB Memory</para>
	      <para>Min 2TB Storage</para>
	      </entry>
	      <entry><para>Pivotal CloudFoundry</para>
	      </entry>
	     </row>
	    <row>
	      <entry><para>4x</para></entry>
	      <entry><para>Storage Server – SUSE Eenterprise Storage</para></entry>
	      <entry><para>2x HDD</para>
	      <para>2x  Quad Core Intel/AMD Proc.</para>
	      <para>64GB Memory</para>
	      <para>2 (4)x  10Gb Ethernet NICs </para>
	      </entry>
	      <entry><para>Admin – Server</para>
	      <para>MON - Server</para>
	      <para>OSD - Server</para>
	      </entry>
	    </row>
	    <row>
	      <entry><para>1</para></entry>
	      <entry><para>Switch min. 10 GbE ports</para></entry>
	      <entry><para></para></entry>
	      <entry><para>All VLAs/Tagged or Untagged</para>
	      </entry>
	    </row>
	    <row>
	      <entry align="center" namest="1" nameend="4">OS: SLES12 SP1</entry>
	    </row>
	    <row>
	      <entry><para>DHCP, DNS</para></entry>
	      <entry><para>Isolated within Admin Network</para></entry>
	      <entry><para></para>
	      </entry>
	    </row>
	    <row>
	      <entry><para>HA</para></entry>
	      <entry><para>2x Control Nodes</para></entry>
	      <entry><para></para>
	      </entry>
	    </row>
	    </tbody>
	   </tgroup>
	  </table>
	  

</sect1>
<sect1 xml:id="sec.depl.poc.topology">
  <title>Network Topology</title>
  <para>
    Configuring and managing your network are <emphasis>two of the most challenging aspects of deploying an &cloud;</emphasis> and need to be planned carefully. Just as OpenStack provides flexibility and agility for compute and storage, SDN in OpenStack gives cloud administrators more control over their networks. However, building and manually configuring the virtual network infrastructure for OpenStack are difficult and error prone. SUSE OpenStack Cloud removes this barrier by delivering a structured installation process for OpenStack which could be customized to adapt the real environment.
  </para>
<sect2>
  <title>The network.json Network Control-File</title>
  <para>
    The deployment of the network configuration is done while setting up the admin node. As a requirement for the deployment, the whole network configuration has to be entered in a file called 'network.json'.
  </para>

  <para>
    The Crowbar network barclamp provides two functions for the system. The first is a common role to instantiate network interfaces on the Crowbar managed systems. The other function is address pool management. While the addresses can be managed with the YaST Crowbar module, complex network setups require to manually edit the network barclamp template file <systemitem>/etc/crowbar/network.json</systemitem>. For more detailed explanation and description see <link xlink:href="https://www.suse.com/documentation/suse-openstack-cloud-6/pdfdoc/book_cloud_deploy/book_cloud_deploy.pdf#page=240&amp;zoom=auto,63.779,788.031" />
  </para>
  <para>
    The network definitions contain IP address assignments, the bridge and VLAN setup and settings for the router preference. Each network is also assigned to a logical interface. These VLAN IDs and Networks can be customized to meet the needs of the customer environment. 
  </para>
</sect2>
<sect2>
  <title>The Network Mode</title>
  <para>
    SUSE Cloud supports different network modes: single, dual and teaming. As of SUSE Cloud 6, the networking mode is applied to all nodes and the Administration Server. That means that all machines need to meet the hardware requirements for the chosen mode. The following network modes are available:
  </para>
  <itemizedlist>
    <listitem>
      <para>
	Single Network Mode - In single mode one Ethernet card is used for all the traffic.
      </para>
    </listitem>
    <listitem>
      <para>
	Dual Network Mode – Dual mode needs two Ethernet cards (on all nodes but Administration Server) and allows to completely separate traffic to/from the Admin Network and to/from the public network.
      </para>
    </listitem>
    <listitem>
      <para>
	Teaming Network Mode – The teaming mode is almost identical to single mode, except that it is combines several Ethernet cards to a "bond" (network device bonding). Teaming mode needs two or more Ethernet cards.
      </para>
    </listitem>
  </itemizedlist>
  <note>
    <para>
      If a HA setup for &cloud; is planed, a teaming network mode is required and please doublecheck to make sure to deploy SUSE OpenStack Cloud with teaming network mode.
     </para>
   </note>
</sect2>
<sect2>
  <title>Default Layout</title>
  <para>
    The following networks are pre-defined when setting up SUSE OpenStack Cloud. The IP addresses listed are the default addresses and can be changed using YaST Crowbar module or by editing a JSON-File. It is also possible to completely customize the network setup suitable for your environment. This requires to manually edit the network barclamp template.
  </para>
  <table>
    <title>Admin  Network Layout</title>
    <tgroup cols="3">
      <colspec colnum="1" colname="1" colwidth="33*"/>
      <colspec colnum="2" colname="2" colwidth="33*"/>
      <colspec colnum="3" colname="3" colwidth="33*"/>
      <thead>
	<row>
	  <entry><para>Network Name</para></entry>
	  <entry><para>VLAN</para></entry>
	  <entry><para>IP Range</para></entry>
	 </row>
	</thead>
	<tbody>
	  <row>
	    <entry><para>Router</para></entry>
	    <entry><para>No - untagged</para></entry>
	    <entry><para>192.168.124.1</para></entry>
	   </row>
	  <row>
	    <entry><para>Admin</para></entry>
	    <entry><para>No - untagged</para></entry>
	    <entry><para>192.168.124.10 – 192.168.124.11</para></entry>
	   </row>
	  <row>
	    <entry><para>DHCP</para></entry>
	    <entry><para>No - untagged</para></entry>
	    <entry><para>192.168.124.21 – 192.168.124.80</para></entry>
	   </row>
	  <row>
	    <entry><para>Host</para></entry>
	    <entry><para>No - untagged</para></entry>
	    <entry><para>192.168.124.81 – 192.168.124.160</para></entry>
	   </row>
	  <row>
	    <entry><para>bmc vlan host</para></entry>
	    <entry><para>100</para></entry>
	    <entry><para>192.168.124.61</para></entry>
	   </row>
	  <row>
	    <entry><para>bmc host</para></entry>
	    <entry><para>No - untagged</para></entry>
	    <entry><para>192.168.124.162 – 192.168.124.240</para></entry>
	   </row>
	  <row>
	    <entry><para>Switch</para></entry>
	    <entry><para>No - untagged</para></entry>
	    <entry><para>192.168.124.241 – 192.168.124.250</para></entry>
	   </row>
	  </tbody>
	 </tgroup>
	</table>
	
	<table>
	  <title>Private Network Layout</title>
	  <tgroup cols="3">
	    <colspec colnum="1" colname="1" colwidth="33*"/>
	    <colspec colnum="2" colname="2" colwidth="33*"/>
	    <colspec colnum="3" colname="3" colwidth="33*"/>
	    <thead>
	      <row>
		<entry><para>Network Name</para></entry>
		<entry><para>VLAN</para></entry>
		<entry><para>IP Range</para></entry>
	</row>
</thead>
<tbody>
  <row>
    <entry><para>Router</para></entry>
    <entry><para>500</para></entry>
    <entry><para>192.168.123.1 – 192.168.123.49</para></entry>
   </row>
<row>
    <entry><para>DHCP</para></entry>
    <entry><para>500</para></entry>
    <entry><para>192.168.123.50 – 192.168.123.254</para></entry>
   </row>
  </tbody>
 </tgroup>
</table>

<table>
  <title>Public / Nova Floating Network Layout / Externally Provided</title>
  <tgroup cols="3">
    <colspec colnum="1" colname="1" colwidth="33*"/>
    <colspec colnum="2" colname="2" colwidth="33*"/>
    <colspec colnum="3" colname="3" colwidth="33*"/>
    <thead>
      <row>
	<entry><para>Network Name</para></entry>
	<entry><para>VLAN</para></entry>
	<entry><para>IP Range</para></entry>
</row></thead>
<tbody>
  <row>
    <entry><para>Public Host</para></entry>
    <entry><para>300</para></entry>
    <entry><para>192.168.126.2 – 192.168.126.49</para></entry>
   </row>
<row>
    <entry><para>Public DHCP</para></entry>
    <entry><para>300</para></entry>
    <entry><para>192.168.126.50 – 192.168.126.127</para></entry>
   </row>
<row>
    <entry><para>Floating Host</para></entry>
    <entry><para>300</para></entry>
    <entry><para>192.168.126.129 – 192.168.126.191</para></entry>
   </row>
  </tbody>
 </tgroup>
</table>

<table>
  <title>Storage Network Layout</title>
  <tgroup cols="3">
    <colspec colnum="1" colname="1" colwidth="33*"/>
    <colspec colnum="2" colname="2" colwidth="33*"/>
    <colspec colnum="3" colname="3" colwidth="33*"/>
    <thead>
      <row>
	<entry><para>Network Name</para></entry>
	<entry><para>VLAN</para></entry>
	<entry><para>IP Range</para></entry>
</row></thead>
<tbody>
  <row>
    <entry><para>Host</para></entry>
    <entry><para>200</para></entry>
    <entry><para>192.168.125.2 – 192.168.125.254</para></entry>
   </row>
  </tbody>
 </tgroup>
</table>


</sect2>
</sect1>
<sect1 xml:id="sec.depl.poc.architecture">
  <title>Network Overall Architecture</title>
  <para>
    SUSE OpenStack Cloud requires a complex network setup consisting of several networks that are configured during installation. These networks are for exclusive cloud usage. To access them from an existing network, a router is needed.
  </para>
  <important>
    <para>
      The network configuration on the nodes in the &cloud; network is entirely controlled by Crowbar. Any network configuration not done with Crowbar will automatically be overwritten. After the cloud is deployed, network settings cannot be changed without reinstalling the Cloud.
    </para>
  </important>
  <para>
    Controller Node: This node serves as the front end for API calls to the compute, image, volume, network and orchestration services. This node also hosts multiple neutron plugins and agents and aggregates all route traffic within tenant as well as between tenant network and outside world.
  </para>
  <para>
    Compute node: Creates virtual machines on-demand using choosen hypervisor for customer application.
  </para>

  <para>
    Admin Node: Automates the installation processes via Crowbar using pre-defined cook-books for configuring and deploying the Control Node as well as Compute and Network Nodes.
  </para>

  <note>
    <para>
      The Admin Node needs a dedicated local and isolated DHCP/PXE environment, which is controlled by crowbar.
    </para>
  </note>

  <para>
    Optional Storage Access: We use Cinder for block storage access exposed through iSCSI or NFS (FC connection is not supported in the current release of OpenStack). This could also be a dedicated Storage Node.
  </para>
  <note>
    <para>
      Implementation of ceph as a cloud storage will require high performance network. The “Storage Net” in the following network topology is just a place holder for upcoming ceph implementation (for use with an established 40GbE network). We recommend running a Ceph Storage Cluster with two networks: a public network and a cluster  network. To support two networks, each ceph node will need to have more than one NIC installed.
    </para>
  </note>
  <para>
    Network Mode: For the Proof of Concept which network mode to leveraged will depend on the High Availability (HA) requirements. For a HA setup for SUSE OpenStack Cloud the teaming network mode is required.
  </para>
<sect2>
  <title>Network Architecture: Pre-Defined VLANs</title>
  <para>
    If you need VLAN support for the admin network, it must be handled at switch level. The following networks are pre-defined when setting up SUSE Cloud. The IP addresses listed are the default addresses and can be changed using the YaST Crowbar module. It is also possible to completely customize the network setup. 
  </para>
  <note>
    <title>Limitations of the Default Network Proposal</title>
    <para>
      The default network proposal as described below limits the maximum number of Compute Nodes to 80, the maximum number of floating IP addresses to 61 and the maximum number of addresses in the nova_fixed network to 204. To overcome these limitations you need to reconfigure the network setup by using appropriate address ranges manually.
    </para>
  </note>
<sect3>
  <title>Pre-Defined Networks and VLANs</title>
  <variablelist>
    <varlistentry>
      <term>
	Admin Network (192.168.124/24)
      </term>
      <listitem>
	<para>
	  A private network to access the Administration Server and all nodes for administration purposes. The default setup lets you also access the BMC (Baseboard Management Controller) data via IPMI (Intelligent Platform Management Interface) from this network. If required, BMC access can be swapped to a separate network. 

You have the following options for controlling access to this network:
do not allow access from the outside and keep the admin network completely separated allow access to the Administration Server from a single network (for example, your company's administration network) via the “bastion network” option configured on an additional network card with a fixed IP address allow access from one or more networks via a gateway
	</para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>
	Storage Network (192.168.125/24)
      </term>
      <listitem>
	<para>
	  Private, SUSE OpenStack Cloud internal virtual network. This network is used by Ceph and Swift only. It should not be accessed by users.
	</para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>
	Private Network (nova-fixed, 192.168.123/24)
      </term>
      <listitem>
	<para>
	  Private, SUSE OpenStack Cloud internal virtual network. This network is used for inter-instance communication and provides access to the outside world for the instances. The gateway required is also automatically provided by SUSE OpenStack Cloud.
	</para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>
	Public Network (nova-floating, public, 192.168.126/24)
      </term>
      <listitem>
	<para>
	  The only public network provided by SUSE OpenStack Cloud. You can access the Nova Dashboard and all instances (provided they have been equipped with a floating IP) on this network. This network can only be accessed via a gateway, which needs to be provided
externally. All SUSE OpenStack Cloud users and administrators need to be able to access the public network.
	</para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>
	Software Defined Network (os_sdn, 192.168.130/24)
      </term>
      <listitem>
	<para>
	  Private, SUSE OpenStack Cloud internal virtual network. This network is used when Neutron is configured to use openvswitch with GRE tunneling for the virtual networks. It should not be accessed by users.
	</para>

	<para>
	  SUSE OpenStack Cloud supports different network modes: single, dual, and teaming. As of SUSE OpenStack Cloud 6, the networking mode is applied to all nodes and the Administration Server. That means that all machines need to meet the hardware requirements for the chosen mode. The network mode can be configured using the YaST Crowbar module (Chapter 7, Crowbar Setup). The network mode cannot be changed after the cloud is deployed.
Other, more flexible network mode setups can be configured by manually editing the Crowbar network configuration files. See Appendix D, The Network Barclamp Template File for more information. SUSE or a partner can assist you in creating a custom setup within the scope of a consulting services agreement (see http://www.suse.com/consulting/ for more information on SUSE consulting).
	</para>
      </listitem>
    </varlistentry>
  </variablelist>
  <important>
    <para>
      Teaming Network Mode is Required for HA Teaming network mode is required for an HA setup of SUSE OpenStack Cloud. If you are planning to move your cloud to an HA setup at a later point in time, make sure to deploy SUSE OpenStack Cloud with teaming network mode from the beginning. Otherwise a migration to an HA setup is not supported.
    </para>
  </important>
</sect3>
</sect2>
</sect1>
<sect1 xml:id="sec.depl.poc.services">
  <title>Services Architecture</title>
  <para>
    &cloud; is based on SUSE Linux Enterprise Server 12 SP2, OpenStack, Crowbar and Chef. &sls; is used as the underlying operating system for all infrastructure nodes.  Crowbar and Chef are used to automatically deploy and manage the OpenStack nodes from a central Administration Server.
  </para>
</sect1>
<sect1 xml:id="sec.depl.poc.testcases">
  <title>Proof of Concept Test Cases</title>
  <para>
    Once we successfully deployed OpenStack, we then need to test the environment by using either the Dashboard or the command line interface. This document provides the most important procedures and steps to perform functional tests agreed upon. A detailed list of test-case should be provided along side this document. 
  </para>
  <note>
    <title>About Test Cases</title>
    <para>
      All test-cases are work on progress and by no means complete. Test-cases need to be worked out by the whole team depending on the requirements and type of workloads. 
    </para>
    </note>
</sect1>
</appendix>
