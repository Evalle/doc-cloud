<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE appendix 
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<appendix xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.deploy.poc">
 <title>&cloud; Deployment Guide Questionnaire</title>
 <info>
<dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
    <dm:maintainer>fs</dm:maintainer>
    <dm:status>editing</dm:status>
    <dm:deadline/>
    <dm:priority/>
    <dm:translation>no</dm:translation>
    <dm:languages/>
</dm:docmanager>
</info>

<sect1 xml:id="sec.depl.poc.scope">
  <title>Document Scope</title>
  <para>
    This document will help to optimally prepare SUSE and their prospective customers for Proof of Concept on &cloud;. Please note  that this document does not aim to replace any of the official documentations. This document is an addition to the general SUSE OpenStack Deployment Guide (presently at
    <link xlink:href="https://www.suse.com/documentation/suse-openstack-cloud-7"/>) which provides specific details for the Proof of Concept (PoC) implementation.
  </para>
</sect1>
<sect1 xml:id="sec.depl.poc.features">
  <title>&cloud; Key Features</title>
  <para>
    The latest version of &cloud; is 7 supports all OpenStack Newton release components for best-in-class capabilities to deploy an open source, private cloud.
  </para>
  <itemizedlist mark="bullet" spacing="normal">
    <listitem>
      <para>
	<emphasis>Installation Framework:</emphasis> Integration with the Crowbar project speeds up and simplifies installation and administration of your physical cloud infrastructure.
      </para>
    </listitem>
    <listitem>
      <para>
	<emphasis>Mixed Hypervisor Support:</emphasis> Enhanced virtualization management through support for multi-hypervisor environments that use KVM, Xen, Microsoft Hyper-V, VMware vSphere and IBM z/VM.
      </para>
    </listitem>
    <listitem>
      <para>
	<emphasis>High Availability:</emphasis> Automated deployment and configuration of control plane clusters. Ensures continuous access to business services and delivery of enterprise-grade SLAs.
      </para>
    </listitem>
    <listitem>
      <para>
	<emphasis>High availability for KVM / Xen Compute Nodes and Workloads:</emphasis> Enhanced support for critical workloads not designed for cloud architectures.
      </para>
    </listitem>
    <listitem>
      <para>
	<emphasis>Ceph:</emphasis> Integration with SUSE Enterprise Storage provides a streamlined deployment of a single solution for distributed block, object and virtual machine image storage.
      </para>
    </listitem>
    <listitem>
      <para>
	<emphasis>Docker Support:</emphasis> Build and run innovative containerized application through Magum integration.
      </para>
    </listitem>
    <listitem>
      <para>
	<emphasis>Scalability:</emphasis> Cloud control system designed to grow with your demands.
      </para>
    </listitem>
    <listitem>
      <para>
	<emphasis>Open APIs:</emphasis> Using the standard APIs, customers can enhance and integrate OpenStack with third-party software.
      </para>
    </listitem>
    <listitem>
      <para>
	<emphasis>Block Storage Plug-Ins:</emphasis> Broad choice of from storage vendors such as EMC, NetApp and others.
      </para>
    </listitem>
    <listitem>
      <para>
	<emphasis>Networking Plug-Ins:</emphasis> Whilst natively supporting Open Source SDN’s via Open vSwitch harnessing the power of DPDK in &sls;  12 SP2 or third-party tools from Cisco, Midokura, Infoblox, Nuage Networks, PLUMgrid and even VLAN bridging solutions for flexibility.
      </para>
    </listitem>
    <listitem>
      <para>
	<emphasis>Award-winning Support:</emphasis> Backed by 24X7 worldwide-technical support.
      </para>
    </listitem>
    <listitem>
      <para>
	<emphasis>Full Integration into SUSE Update Processes:</emphasis> Easily maintain and patch cloud deployments.
      </para>
    </listitem>
    <listitem>
      <para>
	<emphasis>Non-disruptive Upgrade Capabilities:</emphasis> Ease migration to future SUSE OpenStack Cloud releases
      </para>
    </listitem>
  </itemizedlist>
</sect1>

<sect1 xml:id="sec.depl.poc.components">
    <title>Main Components<remark>dpopov 2016-11-11: Proof-of-Concept Deployment Guidelines</remark>
    POC Guide</title>
    <para>
      Components for setting up and managing SUSE OpenStack cloud include:
    </para>

    <para>
      <emphasis>Administration Server:</emphasis> The Administration Server provides all services needed to manage and deploy all other nodes in
the cloud. Most of these services are provided by the Crowbar tool that automates in conjunction with Chef all the required installation and configuration tasks. Among the services provided by the server are DHCP, DNS, NTP, PXE, TFTP.
    </para>

    <para>
      The Administration Server also hosts the software repositories for SUSE Linux Enterprise Server and SUSE OpenStack Cloud, since they are needed for node deployment. Optionally (if no other sources for the software repositories are available) it can also host the Subscription Management Tool (SMT), providing up-to-date repositories with updates and patches for all nodes.
    </para>

    <para>
      Control Nodes: The Control Node(s) hosts all OpenStack services needed to orchestrate virtual machines deployed on the Compute Nodes in the SUSE OpenStack Cloud. OpenStack on SUSE OpenStack Cloud uses a PostgreSQL database, which is also hosted on the Control Node(s). The following OpenStack components—if deployed—run on the Control Node(s):
    </para>

    <itemizedlist>
      <listitem>
	<para>PostgreSQL</para>
      </listitem>
      <listitem>
	<para>Image (Glance)</para>
      </listitem>
      <listitem>
	<para>Identity (Keystone)</para>
      </listitem>
      <listitem>
	<para>Networking (Neutron)</para>
      </listitem>
      <listitem>
	<para>Block Storage (Cinder)</para>
      </listitem>
      <listitem>
	<para>Shared Storage (Manila)</para>
      </listitem>
      <listitem>
	<para>OpenStack Dashboard</para>
      </listitem>
      <listitem>
	<para>Keystone</para>
      </listitem>
     <listitem>
	<para>Pacemaker</para>
      </listitem>
     <listitem>
	<para>Nova controller</para>
      </listitem>
     <listitem>
	<para>Message broker</para>
      </listitem>
     <listitem>
	<para>Swift proxy server</para>
      </listitem>
     <listitem>
	<para>Hawk monitor</para>
      </listitem>
     <listitem>
	<para>Heat an orchestration engine</para>
      </listitem>
     <listitem>
	<para>Ceilometer server and agents</para>
      </listitem>
     <listitem>
	<para>Trove a Database-as-a-Service</para>
      </listitem>
    </itemizedlist>

    <para>
      Being a central point in the SUSE OpenStack Cloud architecture that runs a lot of services, a single Control Node can quickly become a performance bottleneck, especially in large SUSE OpenStack Cloud deployments. It is possible to distribute the services listed above on more than one Control Node, up to a setup where each service runs on its own node.
    </para>

    <para>
      <emphasis>Compute Nodes:</emphasis> The Compute Nodes are the pool of machines on which the instances are running. These machines need to be equipped with a sufficient number of CPUs and enough RAM to start several instances. They also need to provide sufficient hard disk space.  The Control Node effectively distributes instances within the pool of Compute Nodes and provides the necessary network resources. The OpenStack service Compute (Nova) runs on the Compute Nodes and provides means for setting up, starting, and stopping virtual machines. SUSE OpenStack Cloud supports several hypervisors such as Hyper-V, KVM, VMware vSphere, Xen and Docker. Each image that can be started with an instance is bound to one hypervisor. Each Compute Node can only run one hypervisor at a time. For this PoC SUSE recommends to leverage KVM  as hypervisor of choice.
    </para> 

      <para>
	Optionally Storage Nodes: The Storage Nodes are the pool of machines providing object or block storage. Object storage supports several different backends.
      </para>
</sect1>
<sect1 xml:id="sec.depl.poc.objectives">
  <title>Proof of Concept Deployment Objectives</title>
  <para>
    {Every customers requirements in this will be their own, but take care to define 3-5 clearly defined objectives that are provable, measurable and have a specific time scale in which proof is required.  Flexibility in these objectives is encouraged but only through full agreement of both parties and a re-definition of these goals. Please note that is best to add an extension of proof requirements in this document so it remains a full record of the work completed.}
  </para>
  <sect2 xml:id="sec.depl.poc.preparations">
    <title>Proof of Concept Preparations</title>
    <para>
      This document is a summary of tasks and best-practices of &cloud; PoC. It is important that this document does not aim to replace any of the official documentations. This document is an addition to the publicly available Deployment Guide and shows specific details for the  preparations.
    </para>

    <para>
      Before deploying &cloud;, there are a few requirements to be met and considerations to be made. Some decisions need to be made <emphasis>before</emphasis> deploying SUSE OpenStack Cloud, since they <emphasis>cannot</emphasis> be changed afterward.
    </para>

    <para>
      This document provides preparation steps needed for the deployment of &cloud;. This includes software and hardware components for successful implementation.
    </para>
    <procedure>
      <title>Preparations Prior to PoC Deployment</title>
      <step>
	<para>
	  Hardware and VMs provided and setup
	</para>
      </step>
      <step>
	<para>
	  PXE boot from first NIC in BIOS enabled (legacy bios mode / no UEFI)
	</para>
      </step>
      <step>
	<para>
	  Hardware certified with SLES12 SP2
	</para>
      </step>
      <step>
	<para>
	  Booting from ISO works
	</para>
      </step>
      <step>
	<para>
	  All NICs are visible
	</para>
      </step>
      <step>
	<para>
	  Install sar/sysstat for performance troubleshooting
	</para>
      </step>
      <step>
	<para>
	  All needed subscription records are available. Depending on the size of the cloud to be implemented this includes:
	</para>
	<itemizedlist>
	  <listitem>
	    <para>
	      SUSE OpenStack Cloud  subscriptions
	    </para>
	  </listitem>
	  <listitem>
	    <para>
	      SUSE Linux Enteprise Server (SLAS)  subscriptions
	    </para>
	  </listitem>
	  <listitem>
	    <para>
	      SLES High Availability Extensions (HAE) subscriptions
	    </para>
	  </listitem>
	  <listitem>
	    <para>
	      Optionally: SUSE Enterprise Storage (SES) subscriptions
	    </para>
	  </listitem>
	</itemizedlist>
      </step>
      <step>
	<para>
	  All needed channels and updates are either locally or remotely available. Options to providing the repositories and channels are:
	</para>
	<itemizedlist>
	  <listitem>
	    <para>
	      Setup SMT server on the Administration Server  - this is an optional step
	    </para>
	  </listitem>
	  <listitem>
	    <para>
	      Use existing SMT server
	    </para>
	  </listitem>
	  <listitem>
	    <para>
	      Use of existing SUSE Manager
	    </para>
	  </listitem>
	</itemizedlist>
      </step>
      <step>
	<para>
	  Networking planed and wired according to Layout/Topology
	</para>
      </step>
      <step>
	<para>
	  If SUSE Enterprise Storage is to evaluated as well, all nodes should be installed, configured and appropriately tuned prior starting to install SUSE OpenStack Cloud 6. Storage services (Nova, Cinder, Glance, Cluster Stonith, …) needed by SUSE OpenStack Cloud 6 must be available and accessible.
	</para>
      </step>
      <step>
	<para>
	  <systemitem>Network.json</systemitem> is adjusted and customized to meet the needs - as this is the important tasks which need to be done in forehand and should be discussed in depth (see Network section for more details on Network.json and SUSE OpenStack Cloud 6 Deployment Guide)
	</para>
      </step>
    </procedure>
  </sect2>
</sect1>

<sect1 xml:id="sec.depl.poc.matrix">
  <title>Hardware and Software Matrix</title>
  <para>
    All machines are running SLES 12 SP1 Server Operating System and KVM or XEN Hypervisors must be running on bare-metal. The Hardware and software matrix used is detailed below. The Admin Node will (can) be deployed on a KVM / VMWare Virtual Machine.
  </para>

  <para>
This sizing recommendation includes admin node (bare-metal or VM), controller nodes, compute nodes to host all your OpenStack services, as well as the optional SES nodes.  Initiall this is based on real use cases and experiences made by SUSE Cloud-Team in the last 3 years. It also mentions the network equipments needed and including the minimum requirements on bandwidth.
  </para>

  <note>
    <para>
      Nonetheless these recommendations are just an example and by no means final. The final sizing decision depends on the real customer workloads and architecture which must be discussed in depth. The type and number of hardware components such as HDD, CPU, and RAM are also just placeholders for further discussion and evaluation depending on workloads.
     </para>
    </note>
</sect1>
<sect1 xml:id="sec.depl.poc.topology">
  <title>Network Topology</title>
  <para>
    Configuring and managing your network are <emphasis>two of the most challenging aspects of deploying an &cloud;</emphasis> and need to be planned carefully. Just as OpenStack provides flexibility and agility for compute and storage, SDN in OpenStack gives cloud administrators more control over their networks. However, building and manually configuring the virtual network infrastructure for OpenStack are difficult and error prone. SUSE OpenStack Cloud removes this barrier by delivering a structured installation process for OpenStack which could be customized to adapt the real environment.
  </para>
<sect2>
  <title>The network.json Network Control-File</title>
  <para>
    The deployment of the network configuration is done while setting up the admin node. As a requirement for the deployment, the whole network configuration has to be entered in a file called 'network.json'.
  </para>

  <para>
    The Crowbar network barclamp provides two functions for the system. The first is a common role to instantiate network interfaces on the Crowbar managed systems. The other function is address pool management. While the addresses can be managed with the YaST Crowbar module, complex network setups require to manually edit the network barclamp template file <systemitem>/etc/crowbar/network.json</systemitem>. For more detailed explanation and description see <link xlink:href="https://www.suse.com/documentation/suse-openstack-cloud-6/pdfdoc/book_cloud_deploy/book_cloud_deploy.pdf#page=240&amp;zoom=auto,63.779,788.031" />
  </para>
  <para>
    The network definitions contain IP address assignments, the bridge and VLAN setup and settings for the router preference. Each network is also assigned to a logical interface. These VLAN IDs and Networks can be customized to meet the needs of the customer environment. 
  </para>
</sect2>
<sect2>
  <title>The Network Mode</title>
  <para>
    SUSE Cloud supports different network modes: single, dual and teaming. As of SUSE Cloud 6, the networking mode is applied to all nodes and the Administration Server. That means that all machines need to meet the hardware requirements for the chosen mode. The following network modes are available:
  </para>
  <itemizedlist>
    <listitem>
      <para>
	Single Network Mode - In single mode one Ethernet card is used for all the traffic.
      </para>
    </listitem>
    <listitem>
      <para>
	Dual Network Mode – Dual mode needs two Ethernet cards (on all nodes but Administration Server) and allows to completely separate traffic to/from the Admin Network and to/from the public network.
      </para>
    </listitem>
    <listitem>
      <para>
	Teaming Network Mode – The teaming mode is almost identical to single mode, except that it is combines several Ethernet cards to a "bond" (network device bonding). Teaming mode needs two or more Ethernet cards.
      </para>
    </listitem>
  </itemizedlist>
  <note>
    <para>
      If a HA setup for &cloud; is planed, a teaming network mode is required and please doublecheck to make sure to deploy SUSE OpenStack Cloud with teaming network mode.
     </para>
   </note>
</sect2>
<sect2>
  <title>Default Layout</title>
  <para>
    The following networks are pre-defined when setting up SUSE OpenStack Cloud. The IP addresses listed are the default addresses and can be changed using YaST Crowbar module or by editing a JSON-File. It is also possible to completely customize the network setup suitable for your environment. This requires to manually edit the network barclamp template.
  </para>
</sect2>
</sect1>
<sect1 xml:id="sec.depl.poc.architecture">
  <title>Network Overall Architecture</title>
  <para>
    SUSE OpenStack Cloud requires a complex network setup consisting of several networks that are configured during installation. These networks are for exclusive cloud usage. To access them from an existing network, a router is needed.
  </para>
  <important>
    <para>
      The network configuration on the nodes in the &cloud; network is entirely controlled by Crowbar. Any network configuration not done with Crowbar will automatically be overwritten. After the cloud is deployed, network settings cannot be changed without reinstalling the Cloud.
    </para>
  </important>
  <para>
    Controller Node: This node serves as the front end for API calls to the compute, image, volume, network and orchestration services. This node also hosts multiple neutron plugins and agents and aggregates all route traffic within tenant as well as between tenant network and outside world.
  </para>
  <para>
    Compute node: Creates virtual machines on-demand using choosen hypervisor for customer application.
  </para>

  <para>
    Admin Node: Automates the installation processes via Crowbar using pre-defined cook-books for configuring and deploying the Control Node as well as Compute and Network Nodes.
  </para>

  <note>
    <para>
      The Admin Node needs a dedicated local and isolated DHCP/PXE environment, which is controlled by crowbar.
    </para>
  </note>

  <para>
    Optional Storage Access: We use Cinder for block storage access exposed through iSCSI or NFS (FC connection is not supported in the current release of OpenStack). This could also be a dedicated Storage Node.
  </para>
  <note>
    <para>
      Implementation of ceph as a cloud storage will require high performance network. The “Storage Net” in the following network topology is just a place holder for upcoming ceph implementation (for use with an established 40GbE network). We recommend running a Ceph Storage Cluster with two networks: a public network and a cluster  network. To support two networks, each ceph node will need to have more than one NIC installed.
    </para>
  </note>
</sect1>
</appendix>
