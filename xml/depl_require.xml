<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet
 href="urn:x-daps:xslt:profiling:novdoc-profile.xsl" 
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "IGNORE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.depl.req">
 <title>Considerations and Requirements</title>
 <abstract>
  <para>
   Before deploying &productname;, there are a few requirements to be met
   and considerations to be made. Make sure to thoroughly read this
   chapter&mdash;some decisions need to be made <emphasis>before</emphasis>
   deploying &cloud;, since you cannot change them afterwards.
  </para>
 </abstract>
 <sect1 id="sec.depl.req.network">
  <title>Network</title>

  <para>
   &productname; requires a complex network setup consisting of several
   networks that are configured during installation. These networks are for
   exclusive cloud usage. In order to access them from an existing network,
   a router is needed.
  </para>

  <para>
   The network configuration on the nodes in the &cloud; network is entirely
   controlled by &crow;. Any network configuration not done with &crow;
   (e.g. with &yast;) will automatically be overwritten. Once the cloud is
   deployed, network settings cannot be changed anymore!
  </para>

  <figure>
   <title>&cloud; Network: Overview</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="cloud_network_overview.pdf" width="90%" format="PDF"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="cloud_network_overview.png" width="75%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The following networks are pre-defined when setting up &productname;. The IP
   addresses listed are the default addresses and can be changed using the
   &yast; &crow; module (see
   <xref
   linkend="sec.depl.inst.admserv.os.crowbar"/>). It's also possible to
   completely customize the network setup. This requires to manually edit the
   network &barcl; template. See <xref linkend="app.deploy.network_json"/> for
   detailed instructions.
  </para>

  <variablelist>
   <varlistentry>
    <term>
     Admin Network (<systemitem
     class="etheraddress">192.168.124/24</systemitem>)
    </term>
    <listitem>
     <para>
      A private network to access the &admserv; and all nodes for
      administration purposes. The default setup lets you also access the
      BMC (Baseboard Management Controller) data via IPMI (Intelligent
      Platform Management Interface) from this network. If required, BMC
      access can be swapped to a separate network.
     </para>
     <para>
      To access this network, you have the following options:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        do not allow access from the outside and keep the admin network
        completely separated
       </para>
      </listitem>
      <listitem>
       <para>
        allow access from a single network (e.g. your company's
        administration network) via the <quote>bastion network</quote>
        option configured on an additional network card with a fixed IP
        address
       </para>
      </listitem>
      <listitem>
       <para>
        allow access from one or more networks via a gateway
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     Storage Network (<systemitem
     class="etheraddress">192.168.125/24</systemitem>)
    </term>
    <listitem>
     <para>
      Private, &cloud; internal virtual network. This network is used by
      &ceph;, and &swift;, only. It should not be accessed by users.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     Private Network (nova-fixed, <systemitem
     class="etheraddress">192.168.123/24</systemitem>)
    </term>
    <listitem>
     <para>
      Private, &cloud; internal virtual network. This network is used for
      inter-&vmguest; communication only. The gateway required is also
      automatically provided by &cloud;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     Public Network (nova-floating, public, <systemitem
     class="etheraddress">192.168.126/24</systemitem>)
    </term>
    <listitem>
     <para>
      The only public network provided by &cloud;. You can access the Nova
      &dash; as well as &vmguest;s (provided they have been equipped with a
      floating IP) on this network. This network can only be accessed via a
      gateway, which needs to be provided externally. All &cloud; users and
      administrators need to be able to access the public network.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     Software Defined Network (os_sdn, <systemitem class="etheraddress">192.168.130/24</systemitem>)
    </term>
    <listitem>
     <para>
      <remark condition="clarity">
       2013-06-18 - fs: Is this true? 
      </remark>
      FIXME: Private, &cloud; internal virtual network. This network is used by
      &o_netw; only. It should not be accessed by users.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <note>
   <title>No IPv6 support</title>
   <para>
    As of &productname; &productnumber;, IPv6 is not supported. This applies
    to the cloud internal networks as well as to the &vmguest;s.
   </para>
  </note>

  <para>
   The following diagram shows the pre-defined &cloud; network in more
   detail. It demonstrates how the &ostack; nodes and services use the
   different networks.
  </para>

  <figure>
   <title>&cloud; Network: Details</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="cloud_network_detail.pdf" width="100%" format="PDF"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="cloud_network_detail.png" width="100%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 id="sec.depl.req.network.allocation">
   <title>Network Address Allocation</title>
   <para>
    The default networks set up in &cloud; are class C networks with 256 IP
    addresses each. This limits the maximum number of &vmguest;s that can be
    started simultaneously. Addresses within the networks are allocated as
    outlined in the following table. Use the &yast; &crow; module to
    customize (see <xref linkend="sec.depl.inst.admserv.os.crowbar"/>). The
    <systemitem
    class="etheraddress">.255</systemitem> address for each
    network is always reserved as the broadcast address. This assignment
    cannot be changed.
   </para>
   <table>
    <title><systemitem class="etheraddress">192.168.124.0/24</systemitem> (Admin/BMC) Network Address Allocation</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="20*"/>
     <colspec colnum="2" colname="2" colwidth="33*"/>
     <colspec colnum="3" colname="3" colwidth="47*"/>
     <thead>
      <row>
       <entry>
        <para>
         Function
        </para>
       </entry>
       <entry>
        <para>
         Address
        </para>
       </entry>
       <entry>
        <para>
         Remark
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         router
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.1</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Provided externally.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         admin
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.10</systemitem> -
         <systemitem class="etheraddress">192.168.124.11</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Fixed addresses reserved for the &admserv;.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         dhcp
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.21</systemitem> -
         <systemitem class="etheraddress">192.168.124.80</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Address range reserved for node allocation/installation. Determines
         the maximum number of parallel allocations/installations.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.81</systemitem> -
         <systemitem class="etheraddress">192.168.124.160</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Fixed addresses for the &ostack; nodes. Determines the maximum
         number of &ostack; nodes that can be deployed.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         bmc vlan host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.161</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Fixed address for the BMC VLAN.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         bmc host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.162</systemitem> -
         <systemitem class="etheraddress">192.168.124.240</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Fixed addresses for the &ostack; nodes. Determines the maximum
         number of &ostack; nodes that can be deployed.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         switch
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.241</systemitem> -
         <systemitem class="etheraddress">192.168.124.250</systemitem>
        </para>
       </entry>
       <entry>
        <para></para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <table>
    <title><systemitem class="etheraddress">192.168.125/24</systemitem> (Storage) Network Address Allocation</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="20*"/>
     <colspec colnum="2" colname="2" colwidth="33*"/>
     <colspec colnum="3" colname="3" colwidth="47*"/>
     <thead>
      <row>
       <entry>
        <para>
         Function
        </para>
       </entry>
       <entry>
        <para>
         Address
        </para>
       </entry>
       <entry>
        <para>
         Remark
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.125.10</systemitem> -
         <systemitem class="etheraddress">192.168.125.239</systemitem>
        </para>
       </entry>
       <entry>
        <para></para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <table>
    <title><systemitem class="etheraddress">192.168.123/24</systemitem> (Private Network/nova-fixed) Network Address Allocation</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="20*"/>
     <colspec colnum="2" colname="2" colwidth="33*"/>
     <colspec colnum="3" colname="3" colwidth="47*"/>
     <thead>
      <row>
       <entry>
        <para>
         Function
        </para>
       </entry>
       <entry>
        <para>
         Address
        </para>
       </entry>
       <entry>
        <para>
         Remark
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         router
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.123.1</systemitem> -
         <systemitem class="etheraddress">192.168.123.49</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Each &compnode; also acts as a router for <quote>its</quote>
         &vmguest;s, getting an address from this range assigned. This
         effectively limits the maximum number of &compnode;s that can be
         deployed with &productname; to 49.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         dhcp
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.123.50</systemitem> -
         <systemitem class="etheraddress">192.168.123.254</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Address range for &vmguest;s.
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <table>
    <title><systemitem class="etheraddress">192.168.126/24</systemitem> (Public Network nova-floating, public) Network Address Allocation</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="20*"/>
     <colspec colnum="2" colname="2" colwidth="33*"/>
     <colspec colnum="3" colname="3" colwidth="47*"/>
     <thead>
      <row>
       <entry>
        <para>
         Function
        </para>
       </entry>
       <entry>
        <para>
         Address
        </para>
       </entry>
       <entry>
        <para>
         Remark
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         public host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.126.2</systemitem> -
         <systemitem class="etheraddress">192.168.126.49</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Public address range for external &cloud; services such as the
         Nova &dash; or the API.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         public dhcp
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.126.50</systemitem> -
         <systemitem class="etheraddress">192.168.126.127</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         <remark condition="clarity">
 2013-05-22 - fs: Please check if this is correct
</remark>
         Public address range for &vmguest;s. These addresses are
         automatically assigned to the &vmguest;s to e.g. allow them to
         access the internet. Not to be confused with the floating IP
         addresses that are assigned by users and allow to login to the
         &vmguest; from the outside. Determines the maximum number of
         &vmguest;s that can be started concurrently.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         floating host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.126.129</systemitem> -
         <systemitem class="etheraddress">192.168.126.191</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         <remark condition="clarity">
 2013-05-22 - fs: Please check if this is correct
</remark>
         Floating IP address range. Floating IPs can be manually assigned to
         a running &vmguest; to allow to access the guest from the outside.
         Determines the maximum number of &vmguest;s that can concurrently
         be accessed from the outside.
        </para>
        <para>
         The nova_floating network is set up with a netmask of
         255.255.255.192, allowing a maximum number of 61 IP addresses. This
         range is pre-allocated by default and managed by &o_netw;.
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <table>
    <title><systemitem class="etheraddress">192.168.130/24</systemitem>
    (Software Defined Network) Network Address Allocation</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="20*"/>
     <colspec colnum="2" colname="2" colwidth="33*"/>
     <colspec colnum="3" colname="3" colwidth="47*"/>
     <thead>
      <row>
       <entry>
        <para>
         Function
        </para>
       </entry>
       <entry>
        <para>
         Address
        </para>
       </entry>
       <entry>
        <para>
         Remark
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.130.10</systemitem> -
         <systemitem class="etheraddress">192.168.130.254</systemitem>
        </para>
       </entry>
       <entry>
        <para></para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </sect2>

  <sect2 id="sec.depl.req.network.modes">
   <title>Network Modes</title>
   <para>
    &productname; supports different network modes: single, dual, and
    teaming. As of &productname; &productnumber; the networking mode is
    applied to all nodes as well as the &admserv;. That means that all
    machines need to meet the hardware requirements for the chosen mode. The
    network mode can be configured using the &yast; &crow; module
    (<xref
    linkend="sec.depl.inst.admserv.os.crowbar"/>). The network
    mode cannot be changed once the cloud is deployed.
   </para>
   <para>
    &l3-network-support;
   </para>
   <sect3 id="sec.depl.req.network.modes.single">
    <title>Single Network Mode</title>
    <para>
     In single mode you just use one ethernet card for all the traffic:
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="cloud_networking_single_mode.pdf" width="70%"
                  format="PDF"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="cloud_networking_single_mode.png" width="50%"
                  format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </sect3>
   <sect3 id="sec.depl.req.network.modes.dual">
    <title>Dual Network Mode</title>
    <para>
     Dual mode needs two ethernet cards and allows you to completely
     separate traffic to/from the Admin Network and to/from the public
     network:
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="cloud_networking_dual_mode.pdf" width="100%"
                  format="PDF"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="cloud_networking_dual_mode.png" width="70%"
                  format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </sect3>
   <sect3 id="sec.depl.req.network.modes.teaming">
    <title>Teaming Network Mode</title>
    <para>
     Teaming mode is almost identical to single mode, except for the fact
     that you combine several ethernet cards to a <quote>bond</quote> in
     order to increase the performance. Teaming mode needs two or more
     ethernet cards.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="cloud_networking_team_mode.pdf" width="70%"
                  format="PDF"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="cloud_networking_team_mode.png" width="50%"
                  format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </sect3>
  </sect2>

  <sect2 id="sec.depl.req.network.bastion">
   <title>Accessing the Admin Network via a Bastion Network</title>
   <para>
    If you want to allow to access the cloud's admin network from another
    network, you can do so by providing an external gateway. This option
    offers maximum flexibility, but requires additional machines and may be
    less secure than you require. Therefore &cloud; offers a second option
    for accessing a single external network (e.g. a dedicated server
    administration network): the bastion network. You just need a dedicated
    ethernet card and a static IP address from the external network to set
    it up.
   </para>
  </sect2>

  <sect2 id="sec.depl.req.network.dns">
   <title>DNS and Hostnames</title>
   <para>
    The &admserv; acts as a name server for all nodes in the cloud. If you
    allow access to the admin network from outside, you may want to add
    additional name servers to your network setup prior to deploying
    &cloud;. If additional name servers are found on cloud deployment, the
    name server on the &admserv; will automatically be configured to forward
    requests for non-local records to those servers.
   </para>
   <para>
    The &admserv; needs to be configured to have a fully qualified hostname.
    This hostname must not be changed after &cloud; has been deployed. The
    &ostack; nodes will be named after their MAC address by default, but you
    can provide aliases, which are easier to remember when allocating the
    nodes. The aliases for the &ostack; nodes can be changed any time. It is
    useful to have a list of MAC addresses and the intended use of the
    corresponding host at hand when deploying the &ostack; nodes.
   </para>
  </sect2>
 </sect1>
 <sect1 id="sec.depl.req.repos">
  <title>Product and Update Repositories</title>

  <para>
   The &admserv; as well as the &ostack; nodes need to get security updates
   and patches for the operating system (&sls;) as well as for &productname;
   itself. Furthermore product repositories for &sls; and &cloud; are needed
   as an installation source for the &ostack; nodes. In &productname; the
   &admserv; is designed to work as the single source for all repositories.
  </para>

  <para>
   The repositories need to be available locally. While the product
   repositories do not change (and therefore can be made available from the
   installation media), the update repositories need to be mirrored from the
   &ncc; and be regularly updated. For this purpose, &suse; offers the free
   &smtool; add-on product
   (<ulink
   url="http://www.novell.com/linux/smt/"/>). Alternatively
   &suse; Manager
   (<ulink url="https://www.suse.com/products/suse-manager/"/>, &suse;'s
   comprehensive Linux server management solution, can provide the
   repositories.
  </para>

  <para>
   Depending on your network setup there are several possibilities to
   provide up-to-date repositories on the &admserv;:
  </para>

  <variablelist>
   <varlistentry>
    <term>Installing a &smtool; (&smt;) Server on the &admserv;</term>
    <listitem>
     <para>
      The &smt; server, a free add-on product for &sls;, regularly
      synchronizes repository data from &ncc; with your local host.
      Installing the &smt; server on the &admserv; is recommended if you do
      not have access to update repositories from elsewhere within your
      organization. This option requires the &admserv; to be able to access
      the Internet. &smtool; 11 SP3 is available from
      <ulink
      url="http://www.novell.com/linux/smt/"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><quote>Sneakernet</quote>
    </term>
    <listitem>
     <para>
      If you choose to completely seal off your admin network from all other
      networks, you need to manually update the repositories from removable
      media. For this purpose copy the repositories from an existing &smt;
      or &suse; Manager server to the removable media.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Utilizing Existing Repositories</term>
    <listitem>
     <para>
      If you can access existing repositories from within your company
      network from the &admserv;, you can either mount or sync these
      repositories from an existing &smt; or &suse; Manager server to the
      required locations on the &admserv;.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   As of &productname; &productnumber;, the following update repositories
   need to be mirrored:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     SLES11-SP3-Pool
    </para>
   </listitem>
   <listitem>
    <para>
     SLES11-SP3-Updates
    </para>
   </listitem>
   <listitem>
    <para>
     SUSE-Cloud-2.0-Pool
    </para>
   </listitem>
   <listitem>
    <para>
     SUSE-Cloud-2.0-Updates
    </para>
   </listitem>
   <listitem>
    <para>
     SLES11-SMT-SP3-Pool (only needed when the &smt; is installed)
    </para>
   </listitem>
   <listitem>
    <para>
     SLES11-SMT-SP3-Updates (only needed when the &smt; is installed)
    </para>
   </listitem>
  </itemizedlist>

  <para>
   In addition to the update repositories you also need to mirror the
   contents of the product media (&sls; 11 SP3 and &productname;
   &productnumber;) to your local disk.
  </para>
 </sect1>
 <sect1 id="sec.depl.req.storage">
  <title>Storage</title>

  <para>
   When talking about <quote>storage</quote> on &productname;, there are two
   completely different aspects to discuss: the block and object storage
   services &cloud; offers on the one hand and the hardware related storage
   aspects on the different node types.
  </para>

  <sect2 id="sec.depl.req.storage.services">
   <title>Cloud Storage Services</title>
   <para>
    As mentioned above, &cloud; offers two different types of storage
    services: object and block storage. Object storage lets you upload and
    download files (similar to an FTP server), whereas a block storage
    provides mountable devices (similar to a hard-disk partition).
    Furthermore &cloud; provides a repository to store the virtual disk
    images used to start &vmguest;s.
   </para>
   <variablelist>
    <varlistentry>
     <term>Object Storage with &swift;</term>
     <listitem>
      <para>
       The object &ostack; storage service is called &swift;. &swift; needs
       to be deployed on dedicated nodes where no other cloud services run.
       In order to be able to store the objects redundantly, it is required
       to deploy at least two &swift; nodes. &productname; is configured to
       always use all unused disks on a node for storage. Offering object
       storage with &swift; is optional.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Block Storage</term>
     <listitem>
      <para>
       Block storage on &cloud; is provided by &o_blockstore;. By default &o_blockstore;
       uses an LVM backend with iSCSI. This default setup utilizes a
       single device on the &contrnode;&mdash;using a RAID for this purpose
       is strongly recommended.
      </para>
      <para>
       Alternatively, &o_blockstore; can use &ceph; RBD as a backend. &ceph;
       offers data security and speed by storing the devices redundantly on
       different servers. &ceph; needs to be deployed on dedicated nodes
       where no other cloud services run. In order to be able to store the
       objects redundantly, it is required to deploy at least two &ceph;
       nodes. You can configure which devices &ceph; uses for storage.
      </para>
      &no-ceph-support;
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>The &o_img; Image Repository</term>
     <listitem>
      <para>
       &o_img; provides a catalog and repository for virtual disk images used
       to start the &vmguest;s. &o_img; is usually installed on the
       &contrnode;. The image repository resides in a directory on the file
       system by default&mdash;it is recommended to mount a partition or
       volume to that directory.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 id="sec.depl.req.storage.hardware">
   <title>Storage Hardware Requirements</title>
   <para>
    Apart from sufficient disk space to install the &sls; operating system,
    each node in &cloud; has to store additional data. Requirements and
    recommendations for the various node types are listed below.
   </para>
   <important>
    <title>Choose a Hard Disk for the Operating System Installation</title>
    <para>
     The operating system will always be installed on the
     <emphasis>first</emphasis> hard disk, the one that is recognized as
     <filename>/dev/sda</filename>. This is the disk that is listed
     <emphasis>first</emphasis> in the BIOS, the one from which the machine
     will boot. If you have nodes with a certain hard disk you want the
     operating system to be installed on, make sure it will be recognized as
     the first disk.
    </para>
   </important>
   <sect3 id="sec.depl.req.storage.hardware.admin">
    <title>&admserv;</title>
    <para>
     If you store the update repositories directly on the &admserv; (see
     <xref
     linkend="sec.depl.req.repos"/> for details), it is
     recommended to mount <filename>/srv</filename> to a separate partition
     or volume with a minimum of 30 GB space.
    </para>
   </sect3>
   <sect3 id="sec.depl.req.storage.hardware.control">
    <title>&contrnode;</title>
    <para>
     The virtual disk image repository resides under
     <filename>/var/lib/glance/images</filename> by default. It is
     recommended to mount a separate partition or volume into this directory
     that provides enough space to host all virtual disk images needed.
    </para>
    <para>
     Unless deploying the &ceph; RDB service as a backend for &o_blockstore;
     (which is currently not supported, but included as a technical
     preview), it uses LVM with iSCSI on the &contrnode;. This setup allows
     to use only one device, therefore it is highly recommended to provide a
     RAID with sufficient disk space.
    </para>
   </sect3>
   <sect3 id="sec.depl.req.storage.hardware.compute">
    <title>&compnode;s</title>
    <para>
     Unless an &vmguest; is started via <quote>Boot from Volume</quote>, it
     is started with at least one disk&mdash;a copy of the image from which
     it has been started. Depending on the flavor you start, the &vmguest;
     may also have a second, so-called <quote>ephemeral</quote> disk. The
     size of the root disk depends on the image itself, while ephemeral
     disks are always created as sparse image files that grow (up to a
     defined size) when being <quote>filled</quote>. By default ephemeral
     disks have a size of 10 GB.
    </para>
    <para>
     Both disks, root images and ephemeral disk, are directly bound to the
     &vmguest; and are deleted when the &vmguest; is terminated. Therefore
     these disks are bound to the &compnode; on which the &vmguest; has been
     started. The disks are created under <filename>/var/lib/nova</filename>
     on the &compnode;. Your &compnode;s should be equipped with enough disk
     space to store the root images and ephemeral disks.
    </para>
    <note>
     <title>Ephemeral Disks vs. Block Storage</title>
     <para>
      Do not confuse ephemeral disks with persistent block storage. In
      addition to an ephemeral disk, which is automatically provided with
      most &vmguest; flavors, you can optionally add a persistent storage
      device provided by &o_blockstore;. Ephemeral disks are deleted when the
      &vmguest; terminates, while persistent storage devices can be reused
      in another &vmguest;.
     </para>
    </note>
    <para>
     The maximum disk space required on a compute node depends on the
     available flavors. A flavor specifies the number of CPUs, as well as
     RAM and disk size of an &vmguest;. Several flavors ranging from
     <guimenu>tiny</guimenu> (1 CPU, 2512 MB RAM, no ephemeral disk) to
     <guimenu>xlarge</guimenu> (8 CPUs, 8 GB RAM, 10 GB ephemeral disk) are
     available by default. Adding custom flavors as well as editing and
     deleting existing flavors is also supported.
    </para>
    <para>
     To calculate the minimum disk space needed on a compute node, you need
     to determine the highest "disk space to RAM" ratio from your flavors.
     Example:
    </para>
    <simplelist>
     <member>
      Flavor small: 2 GB RAM, 100 GB ephemeral disk => 50 GB disk /1 GB RAM
     </member>
     <member>
      Flavor large: 8 GB RAM, 200 GB ephemeral disk => 25 GB disk /1 GB RAM
     </member>
    </simplelist>
    <para>
     So, 50 GB disk /1 GB RAM is the ratio that matters. If you multiply
     that value by the amount of RAM in GB available on your compute node,
     you have the minimum disk space required by ephemeral disks. Pad that
     value with sufficient space for the root disks plus a buffer that
     enables you to create flavors with a higher disk space to RAM ratio in the
     future.
    </para>
    <warning>
     <title>Overcommitting Disk Space</title>
     <para>
      The scheduler that decides in which node an &vmguest; is started does
      not check for available disk space. If there is no disk space left on
      a compute node, this will not only cause data loss on the &vmguest;s,
      but the compute node itself will also stop operating. Therefore you
      must make sure all compute nodes are equipped with enough hard disk
      space!
     </para>
    </warning>
   </sect3>
   <sect3 id="sec.depl.req.storage.hardware.store">
    <title>Storage Nodes</title>
    <para>
     The block-storage service &ceph; RBD and the object storage service &swift;
     need to be deployed onto dedicated nodes&mdash;it is not possible to
     mix these services. Each storage service requires at least two machines
     (more are recommended) to be able to store data redundantly.
    </para>
    <para>
     Each &ceph;/&swift; &stornode; needs at least two hard disks. The first
     one (<filename>/dev/sda</filename>) will be used for the operating system
     installation, while the others can be used for storage purposes.
     Although it is possible to configure which devices &ceph; uses for
     storage, this is not the case with &swift;, which uses all of the
     devices.
    </para>
    &no-ceph-support;
   </sect3>
  </sect2>
 </sect1>
 <sect1 id="sec.depl.req.ssl">
  <title>SSL Encryption</title>

  <para>
   Whenever non-public data travels over a network it needs to be encrypted.
   Encryption protects the integrity and confidentiality of data. Therefore
   you should enable SSL support when deploying &cloud; to production (it is
   not enabled by default). The following services (and their APIs if
   available) can make use of SSL:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     &o_netw;
    </para>
   </listitem>
   <listitem>
    <para>
     &o_ident;
    </para>
   </listitem>
   <listitem>
    <para>
     &o_img;
    </para>
   </listitem>
   <listitem>
    <para>
     &o_comp;
    </para>
   </listitem>
   <listitem>
    <para>
     VNC
    </para>
   </listitem>
   <listitem>
    <para>
     Nova &dash;
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Each service requires valid certificates signed by a trusted third party.
   You may either use the same certificates for all services or use
   dedicated certificates for each service. See
   <ulink
   url="http://www.suse.com/documentation/sles11/book_sle_admin/data/sec_apache2_ssl.html"/>
   for instructions on how to create certificates and get them signed by a
   trusted organization.
  </para>
  
  <important>
   <title>Host Names</title>
   <para>
    Each SSL certificate is issued for a certain host name and, optional, for
    alternative hostnames (via the <literal>AlternativeName</literal>
    option). Each node publicly available node in &cloud; has got two
    hostnames&mdash;an internal and a public one. The SSL certificate needs to
    be issued for both names. 
   </para>
   <para>
    The internal name has the following scheme:
   </para>
    <screen>d<replaceable>MAC ADRESS</replaceable>.<replaceable>FQDN</replaceable></screen>
   <para>
    <replaceable>MAC ADRESS</replaceable> is the MAC address of the interface
    used to PXE boot the machine with lowercase letters and colons replaced with
    dashes, for example
    <literal>52-54-00-8e-ce-e3</literal>. <replaceable>FQDN</replaceable> is
    the fully qualified domain name. An example name looks like this:
   </para>
   <screen>d52-54-00-8e-ce-e3.&exampledomain;</screen>
   <para>
    Unless you have entered a custom <guimenu>Public Name</guimenu> for a
    client (see <xref linkend="sec.depl.inst.nodes.install"/> for details),
    the public name is the same as the internal name prefixed by
    <literal>public.</literal>: 
   </para>
   <screen>public.d52-54-00-8e-ce-e3.&exampledomain;</screen>
   <para>
    To look up the node names open the &crow; Web interface and click on a
    node name in the <guimenu>Node Dashboard</guimenu>. The names are listed
    as <guimenu>Full Name</guimenu> and <guimenu>Public Name</guimenu>.
   </para>
  </important>
  
 </sect1>
 <sect1 id="sec.depl.req.hardware">
  <title>Hardware Requirements</title>

  <para>
   Precise hardware requirements can only be listed for the &admserv; and
   the &ostack; &contrnode;. The requirements of the &ostack; Compute and
   &stornode;s depends on the number of concurrent &vmguest;s and their
   virtual hardware equipment.
  </para>

  <para>
   The minimum number of machines required for a &cloud; setup featuring all
   services is seven: one &admserv;, one &contrnode;, one &compnode;, and
   four &stornode;s. In addition to that, a gateway providing access to the
   public network is required.
  </para>

  <important>
   <title>Physical Machines and Architecture</title>
   <para>
    All &cloud; nodes need to be physical machines. Although the &admserv;
    and the &contrnode; can be virtualized in test environments, this is not
    supported for production systems.
   </para>
   <para>
    &cloud; currently only runs on <literal>x86_64</literal> hardware.
   </para>
  </important>

  <sect2 id="sec.depl.req.hardware.admserv">
   <title>&admserv;</title>
   <itemizedlist>
    <listitem>
     <para>
      Architecture: x86_64
     </para>
    </listitem>
    <listitem>
     <para>
      RAM: at least 2 GB, 4 GB recommended
     </para>
    </listitem>
    <listitem>
     <para>
      Hard disk: at least 40 GB. It is recommended to put
      <filename>/srv</filename> on a separate partition with at least 30 GB
      space, unless you mount the update repositories from another server
      (see <xref linkend="sec.depl.req.repos"/> for details).
     </para>
    </listitem>
    <listitem>
     <para>
      Number of network cards: 1 for single mode, 2 for dual mode, 2 or more
      for team mode. Additional networks such as the bastion network and/or
      a separate BMC network each need an additional network card. See
      <xref
       linkend="sec.depl.req.network"/> for details.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 id="sec.depl.req.hardware.contrnode">
   <title>&contrnode;</title>
   <itemizedlist>
    <listitem>
     <para>
      Architecture: x86_64
     </para>
    </listitem>
    <listitem>
     <para>
      RAM: at least 1 GB, 2 GB recommended
     </para>
    </listitem>
    <listitem>
     <para>
      Number of network cards: 1 for single mode, 2 for dual mode, 2 or more
      for team mode. See <xref linkend="sec.depl.req.network"/> for details.
     </para>
    </listitem>
    <listitem>
     <para>
      Hard disk: See
      <xref
       linkend="sec.depl.req.storage.hardware.control"/>.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 id="sec.depl.req.hardware.compnode">
   <title>&compnode;</title>
   <para>
    The &compnode;s need to be equipped with a sufficient amount of RAM and
    CPUs, matching the numbers required by the maximum number of &vmguest;s
    running concurrently. An &vmguest; started in &cloud; cannot share
    resources from several physical nodes, but rather uses the resources of
    the node on which it was started. So if you offer a flavor (see
    <xref
     linkend="gloss.flavor"/> for a definition) with 8 CPUs and 12
    GB RAM, at least one of your nodes should be able to provide these
    resources.
   </para>
   <para>
    See <xref linkend="sec.depl.req.storage.hardware.compute"/> for storage
    requirements.
   </para>
  </sect2>

  <sect2 id="sec.depl.req.hardware.stornode">
   <title>&stornode;</title>
   <para>
    The &stornode;s are sufficiently equipped with a single CPU and 1 or 2
    GB of RAM. See
    <xref
     linkend="sec.depl.req.storage.hardware.store"/> for storage
    requirements.
   </para>
  </sect2>
 </sect1>
 <sect1 id="sec.depl.req.software">
  <title>Software Requirements</title>

  <para>
   The following software requirements need to be met in order to install
   &cloud;:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     &sls; 11 SP3 installation media (ISO image, included in the &cloud;
     &admserv; subscription)
    </para>
   </listitem>
   <listitem>
    <para>
     Access to the &sls; 11 SP3 Update repositories (either by registering
     &sls; 11 SP3 or via an existing &smt; server).
    </para>
   </listitem>
   <listitem>
    <para>
     &cloud; installation media (ISO image).
    </para>
   </listitem>
   <listitem>
    <para>
     A &suse;/&novell; account (for product registration and &smt; setup).
     If you do not already have one, go to
     <ulink
     url="http://www.suse.com/login"/> to create it.
    </para>
   </listitem>
   <listitem>
    <para>
     Optional: &smtool; 11 SP3 installation media. A free download is
     available on <ulink url="http://www.novell.com/linux/smt/"/>. See
     <xref
     linkend="sec.depl.req.repos"/>.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 id="sec.depl.req.summary">
  <title>Summary: Considerations and Requirements</title>

  <para>
   As outlined above, there are some important considerations to be made
   before deploying &cloud;. The following briefly summarizes what was
   discussed in detail in this chapter. Keep in mind that as of
   &productname; &productnumber; it is not possible to change some aspects
   such as the network setup once &cloud; is deployed!
  </para>

  <itemizedlist>
   <title>Network</title>
   <listitem>
    <para>
     If you do not want to stick with the default networks and addresses,
     define custom networks and addresses. You need four different networks,
     at least three of them VLANs (nova-fixed/floating, public and storage).
     If you need to separate the admin and the BMC network, a fifth network
     is required. See <xref linkend="sec.depl.req.network"/> for details.
    </para>
   </listitem>
   <listitem>
    <para>
     The &cloud; networks are completely isolated, therefore it is not
     required to use public IP addresses for them. A class C network as used
     in this documentation provides a sufficient number of addresses.
     However, you may alternatively choose addresses from a class B or A
     network.
    </para>
   </listitem>
   <listitem>
    <para>
     Determine how to allocate addresses from your network. Make sure not to
     allocate IP addresses twice. See
     <xref
     linkend="sec.depl.req.network.allocation"/> for the default
     allocation scheme.
    </para>
   </listitem>
   <listitem>
    <para>
     Define which network mode to use. Keep in mind that all machines within
     the cloud (including the &admserv;) will be set up with the chosen mode
     and therefore need to meet the hardware requirements. See
     <xref
      linkend="sec.depl.req.network.modes"/> for details.
    </para>
   </listitem>
   <listitem>
    <para>
     Define how to access the admin and BMC network(s): no access from the
     outside (no action is required), via an external gateway (gateway needs
     to be provided), or via bastion network. See
     <xref
      linkend="sec.depl.req.network.bastion"/> for details.
    </para>
   </listitem>
   <listitem>
    <para>
     Provide a gateway to access the public network (public, nova-floating).
    </para>
   </listitem>
   <listitem>
    <para>
     Make sure the admin server's hostname is correctly configured
     (<command>hostname <option>-f</option></command> needs to return a fully
     qualified hostname).
    </para>
   </listitem>
   <listitem>
    <para>
     Prepare a list of MAC addresses and the intended use of the
     corresponding host for all &ostack; nodes.
    </para>
   </listitem>
  </itemizedlist>

  <itemizedlist>
   <title>Update Repositories</title>
   <listitem>
    <para>
     Depending on your network setup you have different options on how to
     provide up-to-date update repositories for &sls; and &cloud; on the
     &admserv;: Sneakernet, installing &smtool;, syncing data with an
     existing repository, or mounting remote repositories. Choose the option
     that best matches your needs.
    </para>
   </listitem>
  </itemizedlist>

  <itemizedlist>
   <title>Storage</title>
   <listitem>
    <para>
     Decide whether you want to deploy the object storage service &swift;.
     If so, you need to deploy at least two nodes with sufficient disk space
     exclusively dedicated to &swift;.
    </para>
   </listitem>
   <listitem>
    <para>
     Decide whether to use &o_blockstore; with &ceph; as backend for block
     storage (not supported). If deploying &ceph;, you need to deploy at least
     two nodes with sufficient disk space exclusively dedicated to it.  If you
     choose not to deploy &ceph; and use the default setup for &o_blockstore;
     (recommended), your &contrnode; needs to be equipped with additional disk
     space (a RAID is strongly recommended).
    </para>
    &no-ceph-support;
   </listitem>
   <listitem>
    <para>
     Optionally, provide a volume for storing the &o_img; image repository.
     Doing so is recommended.
    </para>
   </listitem>
   <listitem>
    <para>
     Make sure all nodes are equipped with sufficient hard disk space.
    </para>
   </listitem>
  </itemizedlist>

  <itemizedlist>
   <title>SSL Encryption</title>
   <listitem>
    <para>
     Decide whether to use different SSL certificates for the services and
     the API or whether to use a single certificate.
    </para>
   </listitem>
   <listitem>
    <para>
     Get one or more SSL certificates certified by a trusted third party
     source.
    </para>
   </listitem>
  </itemizedlist>

  <itemizedlist>
   <title>Hardware and Software Requirements</title>
   <listitem>
    <para>
     Make sure the hardware requirements for the different node types are
     met.
    </para>
   </listitem>
   <listitem>
    <para>
     Make sure to have all required software at hand.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
<!--
 <sect1 id="sec.depl.arch.security">
  <title>Security Considerations</title>

  <para>FIXME</para>
   * https://github.com/SUSE/cloud/wiki/Common-issues
   * https://github.com/SUSE/cloud/wiki/Nova-compute
   * enabling SSL

    Which communication will be encrypted?

   
     Needs a certificate
      -> point to SLES Apache doc for HowTo
      -> sufficient to put it on Admin node in /etc/apache2/ssl.*/

Passwords:

* SMT DB Password (change via yast)
* MySQL Root Password (MySQL is used for SMT)
* Crowbar Password (change cia yast)
* Chef WebUI Password:
  - initially change via yast (before install-suse-cloud run)
  - in the running system change via
    * Login to the Crowbar Web UI on port 3000:
      * Click on Barclamps in the top right corner of the Web UI.
      * Click on the arrow to the left of the Crowbar barclamp to expand it
      * Click the Edit button to edit the Default proposal.
      * Click on Raw to the right of Attributes.
      * Scroll through the attributes until you see something that looks like:

    "users": {
      "machine-install": {
        "password": "0e7b02a8d2086009c1ff00cc26a827d981306cbd420b1862063e6df8534e0f6a13e45100b9874d0a3fae5962c9ec2de12c0525b8c33685e8ee30406c4eee7133"
      },
      "crowbar": {
        "password": "crowbar"
      }
    },

    * Change the password.
    * Click Apply and confirm you want to apply the changes.
 - nodes root-Password (change in autoyast.xml.erb -> how? hashed, clear text?)

* Keystone
  - "Regular User" Name/Password
  - "Administrator" Name/Password
  Change via Keystone Barclamp

* Glance
  - Service User: Glance User in Keystone
  - Service Password: PW for Service User
  Change via Glance Barclamp

* Swift
  Cluster Admin Password
  Change via Swift Barclamp
    
-->
