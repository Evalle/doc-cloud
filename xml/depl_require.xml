<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet
 href="urn:x-daps:xslt:profiling:novdoc-profile.xsl" 
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "IGNORE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.depl.req">
 <title>Considerations and Requirements</title>
 <abstract>
  <para>
   Before deploying &productname;, there are a few requirements to be met
   and considerations to be made. Make sure to thoroughly read this
   chapter&mdash;some decisions need to be made <emphasis>before</emphasis>
   deploying &cloud;, since you cannot change them afterwards.
  </para>
 </abstract>
 <sect1 id="sec.depl.req.network">
  <title>Network</title>

  <para>
   &productname; requires a complex network setup consisting of several
   networks that are configured during installation. These networks are for
   exclusive cloud usage. In order to access them from an existing network,
   a router is needed.
  </para>

  <para>
   The network configuration on the nodes in the &cloud; network is entirely
   controlled by &crow;. Any network configuration not done with &crow;
   (e.g. with &yast;) will automatically be overwritten. Once the cloud is
   deployed, network settings cannot be changed anymore!
  </para>

  <figure>
   <title>&cloud; Network: Overview</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="cloud_network_overview.png" width="90%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="cloud_network_overview.png" width="75%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The following networks are pre-defined when setting up &productname;. The
   IP addresses listed are the default addresses and can be changed using
   the &yast; &crow; module (see
   <xref
   linkend="sec.depl.inst.admserv.os.crowbar"/>). It's also
   possible to completely customize the network setup. This requires to
   manually edit the network &barcl; template. See
   <xref linkend="app.deploy.network_json"/> for detailed instructions.
  </para>

  <variablelist>
   <varlistentry id="vle.netw.adm">
    <term>
     Admin Network (<systemitem
     class="etheraddress">192.168.124/24</systemitem>)
    </term>
    <listitem>
     <!-- TODO vuntz 2013-12-04: for reference, we got feedback that we should not touch the IPMI configuration at all by default, so we'll likely do this; that would mean access to the BMC would not be configured by Crowbar by default -->
     <para>
      A private network to access the &admserv; and all nodes for
      administration purposes. The default setup lets you also access the
      BMC (Baseboard Management Controller) data via IPMI (Intelligent
      Platform Management Interface) from this network. If required, BMC
      access can be swapped to a separate network.
     </para>
     <para>
      You have the following options for controlling access to this network:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        do not allow access from the outside and keep the admin network
        completely separated
       </para>
      </listitem>
      <listitem>
       <para>
        allow access to the &admserv; from a single network (e.g. your
        company's administration network) via the <quote>bastion
        network</quote> option configured on an additional network card with
        a fixed IP address
       </para>
      </listitem>
      <listitem>
       <para>
        allow access from one or more networks via a gateway
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry id="vle.netw.stor">
    <term>
     Storage Network (<systemitem
     class="etheraddress">192.168.125/24</systemitem>)
    </term>
    <listitem>
     <para>
      Private, &cloud; internal virtual network. This network is used by
      &ceph; and &o_objstore; only. It should not be accessed by users.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     Private Network (nova-fixed, <systemitem
     class="etheraddress">192.168.123/24</systemitem>)
    </term>
    <listitem>
     <para>
      Private, &cloud; internal virtual network. This network is used for
      inter-&vmguest; communication and provides access to the outside world
      for the &vmguest;s. The gateway required is also automatically provided
      by &cloud;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     Public Network (nova-floating, public, <systemitem
     class="etheraddress">192.168.126/24</systemitem>)
    </term>
    <listitem>
     <para>
      The only public network provided by &cloud;. You can access the Nova
      &dash; as well as &vmguest;s (provided they have been equipped with a
      floating IP) on this network. This network can only be accessed via a
      gateway, which needs to be provided externally. All &cloud; users and
      administrators need to be able to access the public network.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry id="vle.netw.sdn">
    <term>
     Software Defined Network (os_sdn, <systemitem class="etheraddress">192.168.130/24</systemitem>)
    </term>
    <listitem>
     <para>
     <!-- <remark condition="clarity">
       2013-06-18 - fs: Is this true? - rhafer 2014-04-08: yes 
      </remark>-->
     <!-- TODO vuntz 2013-12-04: this is used by neutron if we use openvswitch in gre mode, but check with rhafer -->
      Private, &cloud; internal virtual network. This network is used when
      &o_netw; is configured to use openvswitch with <literal>GRE</literal>
      tunneling for the virtual networks. It should not be
      accessed by users.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <warning>
   <title>Protect Networks from External Access</title>
  <para>For security reasons, protect the following networks from external
  access:</para>
  <itemizedlist>
   <listitem>
     <para>
      <xref linkend="vle.netw.adm" xrefstyle="select:label nopage"/>
     </para></listitem>
   <listitem><para>
      <xref linkend="vle.netw.stor" xrefstyle="select:label nopage"/></para></listitem>
    <listitem>
     <para>
      <xref linkend="vle.netw.sdn" xrefstyle="select:label nopage"/>
     </para>
    </listitem>
  </itemizedlist>
   <para>Especially traffic from the cloud instances must not be able to pass
    through these networks.</para>
  </warning>

  &vlan-settings;

  <note>
   <title>No IPv6 support</title>
   <para>
    As of &productname; &productnumber;, IPv6 is not supported. This applies
    to the cloud internal networks as well as to the &vmguest;s.
   </para>
  </note>

  <para>
   The following diagram shows the pre-defined &cloud; network in more
   detail. It demonstrates how the &ostack; nodes and services use the
   different networks.
  </para>

  <figure>
   <title>&cloud; Network: Details</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="cloud_network_detail.png" width="100%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="cloud_network_detail.png" width="100%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 id="sec.depl.req.network.allocation">
   <title>Network Address Allocation</title>
   <para>
    The default networks set up in &cloud; are class C networks with 256 IP
    addresses each. This limits the maximum number of &vmguest;s that can be
    started simultaneously. Addresses within the networks are allocated as
    outlined in the following table. Use the &yast; &crow; module to customize
    (see <xref linkend="sec.depl.inst.admserv.os.crowbar"/>). The last address
    in the IP range of each network is always reserved as the broadcast
    address. This assignment cannot be changed.
   </para>
   <note>
    <title>Limitations of the Default Network Proposal</title>
    <para>
     The default network proposal as described below limits the maximum
     number of &compnode;s to 49, the maximum number of floating IP
     addresses to 61 and the maximum number of &vmguest;s to 204.
    </para>
    <para>
     To overcome these limitations you need to reconfigure the network setup
     by using appropriate network ranges.  Do this by either using the &yast;
     &crow; module as described in <xref
     linkend="sec.depl.inst.admserv.os.crowbar"/> or by manually editing the
     network template file as described in <xref
     linkend="app.deploy.network_json"/>.
    </para>
   </note>
     <!-- TODO vuntz 2013-12-04: after looking at the tables, it might be worth mentioning that the ranges that are used inside the table are not "network ranges" (as in with proper netmask, etc.); they're just arbitrary ranges inside the network range used by each table -->
   
   <table>
    <title><systemitem class="etheraddress">192.168.124.0/24</systemitem> (Admin/BMC) Network Address Allocation</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="20*"/>
     <colspec colnum="2" colname="2" colwidth="33*"/>
     <colspec colnum="3" colname="3" colwidth="47*"/>
     <thead>
      <row>
       <entry>
        <para>
         Function
        </para>
       </entry>
       <entry>
        <para>
         Address
        </para>
       </entry>
       <entry>
        <para>
         Remark
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         router
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.1</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Provided externally.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         admin
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.10</systemitem> -
         <systemitem class="etheraddress">192.168.124.11</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Fixed addresses reserved for the &admserv;.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         dhcp
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.21</systemitem> -
         <systemitem class="etheraddress">192.168.124.80</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Address range reserved for node allocation/installation. Determines
         the maximum number of parallel allocations/installations.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.81</systemitem> -
         <systemitem class="etheraddress">192.168.124.160</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Fixed addresses for the &ostack; nodes. Determines the maximum
         number of &ostack; nodes that can be deployed.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         bmc vlan host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.161</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Fixed address for the BMC VLAN. Used to generate a VLAN tagged
         interface on the Administration Server that can access the BMC
         network. The BMC VLAN needs to be in the same ranges as BMC, and BMC
         has to have VLAN enabled.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         bmc host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.162</systemitem> -
         <systemitem class="etheraddress">192.168.124.240</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Fixed addresses for the &ostack; nodes. Determines the maximum
         number of &ostack; nodes that can be deployed.
        </para>
       </entry>
      </row>
      <row>
     <!-- TODO vuntz 2013-12-04: does anyone know what is the use of this switch bit? I quickly grepped but couldn't find anything -->
       <entry>
        <para>
         switch
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.241</systemitem> -
         <systemitem class="etheraddress">192.168.124.250</systemitem>
        </para>
       </entry>
       <entry>
        <para></para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <table>
    <title><systemitem class="etheraddress">192.168.125/24</systemitem> (Storage) Network Address Allocation</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="20*"/>
     <colspec colnum="2" colname="2" colwidth="33*"/>
     <colspec colnum="3" colname="3" colwidth="47*"/>
     <thead>
      <row>
       <entry>
        <para>
         Function
        </para>
       </entry>
       <entry>
        <para>
         Address
        </para>
       </entry>
       <entry>
        <para>
         Remark
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.125.10</systemitem> -
         <systemitem class="etheraddress">192.168.125.239</systemitem>
        </para>
       </entry>
       <entry>
        <para>Each &stornode; will get an address from this range.</para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <table>
    <title><systemitem class="etheraddress">192.168.123/24</systemitem> (Private Network/nova-fixed) Network Address Allocation</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="20*"/>
     <colspec colnum="2" colname="2" colwidth="33*"/>
     <colspec colnum="3" colname="3" colwidth="47*"/>
     <thead>
      <row>
       <entry>
        <para>
         Function
        </para>
       </entry>
       <entry>
        <para>
         Address
        </para>
       </entry>
       <entry>
        <para>
         Remark
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         router
        </para>
       </entry>
       <entry>
        <para>
     <!-- TODO vuntz 2013-12-04: hrm, it's probably a bug that router is set to this value; it sounds like it shouldn't be set at all as the gateway is provided by the neutron router? -->
         <systemitem class="etheraddress">192.168.123.1</systemitem> -
         <systemitem class="etheraddress">192.168.123.49</systemitem>
        </para>
       </entry>
       <entry>
     <!-- TODO vuntz 2013-12-04: hrm, not true anymore; now routers are on network nodes; so this limits number of network nodes? (or is the router IP address actually common? -->
        <para>
         Each &compnode; also acts as a router for <quote>its</quote>
         &vmguest;s, getting an address from this range assigned. This
         effectively limits the maximum number of &compnode;s that can be
         deployed with &productname; to 49.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         dhcp
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.123.50</systemitem> -
         <systemitem class="etheraddress">192.168.123.254</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Address range for &vmguest;s.
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <table>
    <title><systemitem class="etheraddress">192.168.126/24</systemitem> (Public Network nova-floating, public) Network Address Allocation</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="20*"/>
     <colspec colnum="2" colname="2" colwidth="33*"/>
     <colspec colnum="3" colname="3" colwidth="47*"/>
     <thead>
      <row>
       <entry>
        <para>
         Function
        </para>
       </entry>
       <entry>
        <para>
         Address
        </para>
       </entry>
       <entry>
        <para>
         Remark
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         router
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.126.1</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Provided externally.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         public host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.126.2</systemitem> -
         <systemitem class="etheraddress">192.168.126.49</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Public address range for external &cloud; services such as the
         &ostack; &dash; or the API.
        </para>
       </entry>
      </row>
<!-- TODO vuntz 2013-12-04: hrm, is this actually unused now? Ralf?
     Ralf: Yes, I am pretty sure this is no longer used. I guess it
           was used for some nova-network related thing in the past
      <row>
       <entry>
        <para>
         public dhcp
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.126.50</systemitem> -
         <systemitem class="etheraddress">192.168.126.127</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         <remark condition="clarity">
 2013-05-22 - fs: Please check if this is correct
</remark>
         Public address range for &vmguest;s. These addresses are
         automatically assigned to the &vmguest;s to e.g. allow them to
         access the internet. Not to be confused with the floating IP
         addresses that are assigned by users and allow to login to the
         &vmguest; from the outside. Determines the maximum number of
         &vmguest;s that can be started concurrently.
        </para>
       </entry>
      </row>
-->
      <row>
       <entry>
        <para>
         floating host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.126.129</systemitem> -
         <systemitem class="etheraddress">192.168.126.191</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Floating IP address range. Floating IPs can be manually assigned to
         a running &vmguest; to allow to access the guest from the outside.
         Determines the maximum number of &vmguest;s that can concurrently
         be accessed from the outside.
        </para>
        <para>
         The nova_floating network is set up with a netmask of
         255.255.255.192, allowing a maximum number of 61 IP addresses. This
         range is pre-allocated by default and managed by &o_netw;.
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <table>
    <title><systemitem class="etheraddress">192.168.130/24</systemitem> (Software Defined Network) Network Address Allocation</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="20*"/>
     <colspec colnum="2" colname="2" colwidth="33*"/>
     <colspec colnum="3" colname="3" colwidth="47*"/>
     <thead>
      <row>
       <entry>
        <para>
         Function
        </para>
       </entry>
       <entry>
        <para>
         Address
        </para>
       </entry>
       <entry>
        <para>
         Remark
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.130.10</systemitem> -
         <systemitem class="etheraddress">192.168.130.254</systemitem>
        </para>
       </entry>
       <entry>
        <para>
	 If &o_netw; is configured with <literal>openvswitch</literal> and
	 <literal>gre</literal>, each network node and all &compnode;s will
	 get an IP from this range.
	</para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <note>
    <title>Addresses for Additional Servers</title>
    <para>
     Addresses not used in the ranges mentioned above, can be used to add
     additional servers with static addresses to &cloud;. Such servers can be
     used to provide additional services. A &susemgr; server inside &cloud;,
     for example, needs to be configured using one of these addresses.
    </para>
   </note>
   
  </sect2>

  <sect2 id="sec.depl.req.network.modes">
   <title>Network Modes</title>
   <para>
    &productname; supports different network modes: single, dual, and
    teaming. As of &productname; &productnumber; the networking mode is
    applied to all nodes as well as the &admserv;. That means that all
    machines need to meet the hardware requirements for the chosen mode. The
    network mode can be configured using the &yast; &crow; module
    (<xref
    linkend="sec.depl.inst.admserv.os.crowbar"/>). The network
    mode cannot be changed once the cloud is deployed.
   </para>
   <para>
    &l3-network-support;
   </para>
   <sect3 id="sec.depl.req.network.modes.single">
    <title>Single Network Mode</title>
    <para>
     In single mode you just use one ethernet card for all the traffic:
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="cloud_networking_single_mode.png" width="70%"
                  format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="cloud_networking_single_mode.png" width="50%"
                  format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </sect3>
   <sect3 id="sec.depl.req.network.modes.dual">
    <title>Dual Network Mode</title>
    <para>
     Dual mode needs two ethernet cards (on all nodes but &admserv;) and
     allows you to completely separate traffic to/from the Admin Network and
     to/from the public network:
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="cloud_networking_dual_mode.png" width="100%"
                  format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="cloud_networking_dual_mode.png" width="70%"
                  format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </sect3>
   <sect3 id="sec.depl.req.network.modes.teaming">
    <title>Teaming Network Mode</title>
    <para>
     Teaming mode is almost identical to single mode, except for the fact
     that you combine several ethernet cards to a <quote>bond</quote> in
     order to increase the performance and &ha;. Teaming mode needs two or more
     ethernet cards.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="cloud_networking_team_mode.png" width="70%"
                  format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="cloud_networking_team_mode.png" width="50%"
                  format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </sect3>
  </sect2>

  <sect2 id="sec.depl.req.network.bastion">
   <title>Accessing the &admserv; via a Bastion Network</title>
   <para>
    If you want to enable access to the &admserv; from another network, you
    can do so by providing an external gateway. This option offers maximum
    flexibility, but requires additional machines and may be less secure
    than you require. Therefore &cloud; offers a second option for accessing
    the &admserv;: the bastion network. You just need a dedicated ethernet
    card and a static IP address from the external network to set it up.
   </para>
   <para>
    The bastion network setup enables you to log in to the &admserv; via SSH
    (see <xref linkend="sec.depl.req.network.bastion"/> for setup
    instructions).  A direct login to other nodes in the cloud is not
    possible. However, the &admserv; can act as a <quote>jump host</quote>: To
    log in to a node, first log in to the &admserv; via SSH. From there, you
    can <quote>ssh</quote> to other nodes.
   </para>
  </sect2>

  <sect2 id="sec.depl.req.network.dns">
   <title>DNS and Hostnames</title>
   <para>
    The &admserv; acts as a name server for all nodes in the cloud. If the
    admin node has access to the outside, then you can add additional name
    servers that will automatically be used to forward requests. If additional
    name servers are found on cloud deployment, the name server on the
    &admserv; will automatically be configured to forward requests for
    non-local records to those servers.
   </para>
   <para>
    The &admserv; needs to be configured to have a fully qualified hostname.
    This hostname must not be changed after &cloud; has been deployed. The
    &ostack; nodes will be named after their MAC address by default, but you
    can provide aliases, which are easier to remember when allocating the
    nodes. The aliases for the &ostack; nodes can be changed at any time. It
    is useful to have a list of MAC addresses and the intended use of the
    corresponding host at hand when deploying the &ostack; nodes.
   </para>
  </sect2>
 </sect1>
 <sect1 id="sec.depl.req.repos">
  <title>Product and Update Repositories</title>

  <para>
   In order to deploy &cloud; and to be able to keep a running &cloud;
   up-to-date, a total of seven software repositories is needed. This
   includes the static product repositories, which do not change over the
   product life cycle and the update repositories, which constantly change.
   The following repositories are needed:
  </para>

  <variablelist>
   <varlistentry>
    <term>&sls; 11 SP3 Product</term>
    <listitem>
     <para>
      The &sls; 11 SP3 product repository is a copy of the installation
      media (DVD1) for &sls;. As of &productname; &productnumber; it is
      required to have it available locally on the &admserv;. This
      repository requires approximately 3.5 GB of hard disk space.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&productname; &productnumber; Product (SLE-Cloud)</term>
    <listitem>
     <para>
      The &productname; &productnumber; product repository is a copy of the
      installation media for &cloud;. It can either be made available remote
      via http or locally on the &admserv;. The latter is recommended, since
      it makes the setup of the &admserv; easier. This repository requires
      approximately 350 MB of hard disk space.
     <!-- TODO vuntz 2013-12-04: for cloud 3, it seems to be more like 450? -->
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Cloud-PTF (SLE-Cloud-PTF)</term>
    <listitem>
     <para>
      A repository created automatically on the &admserv; upon the &cloud;
      add-on product installation. It serves as a repository for
      <quote>Program Temporary Fixes</quote> (PTF) which are part of the
      SUSE support program.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SLES11-SP3-Pool and SUSE-Cloud-3-Pool</term>
    <listitem>
     <para>
      The &sls; and &productname; repositories containing all binary RPMs from
      the installation media, plus pattern information and support status
      metadata. These repositories are served from &ncc; and need to be kept
      in sync with their sources. They can be made available remotely via an
      existing &smt; or &susemgr; server or locally on the &admserv; by
      installing a local &smt; server, by mounting or syncing a remote
      directory or by copying them.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SLES11-SP3-Updates and SUSE-Cloud-3-Updates</term>
    <listitem>
     <para>
      These repositories contain maintenance updates to packages in the
      corresponding Pool repositories. These repositories are served from
      &ncc; and need to be kept in sync with their sources. They can be made
      available remotely via an existing &smt; or &susemgr; server or
      locally on the &admserv; by installing a local &smt; server, by
      mounting or syncing a remote directory or by regularly copying them.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   Since the product repositories (for &sls; 11 SP3 and &productname;
   &productnumber;) do not change during the life cycle of a product they
   can be copied to the destination directory from the installation media.
   The update repositories however, need to be kept in sync with their
   sources, the &ncc;. &suse; offers two products taking care of
   synchronizing repositories and making them available within your
   organization: &susemgr;
   (<ulink
   url="http://www.suse.com/products/suse-manager/"/> and
   &smtool; (&smt;,
   <ulink url="http://www.suse.com/solutions/tools/smt.html"/>).
  </para>

  <para>
   All repositories need to be served via <literal>http</literal> in order
   to be available for &cloud; deployment. Repositories that are directly
   available on the &admserv; are made available by the Apache Web server
   running on the &admserv;. If your organization already uses &susemgr; or
   &smt;, you can use the repositories provided by these servers.
  </para>

  <para>
   Making the repositories locally available on the &admserv; makes setting
   up the &admserv; more complicated, but has the advantage of a simple
   network setup within &cloud;. It also allows you to seal off the &cloud;
   network from other networks in your organization. Using a remote server
   as a source for the repositories has the advantage of using existing
   resources and services. It also makes setting up the &admserv; much
   easier, but requires a custom network setup for &cloud;, since the
   &admserv; needs to be able to access the remote server.
  </para>

  <variablelist>
   <varlistentry>
    <term>Using a remote &smt; Server</term>
    <listitem>
     <para>
      If you already run an &smt; server within your organization, you can
      use it within &cloud;. When using a remote &smt; server, update
      repositories are served directly from the &smt; server. Each node is
      configured with these repositories upon its initial setup.
      </para>
      <para>
       The &smt; server needs to be accessible from the &admserv; and all
       nodes in &cloud; (via one or more gateways). Resolving the server's
       hostname also needs to work.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Using a &susemgr; Server</term>
    <listitem>
     <para>
      Each client that is managed by &susemgr; needs to register with the
      &susemgr; server. Therefore the &susemgr; support can only be
      installed after the nodes have been deployed. In order to also be able
      to use repositories provided by &susemgr; during node deployment,
      &sls; 11 SP3 must be set up for autoinstallation on the &susemgr;
      server.
     </para>
     <para>
      The server needs to be accessible from the &admserv; and all nodes in
      &cloud; (via one or more gateways). Resolving the server's hostname
      also needs to work.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Installing a &smtool; (&smt;) Server on the &admserv;</term>
    <listitem>
     <para>
      The &smt; server, a free add-on product for &sls;, regularly
      synchronizes repository data from &ncc; with your local host.
      Installing the &smt; server on the &admserv; is recommended if you do
      not have access to update repositories from elsewhere within your
      organization. This option requires the &admserv; to be able to access
      the Internet. &smtool; 11 SP3 is available for free from
      <ulink
      url="http://www.suse.com/solutions/tools/smt.html"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><quote>Sneakernet</quote>
    </term>
    <listitem>
     <para>
      If you choose to completely seal off your admin network from all other
      networks, you need to manually update the repositories from removable
      media. For this purpose copy the repositories from an existing &smt;
      or &suse; Manager server to the removable media.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Utilizing Existing Repositories</term>
    <listitem>
     <para>
      If you can access existing repositories from within your company
      network from the &admserv;, you can either mount or sync these
      repositories from an existing &smt; or &suse; Manager server to the
      required locations on the &admserv;.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.depl.req.storage">
  <title>Persistent Storage</title>

  <para>
   When talking about <quote>persistent storage</quote> on &productname;,
   there are two completely different aspects to discuss: the block and
   object storage services &cloud; offers on the one hand and the hardware
   related storage aspects on the different node types.
  </para>

  <note>
   <title>Persistent vs. Ephemeral Storage</title>
   <para>
    Block and object storage are persistent storage models where files or
    images are stored until they are explicitly deleted. &cloud; also offers
    ephemeral storage for images attached to &vmguest;s. These ephemeral
    images only exist during the life of a &vmguest; and are deleted once
    the guest is terminated. See
    <xref
    linkend="sec.depl.req.storage.hardware.compute"/> for more
    information.
   </para>
  </note>

  <sect2 id="sec.depl.req.storage.services">
   <title>Cloud Storage Services</title>
   <para>
    As mentioned above, &cloud; offers two different types of services for
    persistent storage: object and block storage. Object storage lets you
    upload and download files (similar to an FTP server), whereas a block
    storage provides mountable devices (similar to a hard-disk partition).
    Furthermore &cloud; provides a repository to store the virtual disk
    images used to start &vmguest;s.
   </para>
   <variablelist>
    <varlistentry>
     <term>Object Storage with &o_objstore;</term>
     <listitem>
      <para>
       The &ostack; object storage service is called &o_objstore;. The
       storage component of &o_objstore; (swift-storage) needs to be
       deployed on dedicated nodes where no other cloud services run. In
       order to be able to store the objects redundantly, it is required to
       deploy at least two &o_objstore; nodes. &productname; is configured
       to always use all unused disks on a node for storage.
      </para>
      <para>
       &o_objstore; can optionally be used by &o_img;, the service that
       manages the images used to boot the &vmguest;s. Offering object
       storage with &o_objstore; is optional.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Block Storage</term>
     <listitem>
      <para>
       Block storage on &cloud; is provided by &o_blockstore;.
       &o_blockstore; can use a variety of storage backends, among them
       network storage solutions like NetApp or EMC. It is also possible to
       use local disks for block storage.
      </para>
      <para>
       Alternatively, &o_blockstore; can use &ceph; RBD as a backend. &ceph;
       offers data security and speed by storing the devices redundantly on
       different servers. &ceph; needs to be deployed on dedicated nodes
       where no other cloud services run. In order to be able to store the
       objects redundantly, it is required to deploy at least two &ceph;
       nodes.
      </para>
      &no-ceph-support;
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>The &o_img; Image Repository</term>
     <listitem>
      <para>
       &o_img; provides a catalog and repository for virtual disk images
       used to start the &vmguest;s. &o_img; is installed on a &contrnode;.
       It either uses &o_objstore; or a directory on the &contrnode; to
       store the images. The image directory can either be a local directory
       or an NFS share.
      </para>
     <!-- TODO vuntz 2013-12-04: ceph can also be used as a backend in Cloud 3 (and update coming to Cloud 2.0); also in Cloud 3, I think glance can store images in a cinder volume (?) - need to double-check -->
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 id="sec.depl.req.storage.hardware">
   <title>Storage Hardware Requirements</title>
   <para>
    Apart from sufficient disk space to install the &sls; operating system,
    each node in &cloud; has to store additional data. Requirements and
    recommendations for the various node types are listed below.
   </para>
   <important>
     <!-- TODO vuntz 2013-12-04: hrm, double-check with dirk, but I think it's wrong now? It's using the first disk in PCI order? -->
    <title>Choose a Hard Disk for the Operating System Installation</title>
    <para>
     The operating system will always be installed on the
     <emphasis>first</emphasis> hard disk, the one that is recognized as
     <filename>/dev/sda</filename>. This is the disk that is listed
     <emphasis>first</emphasis> in the BIOS, the one from which the machine
     will boot. If you have nodes with a certain hard disk you want the
     operating system to be installed on, make sure it will be recognized as
     the first disk.
    </para>
   </important>
   <sect3 id="sec.depl.req.storage.hardware.admin">
    <title>&admserv;</title>
    <para>
     If you store the update repositories directly on the &admserv; (see
     <xref
     linkend="sec.depl.req.repos"/> for details), it is
     recommended to mount <filename>/srv</filename> to a separate partition
     or volume with a minimum of &repospace; space.
    </para>
    <para>
     Log files from all nodes in &cloud; are stored on the &admserv; under
     <filename>/var/log</filename> (see <xref
     linkend="sec.deploy.logs.adminserv"/> for a complete list). Furthermore,
     the message service RabbitMQ requires 1 GB of free space in
     <filename>/var</filename>. Make sure sufficient space is available under
     <filename>/var</filename>.
    </para>
   </sect3>
   <sect3 id="sec.depl.req.storage.hardware.control">
    <title>&contrnode;s</title>
    <para>
     Depending on how the services are set up, &o_img; and &o_blockstore; may
     require additional disk space on the &contrnode; on which they are
     running. &o_img; may be configured to use a local directory, whereas
     &o_blockstore; may use a local image file for storage. For performance
     and scalability reasons this is only recommended for test setups. Make
     sure there is sufficient free disk space available if you use a local
     file for storage.
    </para>
    <para>
     &o_blockstore; may be configured to use local disks for storage
     (configuration option <literal>raw</literal>). If you choose this
     setup, it is recommended to deploy the <guimenu>cinder-volume</guimenu>
     role to one or more dedicated &contrnode;s equipped with several disks
     providing sufficient storage space. It may also be necessary to equip
     this node with two or more bonded network cards (requiring a special
     setup for this node, refer to <xref linkend="app.deploy.network_json"/>
     for details), since it will generate heavy network traffic.
    </para>
   </sect3>
   <sect3 id="sec.depl.req.storage.hardware.compute">
    <title>&compnode;s</title>
    <para>
     Unless an &vmguest; is started via <quote>Boot from Volume</quote>, it
     is started with at least one disk&mdash;a copy of the image from which
     it has been started. Depending on the flavor you start, the &vmguest;
     may also have a second, so-called <quote>ephemeral</quote> disk. The
     size of the root disk depends on the image itself, while ephemeral
     disks are always created as sparse image files that grow (up to a
     defined size) when being <quote>filled</quote>. By default ephemeral
     disks have a size of 10 GB.
    </para>
    <para>
     Both disks, root images and ephemeral disk, are directly bound to the
     &vmguest; and are deleted when the &vmguest; is terminated. Therefore
     these disks are bound to the &compnode; on which the &vmguest; has been
     started. The disks are created under <filename>/var/lib/nova</filename>
     on the &compnode;. Your &compnode;s should be equipped with enough disk
     space to store the root images and ephemeral disks.
    </para>
    <note>
     <title>Ephemeral Disks vs. Block Storage</title>
     <para>
      Do not confuse ephemeral disks with persistent block storage. In
      addition to an ephemeral disk, which is automatically provided with
      most &vmguest; flavors, you can optionally add a persistent storage
      device provided by &o_blockstore;. Ephemeral disks are deleted when
      the &vmguest; terminates, while persistent storage devices can be
      reused in another &vmguest;.
     </para>
    </note>
    <para>
     The maximum disk space required on a compute node depends on the
     available flavors. A flavor specifies the number of CPUs, as well as
     RAM and disk size of an &vmguest;. Several flavors ranging from
     <guimenu>tiny</guimenu> (1 CPU, 2512 MB RAM, no ephemeral disk) to
     <guimenu>xlarge</guimenu> (8 CPUs, 8 GB RAM, 10 GB ephemeral disk) are
     available by default. Adding custom flavors as well as editing and
     deleting existing flavors is also supported.
    </para>
    <para>
     To calculate the minimum disk space needed on a compute node, you need
     to determine the highest "disk space to RAM" ratio from your flavors.
     Example:
    </para>
    <simplelist>
     <member>
      Flavor small: 2 GB RAM, 100 GB ephemeral disk => 50 GB disk /1 GB RAM
     </member>
     <member>
      Flavor large: 8 GB RAM, 200 GB ephemeral disk => 25 GB disk /1 GB RAM
     </member>
    </simplelist>
    <para>
     So, 50 GB disk /1 GB RAM is the ratio that matters. If you multiply
     that value by the amount of RAM in GB available on your compute node,
     you have the minimum disk space required by ephemeral disks. Pad that
     value with sufficient space for the root disks plus a buffer that
     enables you to create flavors with a higher disk space to RAM ratio in
     the future.
    </para>
    <warning>
     <title>Overcommitting Disk Space</title>
     <para>
      The scheduler that decides in which node an &vmguest; is started does
      not check for available disk space. If there is no disk space left on
      a compute node, this will not only cause data loss on the &vmguest;s,
      but the compute node itself will also stop operating. Therefore you
      must make sure all compute nodes are equipped with enough hard disk
      space!
     </para>
    </warning>
   </sect3>
   <sect3 id="sec.depl.req.storage.hardware.store">
    <title>Storage Nodes</title>
    <para>
     The block-storage service &ceph; RBD and the object storage service
     &swift; need to be deployed onto dedicated nodes&mdash;it is not
     possible to mix these services. Each storage service requires at least
     two machines (more are recommended) to be able to store data
     redundantly.
    </para>
    <para>
     Each &ceph;/&swift; &stornode; needs at least two hard disks. The first
     one will be used for the operating system installation, while the others
     can be used for storage purposes.  It's recommended to equip the storage
     nodes with as many disks as possible.
    </para>
    <para>
     Using RAID on &o_objstore; storage nodes is not supported. Swift takes
     care of redundancy and replication on its own. Using RAID with
     &o_objstore; would also result in a huge performance penalty.
    </para>
    &no-ceph-support;
   </sect3>
  </sect2>
 </sect1>
 <sect1 id="sec.depl.req.ssl">
  <title>SSL Encryption</title>
  <para>
   Whenever non-public data travels over a network it needs to be encrypted.
   Encryption protects the integrity and confidentiality of data. Therefore
   you should enable SSL support when deploying &cloud; to production (it is
   not enabled by default since it requires certificates to be provided). The
   following services (and their APIs if available) can make use of SSL:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     &o_netw;
    </para>
   </listitem>
   <listitem>
    <para>
     &o_ident;
    </para>
   </listitem>
   <listitem>
    <para>
     &o_img;
    </para>
   </listitem>
   <listitem>
    <para>
     &o_blockstore;
    </para>
   </listitem>
   <listitem>
    <para>
     &o_comp;
    </para>
   </listitem>
   <listitem>
    <para>
     VNC
    </para>
   </listitem>
   <listitem>
    <para>
     &ostack; &dash;
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Using SSL requires an SSL certificate either for each node on which the
   services that uses encryption run (services sharing a certificate) or,
   alternatively, a dedicated certificate for each service. A single
   certificate for the &contrnode; is the minimum requirement, where all
   services listed above are installed on the &contrnode; and are sharing
   the certificate.
  </para>

  <para>
   Certificates must be signed by a trusted authority. Refer to
   <ulink
   url="http://www.suse.com/documentation/sles11/book_sle_admin/data/sec_apache2_ssl.html"/>
   for instructions on how to create and sign them.
  </para>

  <important>
   <title>Host Names</title>
   <para>
    Each SSL certificate is issued for a certain host name and, optionally,
    for alternative hostnames (via the <literal>AlternativeName</literal>
    option). Each publicly available node in &cloud; has two
    hostnames&mdash;an internal and a public one. The SSL certificate needs to
    be issued for both names.
   </para>
   <para>
    The internal name has the following scheme:
   </para>
<screen>d<replaceable>MAC ADDRESS</replaceable>.<replaceable>FQDN</replaceable></screen>
   <para>
    <replaceable>MAC ADDRESS</replaceable> is the MAC address of the
    interface used to PXE boot the machine with lowercase letters and colons
    replaced with dashes, for example <literal>52-54-00-8e-ce-e3</literal>.
    <replaceable>FQDN</replaceable> is the fully qualified domain name. An
    example name looks like this:
   </para>
<screen>d52-54-00-8e-ce-e3.&exampledomain;</screen>
   <para>
    Unless you have entered a custom <guimenu>Public Name</guimenu> for a
    client (see <xref linkend="sec.depl.inst.nodes.install"/> for details),
    the public name is the same as the internal name prefixed by
    <literal>public.</literal>:
   </para>
<screen>public.d52-54-00-8e-ce-e3.&exampledomain;</screen>
   <para>
    To look up the node names open the &crow; Web interface and click on a
    node name in the <guimenu>Node Dashboard</guimenu>. The names are listed
    as <guimenu>Full Name</guimenu> and <guimenu>Public Name</guimenu>.
   </para>
  </important>
 </sect1>
 <sect1 id="sec.depl.req.hardware">
  <title>Hardware Requirements</title>

  <para>
   Precise hardware requirements can only be listed for the &admserv; and
   the &ostack; &contrnode;. The requirements of the &ostack; Compute and
   &stornode;s depends on the number of concurrent &vmguest;s and their
   virtual hardware equipment.
  </para>
  <para>
   The minimum number of machines required for a &cloud; setup is three: one
   &admserv;, one &contrnode;, and one &compnode;. In addition to that, a
   gateway providing access to the public network is required. Deploying
   storage requires additional nodes: at least two nodes for &o_objstore; and
   a minimum of four nodes for &ceph; (technology preview only).
  </para>

  <important>
   <title>Physical Machines and Architecture</title>
   <para>
    All &cloud; nodes need to be physical machines. Although the &admserv;
    and the &contrnode; can be virtualized in test environments, this is not
    supported for production systems.
   </para>
   <para>
    &cloud; currently only runs on <literal>x86_64</literal> hardware.
   </para>
  </important>

  <sect2 id="sec.depl.req.hardware.admserv">
   <title>&admserv;</title>
   <itemizedlist>
    <listitem>
     <para>
      Architecture: x86_64
     </para>
    </listitem>
    <listitem>
     <para>
      RAM: at least 2 GB, 4 GB recommended
     </para>
    </listitem>
    <listitem>
     <para>
      Hard disk: at least 50 GB. It is recommended to put
      <filename>/srv</filename> on a separate partition with at least
      additional 30 GB of space, unless you mount the update repositories from
      another server (see <xref linkend="sec.depl.req.repos"/> for details).
     </para>
    </listitem>
    <listitem>
     <para>
      Number of network cards: 1 for single and dual mode, 2 or more for
      team mode. Additional networks such as the bastion network and/or a
      separate BMC network each need an additional network card. See
      <xref
       linkend="sec.depl.req.network"/> for details.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 id="sec.depl.req.hardware.contrnode">
   <title>&contrnode;</title>
   <itemizedlist>
    <listitem>
     <para>
      Architecture: x86_64
     </para>
    </listitem>
    <listitem>
     <para>
      RAM: at least 2 GB, 12 GB recommended (when deploying a single &contrnode;)
     </para>
    </listitem>
    <listitem>
     <para>
      Number of network cards: 1 for single mode, 2 for dual mode, 2 or more
      for team mode. See <xref linkend="sec.depl.req.network"/> for details.
     </para>
    </listitem>
    <listitem>
     <para>
      Hard disk: See
      <xref
       linkend="sec.depl.req.storage.hardware.control"/>.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 id="sec.depl.req.hardware.compnode">
   <title>&compnode;</title>
   <para>
    The &compnode;s need to be equipped with a sufficient amount of RAM and
    CPUs, matching the numbers required by the maximum number of &vmguest;s
    running concurrently. An &vmguest; started in &cloud; cannot share
    resources from several physical nodes, but rather uses the resources of
    the node on which it was started. So if you offer a flavor (see
    <xref
     linkend="gloss.flavor"/> for a definition) with 8 CPUs and 12
    GB RAM, at least one of your nodes should be able to provide these
    resources.
   </para>
   <para>
    See <xref linkend="sec.depl.req.storage.hardware.compute"/> for storage
    requirements.
   </para>
  </sect2>

  <sect2 id="sec.depl.req.hardware.stornode">
   <title>&stornode;</title>
   <para>
    The &stornode;s are sufficiently equipped with a single CPU and 1 or 2
    GB of RAM. See
    <xref
     linkend="sec.depl.req.storage.hardware.store"/> for storage
    requirements.
   </para>
  </sect2>
 </sect1>
 <sect1 id="sec.depl.req.software">
  <title>Software Requirements</title>

  <para>
   The following software requirements need to be met in order to install
   &cloud;:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     &sls; 11 SP3 installation media (ISO image, included in the &cloud;
     &admserv; subscription)
    </para>
   </listitem>
   <listitem>
    <para>
     Access to the &sls; 11 SP3 Update repositories
    </para>
   </listitem>
   <listitem>
    <para>
     &productname; &productnumber; installation media (ISO image)
    </para>
   </listitem>
   <listitem>
    <para>
     Access to the &productname; &productnumber; Update repositories
    </para>
   </listitem>
   <listitem>
    <para>
     A &suse;/&novell; account (for product registration and &smt; setup).
     If you do not already have one, go to
     <ulink
     url="http://www.suse.com/login"/> to create it.
    </para>
   </listitem>
   <listitem>
    <para>
     Optional: &smtool; 11 SP3 installation media. A free download is
     available on <ulink url="http://www.novell.com/linux/smt/"/>. See
     <xref
     linkend="sec.depl.req.repos"/>.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 id="sec.depl.req.ha">
  <title>&ha;</title>
  <para>Several components and services in &productname; can become single
   points of failure that may cause system downtime and/or data loss if they
   fail.</para>
  <para>&productname; provides various mechanisms which can ensure that the
   crucial components and services are highly available. The following sections
   provide an overview of which components on each node you should consider to
   make highly available. For making the &contrnode; functions highly
   available, &productname; uses the cluster software &sle; &hasi;.
   Make sure to thoroughly read <xref linkend="sec.depl.reg.ha.general"/> that
   lists additional requirements with regard to that.</para>
  <sect2 id="sec.depl.req.ha.admin">
   <title>&ha; of the &admserv;</title>
   <para>The &admserv; provides all services needed to manage and deploy all
    other nodes in the cloud. If the &admserv; is not available, new cloud
    nodes cannot be allocated, and you cannot add new roles to cloud
    nodes.</para>
   <para>However, only two services on the &admserv; are single point of
    failures, without which the cloud cannot continue to run properly: DNS and
    NTP.</para>
   <sect3 id="sec.depl.req.ha.admin.spof">
    <title>&admserv;&mdash;Avoiding Points of Failure</title>
    <procedure>
      <step>
      <para>To avoid DNS and NTP as potential points of failure, deploy the
       roles <systemitem>dns-server</systemitem> and
        <systemitem>ntp-server</systemitem> to multiple nodes.</para>
      <note>
       <title>Access to External Network</title>
       <para>If any configured DNS forwarder or NTP external server is not
        reachable through the admin network from these nodes, allocate an address in the public
        network for each node that has the  <systemitem>dns-server</systemitem> and
        <systemitem>ntp-server</systemitem> roles:</para>
       <screen>crowbar network allocate_ip default `hostname -f` public host</screen>
       <para>That way, the nodes will be able to use the public gateway to reach
        the external servers. The change will only become effective after the
        next run of <command>chef-client</command> on the affected nodes.
       </para>
       </note>
     </step>
    </procedure>
   </sect3>

   <sect3 id="sec.depl.req.ha.admin.recover">
    <title>&admserv;&mdash;Recovery</title>
    <para>To minimize recovery time for the &admserv;, follow the backup and
     restore recommendations described in FIXME<remark>taroth 2014-03-31: DEVs, there
     is a script for this - where is it located? any documentation or help for
     it?</remark>.</para>
   </sect3>
  </sect2>

  <sect2 id="sec.depl.reg.ha.control">
   <title>&ha; of the &contrnode;(s)</title>
   <para>The &contrnode;(s) usually run a variety of services without which
    the cloud would not be able to run properly.</para>
   <sect3 id="sec.depl.reg.ha.control.spof">
    <title>&contrnode;(s)&mdash;Avoiding Points of Failure</title>
    <para> To prevent the cloud from avoidable downtime in case one or more
     &contrnode;s fail, &productname; offers you the choice to make the
     following roles highly available:</para>
    <itemizedlist>
     <listitem>
      <para><systemitem>database-server</systemitem>
        (<systemitem>database</systemitem> &barcl;)</para>
     </listitem>
     <listitem>
      <para>
       <systemitem>keystone-server</systemitem>
        (<systemitem>keystone</systemitem> &barcl;)</para>
     </listitem>
     <listitem>
      <para><systemitem>rabbitmq-server</systemitem>
        (<systemitem>rabbitmq</systemitem> &barcl;)</para>
     </listitem>
     <listitem>
      <para>
       <systemitem>swift-proxy</systemitem> (<systemitem>swift</systemitem>
       &barcl;)</para>
     </listitem>
     <listitem>
      <para>
       <systemitem>glance-server</systemitem> (<systemitem>glance</systemitem>
       &barcl;)</para>
     </listitem>
     <listitem>
      <para>
       <systemitem>cinder-controller</systemitem>
        (<systemitem>cinder</systemitem> &barcl;)</para>
     </listitem>
     <listitem>
      <para>
       <systemitem>neutron-server</systemitem> (<systemitem>neutron</systemitem>
       &barcl;)</para>
     </listitem>
     <listitem>
      <para>
       <systemitem>neutron-l3</systemitem> (<systemitem>neutron</systemitem>
       &barcl;)</para>
     </listitem>
     <listitem>
      <para>
       <systemitem>nova-multi-controller</systemitem>
        (<systemitem>nova</systemitem> &barcl;)</para>
     </listitem>
     <listitem>
      <para>
       <systemitem>nova_dashboard-server</systemitem>
        (<systemitem>nova_dashboard</systemitem> &barcl;)</para>
     </listitem>
     <listitem>
      <para>
       <systemitem>ceilometer-server</systemitem>
        (<systemitem>ceilometer</systemitem> &barcl;)</para>
     </listitem>
     <listitem>
      <para>
       <systemitem>ceilometer-cagent</systemitem>
        (<systemitem>ceilometer</systemitem> &barcl;)</para>
     </listitem>
     <listitem>
      <para>
       <systemitem>heat-server</systemitem> (<systemitem>heat</systemitem>
       &barcl;)</para>
     </listitem>
    </itemizedlist>
    <para>Instead of assigning these roles to individual cloud nodes, you can
     assign them to one or several &ha; clusters. &productname; will
     then use the Pacemaker cluster stack (shipped with the &sle;
     &hasi;) to manage the services and to fail them over to another
     &contrnode; in case one &contrnode; fails. For details on the
     Pacemaker cluster stack and the &sle; &hasi; 11 SP3, refer to the
     &haguide;, available at <ulink
      url="https://www.suse.com/documentation/sle_ha/"/>. However, whereas
     &sle; &hasi; includes &lvs; as load-balancer, &productname;
     uses &haproxy; for this purpose (<ulink url="http://haproxy.1wt.eu/"
     />).</para>
    <para></para>
    <note>
     <title>Recommended Setup</title>
     <para>Though it is possible to use the same cluster for all of the roles
      above, the recommended setup is to use three clusters and to deploy the
      roles as follows:</para>
     <itemizedlist>
      <listitem>
       <para>
        <literal>data</literal> cluster:
         <systemitem>database-server</systemitem> and
         <systemitem>rabbitmq-server</systemitem>
       </para>
      </listitem>
      <listitem>
       <para>
        <literal>network</literal> cluster: <systemitem>neutron-l3</systemitem>
        (as the <systemitem>neutron-l3</systemitem> role may result in heavy
        network load and CPU impact)</para>
      </listitem>
      <listitem>
       <para>
        <systemitem>services</systemitem> cluster: all other roles listed above
        (as they are related to API/schedulers)</para>
      </listitem>
     </itemizedlist>
    </note>
    <important>
     <title>Cluster Requirements and Recommendations</title>
     <para>For setting up the clusters, some special requirements and
      recommendations apply. For details, refer to <xref
       linkend="sec.depl.reg.ha.general"/>.</para>
    </important>
      </sect3>
   <sect3 id="sec.depl.reg.ha.control.recover">
    <title>&contrnode;(s)&mdash;Recovery</title>
    <para>Recovery of the &contrnode;(s) is done automatically by the
     cluster software: if one &contrnode; fails, Pacemaker will fail over
     the services to another &contrnode;. If a failed &contrnode; is
     repaired and rebuilt via &crow;, it will be automatically configured to
     join the cluster, at which point Pacemaker will have the option to fail
     services back if required.</para>
   </sect3>
  </sect2>
  <sect2 id="sec.depl.reg.ha.compute">
   <title>&ha; of the &compnode;(s)</title>
   <para>If a &compnode; fails, all VMs running on that node will go down,
    too. In that case, you can relaunch all instances that are hosted on the
    failed node if you use shared storage for
     <filename>/var/lib/nova/instances</filename>. For instructions on how to do
    so, see <ulink
     url="http://docs.openstack.org/trunk/openstack-ops/content/maintenance.html#compute_node_failures"
    />, section <citetitle>Compute Node Failures and Maintenance</citetitle>. In
    addition, you can use &o_orch; to monitor VM instances and auto-restart
    them if required.
    <!-- aspiers 2014-04-02: However, there seems to be limited docs on this upstream.  
     None of the stuff I found was yet polished user-facing material suitable for linking to from our docs
     - taroth 2014-04-14: including the links here anyway -->
    For more information, see the following links:</para>
   <itemizedlist>
    <listitem>
     <para>
      <ulink url="https://wiki.openstack.org/wiki/Heat/HA"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <ulink
       url="http://docs.openstack.org/developer/heat/getting_started/index.html"
      />
     </para>
    </listitem>
    <listitem>
     <para>
      <ulink url="https://wiki.openstack.org/wiki/Heat"/>
     </para>
    </listitem>
   </itemizedlist>
   <!--aspiers 2014-04-02: https://wiki.openstack.org/wiki/NovaResiliency is also relevant 
   although probably not directly very useful for writing user docs.   -->

   <sect3 id="sec.depl.reg.ha.compute.recovery">
    <title>&compnode;(s)&mdash;Recovery</title>
    <para>A &compnode; can be recovered with the following steps:</para>
    <procedure>
     <step>
      <para>If required, install new hardware on the &compnode;.</para>
     </step>
     <step>
      <para>Reinstall the &compnode; from &crow; via &ay;. For more
       information, refer to <xref linkend="sec.depl.inst.nodes.edit"/>. </para>
     </step>
     <step>
      <para>Restart the instances that had been running on the failed
       &compnode;.</para>
     </step>
    </procedure>
   </sect3>
  </sect2>
  <sect2 id="sec.depl.reg.ha.storage">
   <title>&ha; of the &stornode;(s)</title>
   <para>&productname; offers two different types of storage that can be
    used for the &stornode;s: object storage (provided by the &ostack;
    &swift; component) and block storage (provided by &ceph;).</para>
   <para>Both already consider &ha; aspects by design, therefore it does not
    require much effort to make the storage highly available.</para>

   <sect3 id="sec.depl.reg.ha.storage.swift">
    <title>&swift;&mdash;Avoiding Points of Failure </title>
    <para>The &ostack; &objstore; replicates the data by design,
     provided the following requirements are met:</para>
    <itemizedlist>
     <listitem>
      <para>The option <guimenu>Replicas</guimenu> in the &o_objstore; &barcl;
       is set to <literal>3</literal>, the tested and recommended value.</para>
     </listitem>
     <listitem>
      <para>The number of &stornode; must be greater than the value set in
       the <guimenu>Replicas</guimenu> option.</para>
     </listitem>
    </itemizedlist>
    
    <procedure>
     <step>
      <para>To avoid single points of failure, assign the
        <systemitem>swift-storage</systemitem> role to multiple nodes.</para>
     </step>
     <step>
      <para>To make the API highly available, too, assign the
        <systemitem>swift-proxy</systemitem> role to a cluster instead of
       assigning it to a single &contrnode;. See <xref
       linkend="sec.depl.reg.ha.control.spof"/>. Other swift roles must not be
       deployed on a cluster.</para>
     </step>
    </procedure>
   </sect3>
   <sect3 id="sec.depl.reg.ha.storage.ceph">
    <title>&ceph;&mdash;Avoiding Points of Failure </title>
    <para>&ceph; is HA-enabled by design. To avoid points of failure, add at
     least one more node than your replica level to avoid a degraded mode
     without any redundancy. A replica level of three will ensure access to data
     while at least two nodes remain up, but performance may degrade. <remark>
      taroth 2014-03-27: DEVs, please see my questions in Etherpad</remark></para>
   </sect3>

  </sect2>
  <sect2 id="sec.depl.reg.ha.general">
   <title>Cluster Requirements and Recommendations</title>
   <para>When considering to set up one ore more &ha; clusters, refer to the
    chapter <citetitle>System Requirements</citetitle> listed in the
    &haguide; for &sle; &hasi; 11 SP3. The guide is available at
     <ulink url="https://www.suse.com/documentation/sle_ha/"/>. If you want to
    make the &contrnode; functions highly available, the requirements listed
    there also apply to &productname;. Note that by buying
    &productname;, you automatically get an entitlement for &sle;
    &hasi;.</para>
   <para>Especially note the following requirements: </para>
   <variablelist>
    <varlistentry id="vle.ha.req.nodes">
     <term>Number of Cluster Nodes</term>
     <listitem>
      <para>Each cluster must consist of at least two cluster nodes. </para>
      <important>
       <title>Odd Number of Cluster Nodes</title>
       <para>It is strongly recommended to use an <emphasis>odd</emphasis>
        number of cluster nodes with a <emphasis>minimum</emphasis> of three
        nodes. </para>
       <para>A cluster needs <xref linkend="gloss.quorum"
         xrefstyle="select:label nopage"/> to keep services running. Therefore a
        three-node cluster can tolerate only failure of one node at a time,
        whereas a five-node cluster can tolerate failures of two nodes
        etc.</para>
      </important>
     </listitem>
    </varlistentry>
    <varlistentry id="vle.ha.req.stonith">
     <term>&stonith;</term>
     <listitem>
      <para>The cluster software will shut down <quote>misbehaving</quote> nodes
       in a cluster to prevent them from causing trouble. This mechanism is
       referred to as <literal>fencing</literal> or <xref
        linkend="gloss.stonith" xrefstyle="select:titel nopage"/>.</para>
      <important>
       <title>No Support Without &stonith;</title>
       <para> A cluster without &stonith; is not supported. </para>
      </important>
      <para>For a supported &hasetup;, ensure the following:</para>
      <itemizedlist>
       <listitem>
        <para>Each node in the &ha; cluster must have at least one
         &stonith; device (usually a piece of hardware). We strongly
         recommend multiple &stonith; devices per node, unless SBD is
         used.</para>
       </listitem>
       <listitem>
        <para>The global cluster options
          <systemitem>stonith-enabled</systemitem> and
          <systemitem>startup-fencing</systemitem> must be set to
          <literal>true</literal>. These options are set automatically when
         deploying the <systemitem>Pacemaker</systemitem> &barcl;. As soon
         as you change them, you will loose support.</para>
       </listitem>
       <listitem>
        <para>When deploying the <literal>Pacemaker</literal> service, select a
          <xref linkend="vle.pacemaker.barcl.stonith"
          xrefstyle="select:label nopage"/> that matches your setup. If your
         &stonith; devices support the IPMI protocol, choosing the IPMI
         option is the easiest way to configure &stonith;. Another
         alternative is SBD (&stonith; Block Device). It provides a way to
         enable &stonith;and fencing in clusters without external power
         switches, but it requires shared storage. For SBD requirements, see
          <ulink url="http://linux-ha.org/wiki/SBD_Fencing"/>, section
          <citetitle>Requirements</citetitle>.</para>
       </listitem>
      </itemizedlist>
      <para>For more information, refer to the &haguide;, available at
        <ulink url="https://www.suse.com/documentation/sle_ha/"/>. Especially
       read the following chapters: <citetitle>Configuration and Administration
        Basics</citetitle>, and <citetitle>Fencing and
       &stonith;</citetitle>, <citetitle> Storage
       Protection</citetitle>.</para>
     </listitem>
    </varlistentry>
    <varlistentry id="vle.ha.req.communication">
     <term>Cluster Communication Channels</term>
     <listitem>
      <para> For successful communication between the cluster nodes, define at
       least one communication channel. </para>
      <important>
       <title>Redundant Communication Paths</title>
       <para> However, for a supported &hasetup;, it is required to set up
        cluster communication via two or more redundant paths. This can be done
        via:</para>
       <itemizedlist>
        <listitem>
         <para>Network device bonding. For this purpose, use teaming network
          mode in your network setup. For details, see <xref
           linkend="sec.depl.req.network.modes.teaming"/>. At least two ethernet
          cards per cluster node are required for network redundancy.</para>
        </listitem>
        <listitem>
         <para> A second communication channel in &corosync;. </para>
        </listitem>
       </itemizedlist>
       <para>If possible, choose teaming network mode (network device bonding).</para>
      </important>
      <para>For more information, refer to the &haguide;, available at
        <ulink url="https://www.suse.com/documentation/sle_ha/"/>. Especially
       read the following chapters: <citetitle>Network Device
        Bonding</citetitle>, and <citetitle>Installation and Basic
        Setup</citetitle> (section <citetitle>Defining the Communication
        Channels</citetitle>).</para>
     </listitem>
    </varlistentry>
    
    <varlistentry id="vle.ha.req.storage" >
     <term>Storage Requirements</term>
     <listitem>
      <!-- taroth 2014-04-14: https://bugzilla.novell.com/show_bug.cgi?id=873373 -->
      <para>The following services require shared storage:
        <systemitem>database-server</systemitem> and
        <systemitem>rabbitmq-server</systemitem>. For this purpose, use either
       an external an external NFS share or DRBD.</para>
      <para>If using an external NFS share, the following additional
       requirements are important:</para>
      <itemizedlist>
       <listitem>
        <para>The share it must be reliably accessible from all cluster nodes
         via redundant communication paths. See <xref
          linkend="vle.ha.req.communication"/>. </para>
       </listitem>
       <listitem>
        <!-- taroth 2014-04-14: https://bugzilla.novell.com/show_bug.cgi?id=872343 -->
        <para>The share needs to have certain settings in
          <filename>/etc/exports</filename> to be usable by the
          <systemitem>database</systemitem> &barcl;. For details, see
          FIXME<remark>taroth 2014-04-14: check with fs if he will explain this
          somewhere, otherwise add it here</remark>
        </para>
       </listitem>
      </itemizedlist>
      <para>If using DRDB, the following additional requirements are
       important:</para>
      <itemizedlist>
       <listitem>
        <para>Due to a DRBD limitation, the cluster used for
          <systemitem>database-server</systemitem> and
          <systemitem>rabbitmq-server</systemitem> is restricted to two
         nodes.</para>
       </listitem>
       <listitem>
        <para>All nodes of the cluster that is used for
          <systemitem>database-server</systemitem> and
          <systemitem>rabbitmq-server</systemitem> must have an additional hard
         disk that will be used for DRBD. For more information on DRBD, see the
          <citetitle>DRBD</citetitle> chapter in the &haguide;, which is
         available at <ulink url="https://www.suse.com/documentation/sle_ha/"/>.
        </para>
       </listitem>
      </itemizedlist>
      <para>When using SBD as &stonith; device, additional requirements apply for
       the shared storage. For
       details, see <ulink url="http://linux-ha.org/wiki/SBD_Fencing"/>,
       section <citetitle>Requirements</citetitle>.</para>
    </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
  <sect2 id="sec.depl.reg.ha.more">
   <title>For More Information</title>
   <para>For more information on the Pacemaker cluster stack and the &sle;
    &hasi; 11 SP3, refer to the &haguide;, available at <ulink
     url="https://www.suse.com/documentation/sle_ha/"/>.</para>
   <para>It also provides comprehensive information about the cluster management
    tools which you can view and check the cluster status in &productname;.
    Refer to the following chapters: </para>
   <itemizedlist>
    <listitem>
     <para>&haweb;: <citetitle>Configuring and Managing Cluster Resources
       (Web Interface)</citetitle>
     </para>
    </listitem>
    <listitem>
     <para>&hbgui;: <citetitle>Configuring and Managing Cluster Resources
       (GUI)</citetitle>
     </para>
    </listitem>
    <listitem>
     <para><command>crm.sh</command>: <citetitle> Configuring and Managing
       Cluster Resources (Command Line)</citetitle>
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

 </sect1>
 <sect1 id="sec.depl.req.summary">
  <title>Summary: Considerations and Requirements</title>

  <para>
   As outlined above, there are some important considerations to be made
   before deploying &cloud;. The following briefly summarizes what was
   discussed in detail in this chapter. Keep in mind that as of
   &productname; &productnumber; it is not possible to change some aspects
   such as the network setup once &cloud; is deployed!
  </para>

  <itemizedlist>
   <title>Network</title>
   <listitem>
    <para>
     If you do not want to stick with the default networks and addresses,
     define custom networks and addresses. You need five different networks.
     If you need to separate the admin and the BMC network, a sixth network is
     required. See <xref linkend="sec.depl.req.network"/> for
     details. Networks that share interfaces need to be configured as VLANs.
    </para>
   </listitem>
   <listitem>
    <para>
     The &cloud; networks are completely isolated, therefore it is not
     required to use public IP addresses for them. A class C network as used
     in this documentation may not provide enough addresses for a cloud that
     is supposed to grow. You may alternatively choose addresses from a class
     B or A network.
    </para>
   </listitem>
   <listitem>
    <para>
     Determine how to allocate addresses from your network. Make sure not to
     allocate IP addresses twice. See
     <xref
     linkend="sec.depl.req.network.allocation"/> for the default
     allocation scheme.
    </para>
   </listitem>
   <listitem>
    <para>
     Define which network mode to use. Keep in mind that all machines within
     the cloud (including the &admserv;) will be set up with the chosen mode
     and therefore need to meet the hardware requirements. See
     <xref
      linkend="sec.depl.req.network.modes"/> for details.
    </para>
   </listitem>
   <listitem>
    <para>
     Define how to access the admin and BMC network(s): no access from the
     outside (no action is required), via an external gateway (gateway needs
     to be provided), or via bastion network. See
     <xref
      linkend="sec.depl.req.network.bastion"/> for details.
    </para>
   </listitem>
   <listitem>
    <para>
     Provide a gateway to access the public network (public, nova-floating).
    </para>
     <!-- TODO vuntz 2013-12-04: check if also for fixed? -->
   </listitem>
   <listitem>
    <para>
     Make sure the admin server's hostname is correctly configured
     (<command>hostname <option>-f</option></command> needs to return a
     fully qualified hostname).
    </para>
   </listitem>
   <listitem>
    <para>
     Prepare a list of MAC addresses and the intended use of the
     corresponding host for all &ostack; nodes.
    </para>
   </listitem>
  </itemizedlist>

  <itemizedlist>
   <title>Update Repositories</title>
   <listitem>
    <para>
     Depending on your network setup you have different options on how to
     provide up-to-date update repositories for &sls; and &cloud; for
     &cloud; deployment: using an existing &smt; or &susemgr; server,
     installing &smt; on the &admserv;, syncing data with an existing
     repository, mounting remote repositories or using a
     <quote>Sneakernet</quote>. Choose the option that best matches your
     needs.
    </para>
   </listitem>
  </itemizedlist>

  <itemizedlist>
   <title>Storage</title>
   <listitem>
    <para>
     Decide whether you want to deploy the object storage service &swift;.
     If so, you need to deploy at least two nodes with sufficient disk space
     exclusively dedicated to &swift;.
    </para>
   </listitem>
   <listitem>
    <para>
     Decide which backend to use with &o_blockstore;. If using the
     <guimenu>raw</guimenu> backend (local disks) it is strongly recommended
     to use a separate node equipped with several hard disks for deploying
     <literal>cinder-volume</literal>. If using &ceph;, you need to deploy
     at least two nodes with sufficient disk space exclusively dedicated to
     it.
    </para>
    &no-ceph-support;
   </listitem>
   <listitem>
    <para>
     Make sure all &compnode;s are equipped with sufficient hard disk space.
    </para>
   </listitem>
  </itemizedlist>

  <itemizedlist>
   <title>SSL Encryption</title>
   <listitem>
    <para>
     Decide whether to use different SSL certificates for the services and
     the API or whether to use a single certificate.
    </para>
   </listitem>
   <listitem>
    <para>
     Get one or more SSL certificates certified by a trusted third party
     source.
    </para>
   </listitem>
  </itemizedlist>

  <itemizedlist>
   <title>Hardware and Software Requirements</title>
   <listitem>
    <para>
     Make sure the hardware requirements for the different node types are
     met.
    </para>
   </listitem>
   <listitem>
    <para>
     Make sure to have all required software at hand.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
<!--
 <sect1 id="sec.depl.arch.security">
  <title>Security Considerations</title>

  <para>FIXME</para>
   * https://github.com/SUSE/cloud/wiki/Common-issues
   * https://github.com/SUSE/cloud/wiki/Nova-compute
   * enabling SSL

    Which communication will be encrypted?

   
     Needs a certificate
      -> point to SLES Apache doc for HowTo
      -> sufficient to put it on Admin node in /etc/apache2/ssl.*/

Passwords:

* SMT DB Password (change via yast)
* MySQL Root Password (MySQL is used for SMT)
* Crowbar Password (change cia yast)
* Chef WebUI Password:
  - initially change via yast (before install-suse-cloud run)
  - in the running system change via
    * Login to the Crowbar Web UI on port 3000:
      * Click on Barclamps in the top right corner of the Web UI.
      * Click on the arrow to the left of the Crowbar barclamp to expand it
      * Click the Edit button to edit the Default proposal.
      * Click on Raw to the right of Attributes.
      * Scroll through the attributes until you see something that looks like:

    "users": {
      "machine-install": {
        "password": "0e7b02a8d2086009c1ff00cc26a827d981306cbd420b1862063e6df8534e0f6a13e45100b9874d0a3fae5962c9ec2de12c0525b8c33685e8ee30406c4eee7133"
      },
      "crowbar": {
        "password": "crowbar"
      }
    },

    * Change the password.
    * Click Apply and confirm you want to apply the changes.
 - nodes root-Password (change in autoyast.xml.erb -> how? hashed, clear text?)

* Keystone
  - "Regular User" Name/Password
  - "Administrator" Name/Password
  Change via Keystone Barclamp

* Glance
  - Service User: Glance User in Keystone
  - Service Password: PW for Service User
  Change via Glance Barclamp

* Swift
  Cluster Admin Password
  Change via Swift Barclamp
    
-->
