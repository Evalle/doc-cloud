<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet
 href="urn:x-daps:xslt:profiling:novdoc-profile.xsl" 
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "IGNORE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.depl.req">
 <title>Considerations and Requirements</title>
 <abstract>
  <para>
   Before deploying &productname;, there are a few requirements to be met
   and considerations to be made. Make sure to thoroughly read this
   chapter&mdash;some decisions need to be made <emphasis>before</emphasis>
   deploying &cloud;, since you cannot change them afterwards.
  </para>
 </abstract>
 <sect1 id="sec.depl.req.network">
  <title>Network</title>

  <para>
   &productname; requires a complex network setup consisting of several
   networks that are configured during installation. These networks are for
   exclusive cloud usage. In order to access them from an existing network,
   a router is needed.
  </para>

  <para>
   The network configuration on the nodes in the &cloud; network is entirely
   controlled by &crow;. Any network configuration not done with &crow;
   (e.g. with &yast;) will automatically be overwritten. Once the cloud is
   deployed, network settings cannot be changed anymore!
  </para>

  <figure>
   <title>&cloud; Network: Overview</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="cloud_network_overview.png" width="90%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="cloud_network_overview.png" width="75%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The following networks are pre-defined when setting up &productname;. The
   IP addresses listed are the default addresses and can be changed using
   the &yast; &crow; module (see
   <xref
   linkend="sec.depl.inst.admserv.os.crowbar"/>). It's also
   possible to completely customize the network setup. This requires to
   manually edit the network &barcl; template. See
   <xref linkend="app.deploy.network_json"/> for detailed instructions.
  </para>

  <variablelist>
   <varlistentry>
    <term>
     Admin Network (<systemitem
     class="etheraddress">192.168.124/24</systemitem>)
    </term>
    <listitem>
     <!-- TODO vuntz 2013-12-04: for reference, we got feedback that we should not touch the IPMI configuration at all by default, so we'll likely do this; that would mean access to the BMC would not be configured by Crowbar by default -->
     <para>
      A private network to access the &admserv; and all nodes for
      administration purposes. The default setup lets you also access the
      BMC (Baseboard Management Controller) data via IPMI (Intelligent
      Platform Management Interface) from this network. If required, BMC
      access can be swapped to a separate network.
     </para>
     <para>
      You have the following options for controlling access to this network:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        do not allow access from the outside and keep the admin network
        completely separated
       </para>
      </listitem>
      <listitem>
       <para>
        allow access to the &admserv; from a single network (e.g. your
        company's administration network) via the <quote>bastion
        network</quote> option configured on an additional network card with
        a fixed IP address
       </para>
      </listitem>
      <listitem>
       <para>
        allow access from one or more networks via a gateway
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     Storage Network (<systemitem
     class="etheraddress">192.168.125/24</systemitem>)
    </term>
    <listitem>
     <para>
      Private, &cloud; internal virtual network. This network is used by
      &ceph; and &o_objstore; only. It should not be accessed by users.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     Private Network (nova-fixed, <systemitem
     class="etheraddress">192.168.123/24</systemitem>)
    </term>
    <listitem>
     <para>
      Private, &cloud; internal virtual network. This network is used for
      inter-&vmguest; communication and provides access to the outside world
      for the &vmguest;s. The gateway required is also automatically provided
      by &cloud;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     Public Network (nova-floating, public, <systemitem
     class="etheraddress">192.168.126/24</systemitem>)
    </term>
    <listitem>
     <para>
      The only public network provided by &cloud;. You can access the Nova
      &dash; as well as &vmguest;s (provided they have been equipped with a
      floating IP) on this network. This network can only be accessed via a
      gateway, which needs to be provided externally. All &cloud; users and
      administrators need to be able to access the public network.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     Software Defined Network (os_sdn, <systemitem class="etheraddress">192.168.130/24</systemitem>)
    </term>
    <listitem>
     <para>
      <remark condition="clarity">
       2013-06-18 - fs: Is this true? 
      </remark>
     <!-- TODO vuntz 2013-12-04: this is used by neutron if we use openvswitch in gre mode, but check with rhafer -->
      Private, &cloud; internal virtual network. This network is used by
      &o_netw; openvswitch in <literal>gre</literal>-mode. It should not be
      accessed by users.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  &vlan-settings;

  <note>
   <title>No IPv6 support</title>
   <para>
    As of &productname; &productnumber;, IPv6 is not supported. This applies
    to the cloud internal networks as well as to the &vmguest;s.
   </para>
  </note>

  <para>
   The following diagram shows the pre-defined &cloud; network in more
   detail. It demonstrates how the &ostack; nodes and services use the
   different networks.
  </para>

  <figure>
   <title>&cloud; Network: Details</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="cloud_network_detail.png" width="100%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="cloud_network_detail.png" width="100%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 id="sec.depl.req.network.allocation">
   <title>Network Address Allocation</title>
   <para>
    The default networks set up in &cloud; are class C networks with 256 IP
    addresses each. This limits the maximum number of &vmguest;s that can be
    started simultaneously. Addresses within the networks are allocated as
    outlined in the following table. Use the &yast; &crow; module to customize
    (see <xref linkend="sec.depl.inst.admserv.os.crowbar"/>). The last address
    in the IP range of each network is always reserved as the broadcast
    address. This assignment cannot be changed.
   </para>
   <note>
    <title>Limitations of the Default Network Proposal</title>
    <para>
     The default network proposal as described below limits the maximum
     number of &compnode;s to 49, the maximum number of floating IP
     addresses to 61 and the maximum number of &vmguest;s to 204.
    </para>
    <para>
     To overcome this limitations you need to reconfigure the network setup by
     using appropriate network ranges.  Do this by either using the &yast;
     &crow; module as described in <xref
     linkend="sec.depl.inst.admserv.os.crowbar"/> or by manually editing the
     network template file as described in <xref
     linkend="app.deploy.network_json"/>.
    </para>
   </note>
     <!-- TODO vuntz 2013-12-04: after looking at the tables, it might be worth mentioning that the ranges that are used inside the table are not "network ranges" (as in with proper netmask, etc.); they're just arbitrary ranges inside the network range used by each table -->
   
   <table>
    <title><systemitem class="etheraddress">192.168.124.0/24</systemitem> (Admin/BMC) Network Address Allocation</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="20*"/>
     <colspec colnum="2" colname="2" colwidth="33*"/>
     <colspec colnum="3" colname="3" colwidth="47*"/>
     <thead>
      <row>
       <entry>
        <para>
         Function
        </para>
       </entry>
       <entry>
        <para>
         Address
        </para>
       </entry>
       <entry>
        <para>
         Remark
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         router
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.1</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Provided externally.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         admin
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.10</systemitem> -
         <systemitem class="etheraddress">192.168.124.11</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Fixed addresses reserved for the &admserv;.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         dhcp
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.21</systemitem> -
         <systemitem class="etheraddress">192.168.124.80</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Address range reserved for node allocation/installation. Determines
         the maximum number of parallel allocations/installations.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.81</systemitem> -
         <systemitem class="etheraddress">192.168.124.160</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Fixed addresses for the &ostack; nodes. Determines the maximum
         number of &ostack; nodes that can be deployed.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         bmc vlan host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.161</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Fixed address for the BMC VLAN. Used to generate a VLAN tagged
         interface on the Administration Server that can access the BMC
         network. The BMC VLAN needs to be in the same ranges as BMC, and BMC
         has to have VLAN enabled.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         bmc host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.162</systemitem> -
         <systemitem class="etheraddress">192.168.124.240</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Fixed addresses for the &ostack; nodes. Determines the maximum
         number of &ostack; nodes that can be deployed.
        </para>
       </entry>
      </row>
      <row>
     <!-- TODO vuntz 2013-12-04: does anyone know what is the use of this switch bit? I quickly grepped but couldn't find anything -->
       <entry>
        <para>
         switch
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.241</systemitem> -
         <systemitem class="etheraddress">192.168.124.250</systemitem>
        </para>
       </entry>
       <entry>
        <para></para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <table>
    <title><systemitem class="etheraddress">192.168.125/24</systemitem> (Storage) Network Address Allocation</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="20*"/>
     <colspec colnum="2" colname="2" colwidth="33*"/>
     <colspec colnum="3" colname="3" colwidth="47*"/>
     <thead>
      <row>
       <entry>
        <para>
         Function
        </para>
       </entry>
       <entry>
        <para>
         Address
        </para>
       </entry>
       <entry>
        <para>
         Remark
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.125.10</systemitem> -
         <systemitem class="etheraddress">192.168.125.239</systemitem>
        </para>
       </entry>
       <entry>
        <para>Each &stornode; will get an address from this range.</para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <table>
    <title><systemitem class="etheraddress">192.168.123/24</systemitem> (Private Network/nova-fixed) Network Address Allocation</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="20*"/>
     <colspec colnum="2" colname="2" colwidth="33*"/>
     <colspec colnum="3" colname="3" colwidth="47*"/>
     <thead>
      <row>
       <entry>
        <para>
         Function
        </para>
       </entry>
       <entry>
        <para>
         Address
        </para>
       </entry>
       <entry>
        <para>
         Remark
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         router
        </para>
       </entry>
       <entry>
        <para>
     <!-- TODO vuntz 2013-12-04: hrm, it's probably a bug that router is set to this value; it sounds like it shouldn't be set at all as the gateway is provided by the neutron router? -->
         <systemitem class="etheraddress">192.168.123.1</systemitem> -
         <systemitem class="etheraddress">192.168.123.49</systemitem>
        </para>
       </entry>
       <entry>
     <!-- TODO vuntz 2013-12-04: hrm, not true anymore; now routers are on network nodes; so this limits number of network nodes? (or is the router IP address actually common? -->
        <para>
         Each &compnode; also acts as a router for <quote>its</quote>
         &vmguest;s, getting an address from this range assigned. This
         effectively limits the maximum number of &compnode;s that can be
         deployed with &productname; to 49.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         dhcp
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.123.50</systemitem> -
         <systemitem class="etheraddress">192.168.123.254</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Address range for &vmguest;s.
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <table>
    <title><systemitem class="etheraddress">192.168.126/24</systemitem> (Public Network nova-floating, public) Network Address Allocation</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="20*"/>
     <colspec colnum="2" colname="2" colwidth="33*"/>
     <colspec colnum="3" colname="3" colwidth="47*"/>
     <thead>
      <row>
       <entry>
        <para>
         Function
        </para>
       </entry>
       <entry>
        <para>
         Address
        </para>
       </entry>
       <entry>
        <para>
         Remark
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         router
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.126.1</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Provided externally.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         public host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.126.2</systemitem> -
         <systemitem class="etheraddress">192.168.126.49</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Public address range for external &cloud; services such as the
         &ostack; &dash; or the API.
        </para>
       </entry>
      </row>
<!-- TODO vuntz 2013-12-04: hrm, is this actually unused now? Ralf?
     Ralf: Yes, I am pretty sure this is no longer used. I guess it
           was used for some nova-network related thing in the past
      <row>
       <entry>
        <para>
         public dhcp
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.126.50</systemitem> -
         <systemitem class="etheraddress">192.168.126.127</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         <remark condition="clarity">
 2013-05-22 - fs: Please check if this is correct
</remark>
         Public address range for &vmguest;s. These addresses are
         automatically assigned to the &vmguest;s to e.g. allow them to
         access the internet. Not to be confused with the floating IP
         addresses that are assigned by users and allow to login to the
         &vmguest; from the outside. Determines the maximum number of
         &vmguest;s that can be started concurrently.
        </para>
       </entry>
      </row>
-->
      <row>
       <entry>
        <para>
         floating host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.126.129</systemitem> -
         <systemitem class="etheraddress">192.168.126.191</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Floating IP address range. Floating IPs can be manually assigned to
         a running &vmguest; to allow to access the guest from the outside.
         Determines the maximum number of &vmguest;s that can concurrently
         be accessed from the outside.
        </para>
        <para>
         The nova_floating network is set up with a netmask of
         255.255.255.192, allowing a maximum number of 61 IP addresses. This
         range is pre-allocated by default and managed by &o_netw;.
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <table>
    <title><systemitem class="etheraddress">192.168.130/24</systemitem> (Software Defined Network) Network Address Allocation</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="20*"/>
     <colspec colnum="2" colname="2" colwidth="33*"/>
     <colspec colnum="3" colname="3" colwidth="47*"/>
     <thead>
      <row>
       <entry>
        <para>
         Function
        </para>
       </entry>
       <entry>
        <para>
         Address
        </para>
       </entry>
       <entry>
        <para>
         Remark
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.130.10</systemitem> -
         <systemitem class="etheraddress">192.168.130.254</systemitem>
        </para>
       </entry>
       <entry>
        <para>
	 If &o_netw; is configured with <literal>openvswitch</literal> and
	 <literal>gre</literal>, each network node and all &compnode;s will
	 get an IP from this range.
	</para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <note>
    <title>Addresses for Additional Servers</title>
    <para>
     Addresses not used in the ranges mentioned above, can be used to add
     additional servers with static addresses to &cloud;. Such servers can be
     used to provide additional services. A &susemgr; server inside &cloud;,
     for example, needs to be configured using one of these addresses.
    </para>
   </note>
   
  </sect2>

  <sect2 id="sec.depl.req.network.modes">
   <title>Network Modes</title>
   <para>
    &productname; supports different network modes: single, dual, and
    teaming. As of &productname; &productnumber; the networking mode is
    applied to all nodes as well as the &admserv;. That means that all
    machines need to meet the hardware requirements for the chosen mode. The
    network mode can be configured using the &yast; &crow; module
    (<xref
    linkend="sec.depl.inst.admserv.os.crowbar"/>). The network
    mode cannot be changed once the cloud is deployed.
   </para>
   <para>
    &l3-network-support;
   </para>
   <sect3 id="sec.depl.req.network.modes.single">
    <title>Single Network Mode</title>
    <para>
     In single mode you just use one ethernet card for all the traffic:
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="cloud_networking_single_mode.png" width="70%"
                  format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="cloud_networking_single_mode.png" width="50%"
                  format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </sect3>
   <sect3 id="sec.depl.req.network.modes.dual">
    <title>Dual Network Mode</title>
    <para>
     Dual mode needs two ethernet cards (on all nodes but &admserv;) and
     allows you to completely separate traffic to/from the Admin Network and
     to/from the public network:
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="cloud_networking_dual_mode.png" width="100%"
                  format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="cloud_networking_dual_mode.png" width="70%"
                  format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </sect3>
   <sect3 id="sec.depl.req.network.modes.teaming">
    <title>Teaming Network Mode</title>
    <para>
     Teaming mode is almost identical to single mode, except for the fact
     that you combine several ethernet cards to a <quote>bond</quote> in
     order to increase the performance. Teaming mode needs two or more
     ethernet cards.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="cloud_networking_team_mode.png" width="70%"
                  format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="cloud_networking_team_mode.png" width="50%"
                  format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </sect3>
  </sect2>

  <sect2 id="sec.depl.req.network.bastion">
   <title>Accessing the &admserv; via a Bastion Network</title>
   <para>
    If you want to enable access to the &admserv; from another network, you
    can do so by providing an external gateway. This option offers maximum
    flexibility, but requires additional machines and may be less secure
    than you require. Therefore &cloud; offers a second option for accessing
    the &admserv;: the bastion network. You just need a dedicated ethernet
    card and a static IP address from the external network to set it up.
   </para>
   <para>
    The bastion network setup enables you to login to the &admserv; via SSH
    (see <xref linkend="sec.depl.req.network.bastion"/> for setup
    instructions).  A direct login to other nodes in the cloud is not
    possible. However, the &admserv; can act as a <quote>jump host</quote>: To
    log in to a node, first log in to the &admserv; via SSH. From there, you
    can <quote>ssh</quote> to other nodes.
   </para>
  </sect2>

  <sect2 id="sec.depl.req.network.dns">
   <title>DNS and Hostnames</title>
   <para>
    The &admserv; acts as a name server for all nodes in the cloud. If the
    admin node has access to the outside, then you can add additional name
    servers that will automatically be used to forward requests. If additional
    name servers are found on cloud deployment, the name server on the
    &admserv; will automatically be configured to forward requests for
    non-local records to those servers.
   </para>
   <para>
    The &admserv; needs to be configured to have a fully qualified hostname.
    This hostname must not be changed after &cloud; has been deployed. The
    &ostack; nodes will be named after their MAC address by default, but you
    can provide aliases, which are easier to remember when allocating the
    nodes. The aliases for the &ostack; nodes can be changed any time. It is
    useful to have a list of MAC addresses and the intended use of the
    corresponding host at hand when deploying the &ostack; nodes.
   </para>
  </sect2>
 </sect1>
 <sect1 id="sec.depl.req.repos">
  <title>Product and Update Repositories</title>

  <para>
   In order to deploy &cloud; and to be able to keep a running &cloud;
   up-to-date, a total of seven software repositories is needed. This
   includes the static product repositories, which do not change over the
   product life cycle and the update repositories which constantly change.
   The following repositories are needed:
  </para>

  <variablelist>
   <varlistentry>
    <term>&sls; 11 SP3 Product</term>
    <listitem>
     <para>
      The &sls; 11 SP3 product repository is a copy of the installation
      media (DVD1) for &sls;. As of &productname; &productnumber; it is
      required to have it available locally on the &admserv;. This
      repository requires approximately 3.5 GB of hard disk space.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&productname; &productnumber; Product (SLE-Cloud)</term>
    <listitem>
     <para>
      The &productname; &productnumber; product repository is a copy of the
      installation media for &cloud;. It can either be made available remote
      via http or locally on the &admserv;. The latter is recommended, since
      it makes the setup of the &admserv; easier. This repository requires
      approximately 350 MB of hard disk space.
     <!-- TODO vuntz 2013-12-04: for cloud 3, it seems to be more like 450? -->
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Cloud-PTF (SLE-Cloud-PTF)</term>
    <listitem>
     <para>
      A repository created automatically on the &admserv; upon the &cloud;
      add-on product installation. It serves as a repository for
      <quote>Program Temporary Fixes</quote> (PTF) which are part of the
      SUSE support program.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SLES11-SP3-Pool and SUSE-Cloud-3-Pool</term>
    <listitem>
     <para>
      The &sls; and &productname; repositories containing all binary RPMs from
      the installation media, plus pattern information and support status
      metadata. These repositories are served from &ncc; and need to be kept
      in sync with their sources. They can be made available remotely via an
      existing &smt; or &susemgr; server or locally on the &admserv; by
      installing a local &smt; server, by mounting or syncing a remote
      directory or by copying them.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SLES11-SP3-Updates and SUSE-Cloud-3-Updates</term>
    <listitem>
     <para>
      These repositories contain maintenance updates to packages in the
      corresponding Pool repositories. These repositories are served from
      &ncc; and need to be kept in sync with their sources. They can be made
      available remotely via an existing &smt; or &susemgr; server or
      locally on the &admserv; by installing a local &smt; server, by
      mounting or syncing a remote directory or by regularly copying them.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   Since the product repositories (for &sls; 11 SP3 and &productname;
   &productnumber;) do not change during the life cycle of a product they
   can be copied to the destination directory from the installation media.
   The update repositories however, need to be kept in sync with their
   sources, the &ncc;. &suse; offers two products taking care of
   synchronizing repositories and making them available within your
   organization: &susemgr;
   (<ulink
   url="http://www.suse.com/products/suse-manager/"/> and
   &smtool; (&smt;,
   <ulink url="http://www.suse.com/solutions/tools/smt.html"/>).
  </para>

  <para>
   All repositories need to be served via <literal>http</literal> in order
   to be available for &cloud; deployment. Repositories that are directly
   available on the &admserv; are made available by the Apache Web server
   running on the &admserv;. If your organization already uses &susemgr; or
   &smt;, you can use the repositories provided by theses servers.
  </para>

  <para>
   Making the repositories locally available on the &admserv; makes setting
   up the &admserv; more complicated, but has the advantage of a simple
   network setup within &cloud;. It also allows you to seal off the &cloud;
   network from other networks in your organization. Using a remote server
   as a source for the repositories has the advantage of using existing
   resources and services. It also makes setting up the &admserv; much
   easier, but requires a custom network setup for &cloud;, since the
   &admserv; needs to be able to access the remote server.
  </para>

  <variablelist>
   <varlistentry>
    <term>Using a remote &smt; Server</term>
    <listitem>
     <para>
      If you already run an &smt; server within your organization, you can
      use it within &cloud;. When using a remote &smt; server, update
      repositories are served directly from the &smt; server. Each node is
      configured with this repositories upon its initial setup.
      </para>
      <para>
       The &smt; server needs to be accessible from the &admserv; and all
       nodes in &cloud; (via one or more gateways). Resolving the server's
       hostname also needs to work.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Using a &susemgr; Server</term>
    <listitem>
     <para>
      Each client that is managed by &susemgr; needs to register with the
      &susemgr; server. Therefore the &susemgr; support can only be
      installed after the nodes have been deployed. In order to also be able
      to use repositories provided by &susemgr; during node deployment,
      &sls; 11 SP3 must be set up for autoinstallation on the &susemgr;
      server.
     </para>
     <para>
      The server needs to be accessible from the &admserv; and all nodes in
      &cloud; (via one or more gateways). Resolving the server's hostname
      also needs to work.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Installing a &smtool; (&smt;) Server on the &admserv;</term>
    <listitem>
     <para>
      The &smt; server, a free add-on product for &sls;, regularly
      synchronizes repository data from &ncc; with your local host.
      Installing the &smt; server on the &admserv; is recommended if you do
      not have access to update repositories from elsewhere within your
      organization. This option requires the &admserv; to be able to access
      the Internet. &smtool; 11 SP3 is available for free from
      <ulink
      url="http://www.suse.com/solutions/tools/smt.html"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><quote>Sneakernet</quote>
    </term>
    <listitem>
     <para>
      If you choose to completely seal off your admin network from all other
      networks, you need to manually update the repositories from removable
      media. For this purpose copy the repositories from an existing &smt;
      or &suse; Manager server to the removable media.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Utilizing Existing Repositories</term>
    <listitem>
     <para>
      If you can access existing repositories from within your company
      network from the &admserv;, you can either mount or sync these
      repositories from an existing &smt; or &suse; Manager server to the
      required locations on the &admserv;.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.depl.req.storage">
  <title>Persistent Storage</title>

  <para>
   When talking about <quote>persistent storage</quote> on &productname;,
   there are two completely different aspects to discuss: the block and
   object storage services &cloud; offers on the one hand and the hardware
   related storage aspects on the different node types.
  </para>

  <note>
   <title>Persistent vs. Ephemeral Storage</title>
   <para>
    Block and object storage are persistent storage models where files or
    images are stored until they are explicitly deleted. &cloud; also offers
    ephemeral storage for images attached to &vmguest;s. These ephemeral
    images only exist during the life of a &vmguest; and are deleted once
    the guest is terminated. See
    <xref
    linkend="sec.depl.req.storage.hardware.compute"/> for more
    information.
   </para>
  </note>

  <sect2 id="sec.depl.req.storage.services">
   <title>Cloud Storage Services</title>
   <para>
    As mentioned above, &cloud; offers two different types of services for
    persistent storage: object and block storage. Object storage lets you
    upload and download files (similar to a FTP server), whereas a block
    storage provides mountable devices (similar to a hard-disk partition).
    Furthermore &cloud; provides a repository to store the virtual disk
    images used to start &vmguest;s.
   </para>
   <variablelist>
    <varlistentry>
     <term>Object Storage with &o_objstore;</term>
     <listitem>
      <para>
       The &ostack; object storage service is called &o_objstore;. The
       storage component of &o_objstore; (swift-storage) needs to be
       deployed on dedicated nodes where no other cloud services run. In
       order to be able to store the objects redundantly, it is required to
       deploy at least two &o_objstore; nodes. &productname; is configured
       to always use all unused disks on a node for storage.
      </para>
      <para>
       &o_objstore; can optionally be used by &o_img;, the service that
       manages the images used to boot the &vmguest;s. Offering object
       storage with &o_objstore; is optional.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Block Storage</term>
     <listitem>
      <para>
       Block storage on &cloud; is provided by &o_blockstore;.
       &o_blockstore; can use a variety of storage backends, among them
       network storage solutions like NetApp or EMC. It is also possible to
       use local disks for block storage.
      </para>
      <para>
       Alternatively, &o_blockstore; can use &ceph; RBD as a backend. &ceph;
       offers data security and speed by storing the devices redundantly on
       different servers. &ceph; needs to be deployed on dedicated nodes
       where no other cloud services run. In order to be able to store the
       objects redundantly, it is required to deploy at least two &ceph;
       nodes.
      </para>
      &no-ceph-support;
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>The &o_img; Image Repository</term>
     <listitem>
      <para>
       &o_img; provides a catalog and repository for virtual disk images
       used to start the &vmguest;s. &o_img; is installed on a &contrnode;.
       It either uses &o_objstore; or a directory on the &contrnode; to
       store the images. The image directory can either be a local directory
       or an NFS share.
      </para>
     <!-- TODO vuntz 2013-12-04: ceph can also be used as a backend in Cloud 3 (and update coming to Cloud 2.0); also in Cloud 3, I think glance can store images in a cinder volume (?) - need to double-check -->
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 id="sec.depl.req.storage.hardware">
   <title>Storage Hardware Requirements</title>
   <para>
    Apart from sufficient disk space to install the &sls; operating system,
    each node in &cloud; has to store additional data. Requirements and
    recommendations for the various node types are listed below.
   </para>
   <important>
     <!-- TODO vuntz 2013-12-04: hrm, double-check with dirk, but I think it's wrong now? It's using the first disk in PCI order? -->
    <title>Choose a Hard Disk for the Operating System Installation</title>
    <para>
     The operating system will always be installed on the
     <emphasis>first</emphasis> hard disk, the one that is recognized as
     <filename>/dev/sda</filename>. This is the disk that is listed
     <emphasis>first</emphasis> in the BIOS, the one from which the machine
     will boot. If you have nodes with a certain hard disk you want the
     operating system to be installed on, make sure it will be recognized as
     the first disk.
    </para>
   </important>
   <sect3 id="sec.depl.req.storage.hardware.admin">
    <title>&admserv;</title>
    <para>
     If you store the update repositories directly on the &admserv; (see
     <xref
     linkend="sec.depl.req.repos"/> for details), it is
     recommended to mount <filename>/srv</filename> to a separate partition
     or volume with a minimum of &repospace; space.
    </para>
    <para>
     Log files from all nodes in &cloud; are stored on the &admserv; under
     <filename>/var/log</filename> (see <xref
     linkend="sec.deploy.logs.adminserv"/> for a complete list. Furthermore,
     the message service RabbitMQ requires 1 GB of free space in
     <filename>/var</filename>. Make sure sufficient space is available under
     <filename>/var</filename>.
    </para>
   </sect3>
   <sect3 id="sec.depl.req.storage.hardware.control">
    <title>&contrnode;s</title>
    <para>
     Depending on how the services are set up, &o_img; and &o_blockstore; may
     require additional disk space on the &contrnode; on which they are
     running. &o_img; may be configured to use a local directory, whereas
     &o_blockstore; may use a local image file for storage. For performance
     and scalability reasons this is only recommended for test setups. Make
     sure there is sufficient free disk space available if you use a local
     file for storage.
    </para>
    <para>
     &o_blockstore; may be configured to use local disks for storage
     (configuration option <literal>raw</literal>). If you choose this
     setup, it is recommended to deploy the <guimenu>cinder-volume</guimenu>
     role to one or more dedicated &contrnode;s equipped with several disks
     providing sufficient storage space. It may also be necessary to equip
     this node with two or more bonded network cards (requiring a special
     setup for this node, refer to <xref linkend="app.deploy.network_json"/>
     for details), since it will generate heavy network traffic.
    </para>
   </sect3>
   <sect3 id="sec.depl.req.storage.hardware.compute">
    <title>&compnode;s</title>
    <para>
     Unless an &vmguest; is started via <quote>Boot from Volume</quote>, it
     is started with at least one disk&mdash;a copy of the image from which
     it has been started. Depending on the flavor you start, the &vmguest;
     may also have a second, so-called <quote>ephemeral</quote> disk. The
     size of the root disk depends on the image itself, while ephemeral
     disks are always created as sparse image files that grow (up to a
     defined size) when being <quote>filled</quote>. By default ephemeral
     disks have a size of 10 GB.
    </para>
    <para>
     Both disks, root images and ephemeral disk, are directly bound to the
     &vmguest; and are deleted when the &vmguest; is terminated. Therefore
     these disks are bound to the &compnode; on which the &vmguest; has been
     started. The disks are created under <filename>/var/lib/nova</filename>
     on the &compnode;. Your &compnode;s should be equipped with enough disk
     space to store the root images and ephemeral disks.
    </para>
    <note>
     <title>Ephemeral Disks vs. Block Storage</title>
     <para>
      Do not confuse ephemeral disks with persistent block storage. In
      addition to an ephemeral disk, which is automatically provided with
      most &vmguest; flavors, you can optionally add a persistent storage
      device provided by &o_blockstore;. Ephemeral disks are deleted when
      the &vmguest; terminates, while persistent storage devices can be
      reused in another &vmguest;.
     </para>
    </note>
    <para>
     The maximum disk space required on a compute node depends on the
     available flavors. A flavor specifies the number of CPUs, as well as
     RAM and disk size of an &vmguest;. Several flavors ranging from
     <guimenu>tiny</guimenu> (1 CPU, 2512 MB RAM, no ephemeral disk) to
     <guimenu>xlarge</guimenu> (8 CPUs, 8 GB RAM, 10 GB ephemeral disk) are
     available by default. Adding custom flavors as well as editing and
     deleting existing flavors is also supported.
    </para>
    <para>
     To calculate the minimum disk space needed on a compute node, you need
     to determine the highest "disk space to RAM" ratio from your flavors.
     Example:
    </para>
    <simplelist>
     <member>
      Flavor small: 2 GB RAM, 100 GB ephemeral disk => 50 GB disk /1 GB RAM
     </member>
     <member>
      Flavor large: 8 GB RAM, 200 GB ephemeral disk => 25 GB disk /1 GB RAM
     </member>
    </simplelist>
    <para>
     So, 50 GB disk /1 GB RAM is the ratio that matters. If you multiply
     that value by the amount of RAM in GB available on your compute node,
     you have the minimum disk space required by ephemeral disks. Pad that
     value with sufficient space for the root disks plus a buffer that
     enables you to create flavors with a higher disk space to RAM ratio in
     the future.
    </para>
    <warning>
     <title>Overcommitting Disk Space</title>
     <para>
      The scheduler that decides in which node an &vmguest; is started does
      not check for available disk space. If there is no disk space left on
      a compute node, this will not only cause data loss on the &vmguest;s,
      but the compute node itself will also stop operating. Therefore you
      must make sure all compute nodes are equipped with enough hard disk
      space!
     </para>
    </warning>
   </sect3>
   <sect3 id="sec.depl.req.storage.hardware.store">
    <title>Storage Nodes</title>
    <para>
     The block-storage service &ceph; RBD and the object storage service
     &swift; need to be deployed onto dedicated nodes&mdash;it is not
     possible to mix these services. Each storage service requires at least
     two machines (more are recommended) to be able to store data
     redundantly.
    </para>
    <para>
     Each &ceph;/&swift; &stornode; needs at least two hard disks. The first
     one will be used for the operating system installation, while the others
     can be used for storage purposes.  It's recommended to equip the storage
     nodes with as much disks as possible.
    </para>
    <para>
     Using RAID on &o_objstore; storage nodes is not supported. Swift takes
     care of redundancy and replication on its own. Using RAID with
     &o_objstore; would also result in a huge performance penalty.
    </para>
    &no-ceph-support;
   </sect3>
  </sect2>
 </sect1>
 <sect1 id="sec.depl.req.ssl">
  <title>SSL Encryption</title>
  <para>
   Whenever non-public data travels over a network it needs to be encrypted.
   Encryption protects the integrity and confidentiality of data. Therefore
   you should enable SSL support when deploying &cloud; to production (it is
   not enabled by default since it requires certificates to be provided). The
   following services (and their APIs if available) can make use of SSL:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     &o_netw;
    </para>
   </listitem>
   <listitem>
    <para>
     &o_ident;
    </para>
   </listitem>
   <listitem>
    <para>
     &o_img;
    </para>
   </listitem>
   <listitem>
    <para>
     &o_blockstore;
    </para>
   </listitem>
   <listitem>
    <para>
     &o_comp;
    </para>
   </listitem>
   <listitem>
    <para>
     VNC
    </para>
   </listitem>
   <listitem>
    <para>
     &ostack; &dash;
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Using SSL requires an SSL certificate either for each node on which the
   services that uses encryption run (services sharing a certificate) or,
   alternatively, a dedicated certificate for each service. A single
   certificate for the &contrnode; is the minimum requirement, where all
   services listed above are installed on the &contrnode; and are sharing
   the certificate.
  </para>

  <para>
   Certificates must be signed by a trusted authority. Refer to
   <ulink
   url="http://www.suse.com/documentation/sles11/book_sle_admin/data/sec_apache2_ssl.html"/>
   for instructions how to create and sign them.
  </para>

  <important>
   <title>Host Names</title>
   <para>
    Each SSL certificate is issued for a certain host name and, optional,
    for alternative hostnames (via the <literal>AlternativeName</literal>
    option). Each node publicly available node in &cloud; has got two
    hostnames&mdash;an internal and a public one. The SSL certificate needs
    to be issued for both names.
   </para>
   <para>
    The internal name has the following scheme:
   </para>
<screen>d<replaceable>MAC ADDRESS</replaceable>.<replaceable>FQDN</replaceable></screen>
   <para>
    <replaceable>MAC ADDRESS</replaceable> is the MAC address of the
    interface used to PXE boot the machine with lowercase letters and colons
    replaced with dashes, for example <literal>52-54-00-8e-ce-e3</literal>.
    <replaceable>FQDN</replaceable> is the fully qualified domain name. An
    example name looks like this:
   </para>
<screen>d52-54-00-8e-ce-e3.&exampledomain;</screen>
   <para>
    Unless you have entered a custom <guimenu>Public Name</guimenu> for a
    client (see <xref linkend="sec.depl.inst.nodes.install"/> for details),
    the public name is the same as the internal name prefixed by
    <literal>public.</literal>:
   </para>
<screen>public.d52-54-00-8e-ce-e3.&exampledomain;</screen>
   <para>
    To look up the node names open the &crow; Web interface and click on a
    node name in the <guimenu>Node Dashboard</guimenu>. The names are listed
    as <guimenu>Full Name</guimenu> and <guimenu>Public Name</guimenu>.
   </para>
  </important>
 </sect1>
 <sect1 id="sec.depl.req.hardware">
  <title>Hardware Requirements</title>

  <para>
   Precise hardware requirements can only be listed for the &admserv; and
   the &ostack; &contrnode;. The requirements of the &ostack; Compute and
   &stornode;s depends on the number of concurrent &vmguest;s and their
   virtual hardware equipment.
  </para>
  <para>
   The minimum number of machines required for a &cloud; setup is three: one
   &admserv;, one &contrnode; and one &compnode;. In addition to that, a
   gateway providing access to the public network is required. Deploying
   storage requires additional nodes: at least two nodes for &o_objstore; and
   a minimum of four nodes for &ceph; (technology preview only).
  </para>

  <important>
   <title>Physical Machines and Architecture</title>
   <para>
    All &cloud; nodes need to be physical machines. Although the &admserv;
    and the &contrnode; can be virtualized in test environments, this is not
    supported for production systems.
   </para>
   <para>
    &cloud; currently only runs on <literal>x86_64</literal> hardware.
   </para>
  </important>

  <sect2 id="sec.depl.req.hardware.admserv">
   <title>&admserv;</title>
   <itemizedlist>
    <listitem>
     <para>
      Architecture: x86_64
     </para>
    </listitem>
    <listitem>
     <para>
      RAM: at least 2 GB, 4 GB recommended
     </para>
    </listitem>
    <listitem>
     <para>
      Hard disk: at least 50 GB. It is recommended to put
      <filename>/srv</filename> on a separate partition with at least
      additional 30 GB of space, unless you mount the update repositories from
      another server (see <xref linkend="sec.depl.req.repos"/> for details).
     </para>
    </listitem>
    <listitem>
     <para>
      Number of network cards: 1 for single and dual mode, 2 or more for
      team mode. Additional networks such as the bastion network and/or a
      separate BMC network each need an additional network card. See
      <xref
       linkend="sec.depl.req.network"/> for details.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 id="sec.depl.req.hardware.contrnode">
   <title>&contrnode;</title>
   <itemizedlist>
    <listitem>
     <para>
      Architecture: x86_64
     </para>
    </listitem>
    <listitem>
     <para>
      RAM: at least 2 GB, 12 GB recommended (when deploying a single &contrnode;)
     </para>
    </listitem>
    <listitem>
     <para>
      Number of network cards: 1 for single mode, 2 for dual mode, 2 or more
      for team mode. See <xref linkend="sec.depl.req.network"/> for details.
     </para>
    </listitem>
    <listitem>
     <para>
      Hard disk: See
      <xref
       linkend="sec.depl.req.storage.hardware.control"/>.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 id="sec.depl.req.hardware.compnode">
   <title>&compnode;</title>
   <para>
    The &compnode;s need to be equipped with a sufficient amount of RAM and
    CPUs, matching the numbers required by the maximum number of &vmguest;s
    running concurrently. An &vmguest; started in &cloud; cannot share
    resources from several physical nodes, but rather uses the resources of
    the node on which it was started. So if you offer a flavor (see
    <xref
     linkend="gloss.flavor"/> for a definition) with 8 CPUs and 12
    GB RAM, at least one of your nodes should be able to provide these
    resources.
   </para>
   <para>
    See <xref linkend="sec.depl.req.storage.hardware.compute"/> for storage
    requirements.
   </para>
  </sect2>

  <sect2 id="sec.depl.req.hardware.stornode">
   <title>&stornode;</title>
   <para>
    The &stornode;s are sufficiently equipped with a single CPU and 1 or 2
    GB of RAM. See
    <xref
     linkend="sec.depl.req.storage.hardware.store"/> for storage
    requirements.
   </para>
  </sect2>
 </sect1>
 <sect1 id="sec.depl.req.software">
  <title>Software Requirements</title>

  <para>
   The following software requirements need to be met in order to install
   &cloud;:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     &sls; 11 SP3 installation media (ISO image, included in the &cloud;
     &admserv; subscription)
    </para>
   </listitem>
   <listitem>
    <para>
     Access to the &sls; 11 SP3 Update repositories
    </para>
   </listitem>
   <listitem>
    <para>
     &productname; &productnumber; installation media (ISO image)
    </para>
   </listitem>
   <listitem>
    <para>
     Access to the &productname; &productnumber; Update repositories
    </para>
   </listitem>
   <listitem>
    <para>
     A &suse;/&novell; account (for product registration and &smt; setup).
     If you do not already have one, go to
     <ulink
     url="http://www.suse.com/login"/> to create it.
    </para>
   </listitem>
   <listitem>
    <para>
     Optional: &smtool; 11 SP3 installation media. A free download is
     available on <ulink url="http://www.novell.com/linux/smt/"/>. See
     <xref
     linkend="sec.depl.req.repos"/>.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 id="sec.depl.req.summary">
  <title>Summary: Considerations and Requirements</title>

  <para>
   As outlined above, there are some important considerations to be made
   before deploying &cloud;. The following briefly summarizes what was
   discussed in detail in this chapter. Keep in mind that as of
   &productname; &productnumber; it is not possible to change some aspects
   such as the network setup once &cloud; is deployed!
  </para>

  <itemizedlist>
   <title>Network</title>
   <listitem>
    <para>
     If you do not want to stick with the default networks and addresses,
     define custom networks and addresses. You need five different networks.
     If you need to separate the admin and the BMC network, a sixth network is
     required. See <xref linkend="sec.depl.req.network"/> for
     details. Networks that share interfaces need to be configured as VLANs.
    </para>
   </listitem>
   <listitem>
    <para>
     The &cloud; networks are completely isolated, therefore it is not
     required to use public IP addresses for them. A class C network as used
     in this documentation may not provide enough addresses for a cloud that
     is supposed to grow. You may alternatively choose addresses from a class
     B or A network.
    </para>
   </listitem>
   <listitem>
    <para>
     Determine how to allocate addresses from your network. Make sure not to
     allocate IP addresses twice. See
     <xref
     linkend="sec.depl.req.network.allocation"/> for the default
     allocation scheme.
    </para>
   </listitem>
   <listitem>
    <para>
     Define which network mode to use. Keep in mind that all machines within
     the cloud (including the &admserv;) will be set up with the chosen mode
     and therefore need to meet the hardware requirements. See
     <xref
      linkend="sec.depl.req.network.modes"/> for details.
    </para>
   </listitem>
   <listitem>
    <para>
     Define how to access the admin and BMC network(s): no access from the
     outside (no action is required), via an external gateway (gateway needs
     to be provided), or via bastion network. See
     <xref
      linkend="sec.depl.req.network.bastion"/> for details.
    </para>
   </listitem>
   <listitem>
    <para>
     Provide a gateway to access the public network (public, nova-floating).
    </para>
     <!-- TODO vuntz 2013-12-04: check if also for fixed? -->
   </listitem>
   <listitem>
    <para>
     Make sure the admin server's hostname is correctly configured
     (<command>hostname <option>-f</option></command> needs to return a
     fully qualified hostname).
    </para>
   </listitem>
   <listitem>
    <para>
     Prepare a list of MAC addresses and the intended use of the
     corresponding host for all &ostack; nodes.
    </para>
   </listitem>
  </itemizedlist>

  <itemizedlist>
   <title>Update Repositories</title>
   <listitem>
    <para>
     Depending on your network setup you have different options on how to
     provide up-to-date update repositories for &sls; and &cloud; for
     &cloud; deployment: using an existing &smt; or &susemgr; server,
     installing &smt; on the &admserv;, syncing data with an existing
     repository, mounting remote repositories or using a
     <quote>Sneakernet</quote>. Choose the option that best matches your
     needs.
    </para>
   </listitem>
  </itemizedlist>

  <itemizedlist>
   <title>Storage</title>
   <listitem>
    <para>
     Decide whether you want to deploy the object storage service &swift;.
     If so, you need to deploy at least two nodes with sufficient disk space
     exclusively dedicated to &swift;.
    </para>
   </listitem>
   <listitem>
    <para>
     Decide which backend to use with &o_blockstore;. If using the
     <guimenu>raw</guimenu> backend (local disks) it is strongly recommended
     to use a separate node equipped with several hard disks for deploying
     <literal>cinder-volume</literal>. If using &ceph;, you need to deploy
     at least two nodes with sufficient disk space exclusively dedicated to
     it.
    </para>
    &no-ceph-support;
   </listitem>
   <listitem>
    <para>
     Make sure all &compnode;s are equipped with sufficient hard disk space.
    </para>
   </listitem>
  </itemizedlist>

  <itemizedlist>
   <title>SSL Encryption</title>
   <listitem>
    <para>
     Decide whether to use different SSL certificates for the services and
     the API or whether to use a single certificate.
    </para>
   </listitem>
   <listitem>
    <para>
     Get one or more SSL certificates certified by a trusted third party
     source.
    </para>
   </listitem>
  </itemizedlist>

  <itemizedlist>
   <title>Hardware and Software Requirements</title>
   <listitem>
    <para>
     Make sure the hardware requirements for the different node types are
     met.
    </para>
   </listitem>
   <listitem>
    <para>
     Make sure to have all required software at hand.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
<!--
 <sect1 id="sec.depl.arch.security">
  <title>Security Considerations</title>

  <para>FIXME</para>
   * https://github.com/SUSE/cloud/wiki/Common-issues
   * https://github.com/SUSE/cloud/wiki/Nova-compute
   * enabling SSL

    Which communication will be encrypted?

   
     Needs a certificate
      -> point to SLES Apache doc for HowTo
      -> sufficient to put it on Admin node in /etc/apache2/ssl.*/

Passwords:

* SMT DB Password (change via yast)
* MySQL Root Password (MySQL is used for SMT)
* Crowbar Password (change cia yast)
* Chef WebUI Password:
  - initially change via yast (before install-suse-cloud run)
  - in the running system change via
    * Login to the Crowbar Web UI on port 3000:
      * Click on Barclamps in the top right corner of the Web UI.
      * Click on the arrow to the left of the Crowbar barclamp to expand it
      * Click the Edit button to edit the Default proposal.
      * Click on Raw to the right of Attributes.
      * Scroll through the attributes until you see something that looks like:

    "users": {
      "machine-install": {
        "password": "0e7b02a8d2086009c1ff00cc26a827d981306cbd420b1862063e6df8534e0f6a13e45100b9874d0a3fae5962c9ec2de12c0525b8c33685e8ee30406c4eee7133"
      },
      "crowbar": {
        "password": "crowbar"
      }
    },

    * Change the password.
    * Click Apply and confirm you want to apply the changes.
 - nodes root-Password (change in autoyast.xml.erb -> how? hashed, clear text?)

* Keystone
  - "Regular User" Name/Password
  - "Administrator" Name/Password
  Change via Keystone Barclamp

* Glance
  - Service User: Glance User in Keystone
  - Service Password: PW for Service User
  Change via Glance Barclamp

* Swift
  Cluster Admin Password
  Change via Swift Barclamp
    
-->
