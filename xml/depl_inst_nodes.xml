<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet
href="urn:x-daps:xslt:profiling:novdoc-profile.xsl" 
type="text/xml"
title="Profiling step"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
<!ENTITY % NOVDOC.DEACTIVATE.IDREF "IGNORE">
<!ENTITY % entities SYSTEM "entity-decl.ent">
%entities;
]>
<chapter id="cha.depl.inst.nodes">
 <title>Installing the &ostack; Nodes</title>
 <para>
  The &ostack; nodes represent the actual cloud infrastructure. Node
  installation and service deployment is done automatically from the
  &admserv;. Before deploying the &ostack; services, you need to install
  &sls; on every node. In order to do so, each node needs to be PXE booted
  using the <systemitem class="resource">tftp</systemitem> server from the
  &admserv;. Afterwards you can allocate the nodes and trigger the operating
  system installation. There are three different types of nodes:
 </para>
 <simplelist>
  <member><emphasis role="bold">&contrnode;(s):</emphasis> One or more central
  management node(s) interacting with all other nodes.
  </member>
  <member><emphasis role="bold">&compnode;s:</emphasis> The nodes on which the
   &vmguest;s are started.
  </member>
  <member><emphasis role="bold">&stornode;s:</emphasis> Nodes providing object or
   block storage.
  </member>
 </simplelist>
 <sect1 id="sec.depl.inst.nodes.prep">
  <title>Preparations</title>

  <variablelist>
   <varlistentry>
    <term>Meaningful Node names</term>
    <listitem>
     <para>
      Make a note of the MAC address and the purpose of each node (for
      example, controller, storage &ceph;, storage &swift;, compute). This
      will make deploying the &ostack; services a lot easier and less
      error-prone, since it enables you to assign meaningful names (aliases)
      to the nodes, which are otherwise listed with the MAC address by
      default.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>BIOS Boot Settings</term>
    <listitem>
     <para>
      Make sure PXE-booting (booting from the network) is enabled and
      configured as the <emphasis>primary</emphasis> boot-option for each
      node. The nodes will boot twice from the network during the allocation
      and installation phase.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Custom Node Configuration</term>
    <listitem>
     <para>
      All nodes are installed using &ay; with the same configuration located
      at
      <filename>/opt/dell/chef/cookbooks/provisioner/templates/default/autoyast.xml.erb</filename>.
      If this configuration does not match your needs (for example if you
      need special third party drivers) you need to make adjustments to this
      file. An &ay; manual can be found at
      <ulink
      url="http://www.suse.com/documentation/sles11/book_autoyast/data/book_autoyast.html"/>.
      Having changed the &ay; config file, you need to re-upload it to
      &chef;, using the following command:
     </para>
<screen>knife cookbook upload -o /opt/dell/chef/cookbooks/ provisioner</screen>
    </listitem>
   </varlistentry>
   <varlistentry id="var.depl.inst.nodes.prep.root_login">
    <term>Direct &rootuser; Login</term>
    <listitem>
     <para>
      By default, the &rootuser; account on the nodes has no password
      assigned, so a direct &rootuser; login is not possible. Logging in on
      the nodes as &rootuser; is only possible via SSH public keys (for
      example, from the &admserv;).
     </para>
     <para>
      If you want to allow direct &rootuser; login, you can set a password via
      the &crow; Provisioner &barcl; before deploying the nodes. That password
      will be used for the &rootuser; account on all &ostack; nodes. Using
      this method after the nodes are deployed is not possible. In that case
      you would have to log in to each node via ssh from the
      &admserv; and change the password manually with <command>passwd</command>.
     </para>
<!-- fs 2012-09-20: Stupid NovDoc does not allow procedures in
          variablelists, so using an orderedlist instead ;-(( -->
     <orderedlist>
      <title>Setting a &rootuser; Password for the &ostack; Nodes</title>
      <listitem>
       <para>
        Create an md5-hashed &rootuser;-password, for example by using
        <command>openssl passwd <option>-1</option></command>.
       </para>
      </listitem>
      <listitem>
       <para>
        Open a browser and point it to the &crow; Web interface available at
        port <literal>3000</literal> of the &admserv;, for example
        <ulink
        url="http://192.168.124.10:3000/"/>. Log in as user
        <systemitem
        class="username">crowbar</systemitem>. The
        password defaults to <literal>crowbar</literal>, if you have not
        changed it during the installation.
       </para>
      </listitem>
      <listitem>
       <para>
        Open the &barcl; menu by clicking <menuchoice>
        <guimenu>Barclamps</guimenu> <guimenu>Crowbar</guimenu>
        </menuchoice>. Click the <guimenu>Provisioner</guimenu> &barcl;
        entry and <guimenu>Edit</guimenu> the <guimenu>Default</guimenu>
        proposal.
       </para>
      </listitem>
      <listitem>
       <para>
        Click <guimenu>Raw</guimenu> in the <guimenu>Attributes</guimenu>
	section to edit the configuration file.
       </para>
      </listitem>
      <listitem>
       <para>
        Add the following line to the end of the file before the last closing
	curly bracket:
       </para>
<screen>"root_password_hash": "<replaceable>HASHED_PASSWORD</replaceable>"</screen>
       <para>
        replacing "<replaceable>HASHED_PASSWORD</replaceable>" with the
        password you generated in the first step.
       </para>
      </listitem>
      <listitem>
       <para>
	Click <guimenu>Apply</guimenu>.
       </para>
      </listitem>
     </orderedlist>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.depl.inst.nodes.install">
  <title>Node Installation</title>

  <para>
   To install a node, you need to PXE boot it first. It will be booted with
   an image that enables the &admserv; to discover the node and make it
   available for installation. Once you have allocated the node, it will PXE
   boot again and the automatic installation will start.
  </para>

  <procedure>
   <remark condition="clarity">
    2012-08-16 - fs: Screenshots?
   </remark>
   <step>
    <para>
     PXE-boot all nodes you want to deploy. The nodes will boot into the
     <quote>SLEShammer</quote> image, which performs the initial hardware
     discovery.
    </para>
    
    <important>
     <title>Limit the Number of Concurrent PXE boots</title>
     <para>
      PXE Booting a large number nodes a the the same time will cause heavy load
      on the TFPT server, because all nodes will request the boot image at the
      same time. It's recommended to boot the nodes time-delayed.
     </para>
    </important>
    
   </step>
   <step>
    <para>
     Open a browser and point it to the &crow; Web interface available at
     port <literal>3000</literal> of the &admserv;, for example
     <ulink
     url="http://192.168.124.10:3000/"/>. Log in as user
     <systemitem
     class="username">crowbar</systemitem>. The password
     defaults to <literal>crowbar</literal>, if you have not changed it.
    </para>
    <para>
     Click <menuchoice> <guimenu>Nodes</guimenu>
     <guimenu>&dash;</guimenu> </menuchoice> to open the <guimenu>Node
     Dashboard</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Each node that has successfully booted will be listed as being in state
     <literal>Discovered</literal>, indicated by a yellow bullet. The nodes
     will be listed with their MAC address as a name. Wait until all nodes
     are listed as being <literal>Discovered</literal> before proceeding. In
     case a node does not report as being <literal>Discovered</literal>, it
     may need to be rebooted manually.
    </para>
   </step>
   <step>
    <para>
     Although this step is optional, it is recommended to properly group
     your nodes at this stage, since it lets you clearly arrange all
     nodes. Grouping the nodes by role would be one option, for example
     control, compute, object storage (&swift;), and block storage (&ceph;).
    </para>
    <substeps>
     <step>
      <para>
       Enter the name of a new group into the <guimenu>New Group</guimenu>
       input field and click <guimenu>Add Group</guimenu>.
      </para>
     </step>
     <step>
      <para>
       Drag and drop a node onto the title of the newly created group.
       Repeat this step for each node you would like to put into the group.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     To allocate the nodes click on <menuchoice> <guimenu>Nodes</guimenu>
     <guimenu>Bulk Edit</guimenu></menuchoice>. If you prefer to allocate
     the nodes one-by-one, click a node's name followed by a click on
     <guimenu>Edit</guimenu> instead.
    </para>

    <important>
     <title>Limit the Number of Concurrent Node Deployments</title>
     <para>
      Deploying a large number nodes in bulk mode will cause heavy load on the
      &admserv; server. The subsequent concurrent &chef; client runs triggered
      by the nodes will require a lot of RAM on the &admserv;.
     </para>
     <para>
      Therefore it is recommended to limit the number of concurrent
      <quote>Allocations</quote> in bulk mode. The maximum number depends on
      the amount of RAM on the &admserv;&mdash;limiting concurrent deployments
      to five up to ten is recommended.
     </para>
    </important>

   </step>
   <step>
    <para>
     Provide a meaningful <guimenu>Alias</guimenu>, <guimenu>Public
     Name</guimenu> and a <guimenu>Description</guimenu> for each node and
     check the <guimenu>Allocate</guimenu> box. The entries for
     <guimenu>BIOS</guimenu> and <guimenu>RAID</guimenu> are currently not
     used. Normally <guimenu>Target Platform</guimenu> needs to be set to
     <guimenu>suse-11.3</guimenu>. If you plan to support HyperV in your
     cloud, you need to set the <guimenu>Target Platform</guimenu> of the
     &compnode;s that should run Windows to either
     <guimenu>Windows Server</guimenu> or <guimenu>HyperV
     Server</guimenu>. When specifying <guimenu>Windows Server</guimenu> you
     also need to add a valid <guimenu>License Key</guimenu>.
    </para>
    <tip>
     <title>Alias Names</title>
     <para>
      Providing an alias name will change the default node names (MAC
      address) to the name you provided, making it easier to identify the
      node. Furthermore, this alias will also be used as a DNS
      <literal>CNAME</literal> for the node in the admin network. As a
      result, you will be able to access the node via this alias when, for
      example, logging in via SSH.
     </para>
    </tip>
    
    <tip>
     <title>Public Names</title>
     <para>
      A node's <guimenu>Alias Name</guimenu> is resolved by the DNS server
      installed on the &admserv; and therefore only available within the cloud
      network. The Nova &dash; or some APIs (<systemitem
      class="service">keystone-server</systemitem>, <systemitem
      class="service">glance-server</systemitem>, <systemitem
      class="service">cinder-controller</systemitem>, <systemitem
      class="service">quantum-server</systemitem>, <systemitem
      class="service">nova-multi-controller</systemitem>, and <systemitem
      class="service">swift-proxy</systemitem>) can be accessed from outside
      the &cloud; network. In order to be able to access them by name, these
      names need to be resolved by a name server placed outside of the &cloud;
      network. If you have created DNS entries for nodes, specify the name in
      the <guimenu>Public Name</guimenu> field.
     </para>
     <para>
      The <guimenu>Public Name</guimenu> is never used within the &cloud;
      network. However, if you create an SSL certificate for a node that has a
      public name, this name must be added as an
      <literal>AlternativeName</literal> to the certificate (see <xref
      linkend="sec.depl.req.ssl"/> for more information)..
     </para>
    </tip>
    
   </step>
   <step>
    <para>
     Once you have filled in the data for all nodes, click
     <guimenu>Save</guimenu>. The nodes will reboot and commence the
     &ay;-based &sls; installation via a second PXE boot. Click <menuchoice>
     <guimenu>Nodes</guimenu> <guimenu>&dash;</guimenu> </menuchoice> to
     return to the <guimenu>Node Dashboard.</guimenu>
    </para>
   </step>
   <step>
    <para>
     Nodes that are being installed are listed with the status
     <literal>Installing</literal> (yellow/green bullet). Once the
     installation of a node has finished, it is listed as being
     <literal>Ready</literal>, indicated by a green bullet. Wait until all
     nodes are listed as being <literal>Ready</literal> before proceeding.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 id="sec.depl.inst.nodes.post">
  <title>Post-Installation Configuration</title>

  <para>
   The following lists some <emphasis>optional</emphasis> configuration steps
   like configuring node updates, monitoring, access and SSL-enablement. You
   may entirely skip the following steps or perform them any of them at a
   later stage.
  </para>

  
  <sect2 id="sec.depl.inst.nodes.post.updater">
   <title>Configuring Node Updates with the Updater &barcl;</title>
   <para>
    In order to keep the operating system and the &cloud; software itself
    up-to-date on the nodes, you can either deploy the Updater &barcl; or the
    &susemgr; &barcl;. While the latter requires access to a &susemgr;
    server, the Updater &barcl; uses zypper to install updates and patches
    from repositories made available on the &admserv;.
   </para>
   <para>
    The easiest way to provide the required repositories on the &admserv; is
    to set up an &smt; server as described in <xref
    linkend="sec.depl.inst.admserv.os.smt"/> and <xref
    linkend="sec.depl.inst.admserv.post.smt_repos"/>. Alternatives to setting
    up an &smt; server are described in <xref linkend="sec.depl.req.repos"/>.
   </para>
   <para>
    To deploy the Updater &barcl;, proceed as follows. For general
    instructions on how to edit &barcl; proposals refer to <xref
    linkend="sec.depl.ostack.barclamps"/>.  
   </para>
   <procedure>
    <step>
     <para>
      Open a browser and point it to the &crow; Web interface available at
      port <literal>3000</literal> of the &admserv;, for example <ulink
      url="http://192.168.124.10:3000/"/>. Log in as user <systemitem
      class="username">crowbar</systemitem>. The password defaults to
      <literal>crowbar</literal>, if you have not changed it during the
      installation.
     </para>
    </step>
    <step>
     <para>
      Open the &barcl; menu by clicking <menuchoice>
       <guimenu>Barclamps</guimenu> <guimenu>Crowbar</guimenu>
      </menuchoice>. Click the <guimenu>Updater</guimenu> &barcl; entry and
      <guimenu>Create</guimenu> to open the proposal.
     </para>
    </step>
    <step>
     <para>
      Configure the &barcl; by the following attributes. This configuration
      always applies to all nodes on which the &barcl; is deployed. Creating
      individual configurations for certain nodes is not supported. 
     </para>
     <variablelist>
      <varlistentry>
       <term><guimenu>Use zypper</guimenu></term>
       <listitem>
	<para>
	 Define which zypper subcommand to use for
	 updating. <guimenu>patch</guimenu> will install all patches applying
	 to the system from the configured update repositories that are
	 available. <guimenu>update</guimenu> will update packages from all
	 configured repositories (not just the update repositories) that
	 have a higher version number as the installed
	 packages. <guimenu>dist-upgrade</guimenu> replaces each package
	 installed with the version from the repository and deletes packages
	 not available in the repositories.
	</para>
	<para>Using <guimenu>patch</guimenu> is recommended.</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Enable GPG Checks</guimenu></term>
       <listitem>
	<para>
	 If set to true (recommended), checks if packages are correctly signed.
	</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Automatically Agree With Licenses</guimenu></term>
       <listitem>
	<para>
	 If set to true (recommended), zypper automatically accepts third
	 party licenses.
	</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Include Patches that need Reboots (Kernel)</guimenu></term>
       <listitem>
	<para>
	 Installs patches that require a reboot (for example Kernel or glibc
	 updates). It's strongly recommended to set this option to
	 <guimenu>false</guimenu> and to install these updates manually when
	 you have a chance to safely reboot the node<!-- (see FIXME for more
	 information)-->. Installing a new Kernel and not rebooting may result
	 in an unstable system.
	</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Reboot Nodes if Needed</guimenu></term>
       <listitem>
	<para>
	 Automatically reboots the system in case a patch requiring a reboot
	 has been installed. It's strongly recommended to set this option to
	 <guimenu>false</guimenu> and to install updates requiring a reboot
	 manually when you have a chance to safely reboot the node<!-- (see
	 FIXME for more information)-->. Automatically rebooting for example a
	 &compnode; will immediately terminate all &vmguest;s. Unsaved data on
	 these guests will be lost and running processes will be aborted.
	</para>
       </listitem>
      </varlistentry>
     </variablelist>
    </step>
    <step>
     <para>
      Choose the nodes on which the Updater &barcl; should be deployed in the
      <guimenu>Node Deployment</guimenu> section by dragging them to the
      <guimenu>Updater</guimenu> column. It's recommended to deploy it on all
      nodes in the &cloud;.
     </para>
    </step>
   </procedure>
   <para>
    <command>zypper</command> keeps track of the packages and patches it
    installs in <filename>/var/log/zypp/history</filename>. Review that log
    file on a node to find out which updates have been installed. A second log
    file recording debug information on the <command>zypper</command> runs can
    be found at <filename>/var/log/zypper.log</filename> on each node.
   </para>
  </sect2>

  <sect2 id="sec.depl.inst.nodes.post.manager">
   <title>Configuring Node Updates with the &susemgr; &barcl;</title>
   <para>
    In order to keep the operating system and the &cloud; software itself
    up-to-date on the nodes, you can either deploy &susemgr; &barcl; or the
    Updater &barcl;. While the latter uses zypper to install updates and
    patches from repositories made available on the &admserv;.
   </para>
   <para>
    To enable the &susemgr; server to manage the &cloud; nodes, the respective
    &productname; &productnumber; channels (SUSE-Cloud-2.0-Pool,
    SUSE-Cloud-2.0-Updates) need to be available on the server. It also
    requires to generate an <literal>Activation Key</literal> for &cloud;.
   </para>
   <para>
    The &susemgr; &barcl; requires access to the &susemgr;
    server from every node it is deployed to. <!-- See FIXME for more
    information. -->
   </para>
   <para>
    To deploy the &susemgr; &barcl;, proceed as follows. For general
    instructions on how to edit &barcl; proposals refer to <xref
    linkend="sec.depl.ostack.barclamps"/>.  
   </para>
   <procedure>
    <step>
     <para>
      Generate an <literal>Activation Key</literal> for &cloud; on the
      &susemgr; server. See the section <citetitle>Activation Keys</citetitle>
      at <ulink
      url="http://www.suse.com/documentation/suse_manager/book_susemanager_ref/data/s1-sm-systems.html"/>
      for instructions).
     </para>
    </step>
    <step>
     <para>
      Download the package
      <literal>rhn-org-trusted-ssl-cert-<replaceable>VERSION</replaceable>-<replaceable>RELEASE</replaceable>.noarch.rpm</literal>
      from https://<replaceable>susemanager.&exampledomain;</replaceable>/pub/
      (<replaceable>VERSION</replaceable> and
      <replaceable>RELEASE</replaceable> may vary,
      <replaceable>susemanager.&exampledomain;</replaceable> has to be
      replaced by the address of your &susemgr; server. Copy the file you just
      downloaded to
      <filename>/opt/dell/chef/cookbooks/suse-manager-client/files/default/ssl-cert.rpm</filename>
      on the &admserv;. The package contains the &susemgr;'s CA SSL Public
      Certificate. The certificate installation has not been automated on
      purpose, because downloading the certificate manually enables you to
      check it before copying it.
     </para>
    </step>
    <step>
     <para>
      Re-install the &barcl; by running the following command:
     </para>
     <screen>/opt/dell/bin/barclamp_install.rb --rpm suse-manager-client</screen>
    </step>
    <step>
     <para>
      Open a browser and point it to the &crow; Web interface available at
      port <literal>3000</literal> of the &admserv;, for example <ulink
      url="http://192.168.124.10:3000/"/>. Log in as user <systemitem
      class="username">crowbar</systemitem>. The password defaults to
      <literal>crowbar</literal>, if you have not changed it during the
      installation.
     </para>
    </step>
    <step>
     <para>
      Open the &barcl; menu by clicking <menuchoice>
       <guimenu>Barclamps</guimenu> <guimenu>Crowbar</guimenu>
       </menuchoice>. Click the <guimenu>SUSE Manager Client</guimenu> &barcl;
       entry and <guimenu>Create</guimenu> to open the proposal.
     </para>
    </step>
    <step>
     <para>
      Configure the &barcl; by the following attributes. This configuration
      always applies to all nodes on which the &barcl; is deployed. Creating
      individual configurations for certain nodes is not supported. 
     </para>
     <variablelist>
      <varlistentry>
       <term><guimenu>Activation Key</guimenu></term>
       <listitem>
	<para>
	 Enter the &susemgr; activation key for &cloud; here. This key must
	 have been generated on the &susemgr; server.
	</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SUSE Manager Server Hostname</guimenu></term>
       <listitem>
	<para>
	 Fully qualified hostname of the &susemgr; server. This name must be
	 resolvable via the DNS server on the &admserv;.
	</para>
       </listitem>
      </varlistentry>
     </variablelist>
    </step>
    <step>
     <para>
      Choose the nodes on which the &susemgr; &barcl; should be deployed in
      the <guimenu>Node Deployment</guimenu> section by dragging them to the
      <guimenu>SUSE Manager Client</guimenu> column. It's recommended to
      deploy it on all nodes in the &cloud;.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 id="sec.depl.inst.nodes.post.nfs">
   <title>Mounting NFS Shares on a Node</title>
   <para>
    The NFS &barcl; allows you to mount NFS share from a remote host on nodes
    in the cloud. This feature can, for example, be used to provide an image
    repository for &o_img;. Note that all nodes which are to mount an NFS share
    must be able to reach the NFS server. This requires to manually adjust the
    network configuration.
   </para>
   <para>
    To deploy the &susemgr; &barcl;, proceed as follows. For general
    instructions on how to edit &barcl; proposals refer to <xref
    linkend="sec.depl.ostack.barclamps"/>.  
   </para>
   <procedure>
    <step>
     <para>
      Open a browser and point it to the &crow; Web interface available at
      port <literal>3000</literal> of the &admserv;, for example <ulink
      url="http://192.168.124.10:3000/"/>. Log in as user <systemitem
      class="username">crowbar</systemitem>. The password defaults to
      <literal>crowbar</literal>, if you have not changed it during the
      installation.
     </para>
    </step>
    <step>
     <para>
      Open the &barcl; menu by clicking <menuchoice>
       <guimenu>Barclamps</guimenu> <guimenu>Crowbar</guimenu>
      </menuchoice>. Click the <guimenu>NFS Client</guimenu> &barcl; entry and
      <guimenu>Create</guimenu> to open the proposal.
     </para>
    </step>
    <step>
     <para>
      Configure the &barcl; by the following attributes. Each set of
      attributes is used to mount a single NFS share. If you want to mount
      more than one share, click <guimenu>Add</guimenu> after having filled in
      all attributes.
     </para>
     <variablelist>
      <varlistentry>
       <term><guimenu>Name</guimenu></term>
       <listitem>
	<para>
	 Unique name for the current configuration. This name is used in the
	 Web interface only to distinguish between different shares.
	</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>NFS Server</guimenu></term>
       <listitem>
	<para>
	 Fully qualified hostname or IP address of the NFS server.
	</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Export on Server</guimenu></term>
       <listitem>
	<para>
	 Export name for the share on the NFS server.
	</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Mount Options</guimenu></term>
       <listitem>
	<para>
	 Mount options that will be used on the node. See <command>Man 8 mount
	 </command> for general mount options and <command>man 5
	 nfs</command> for a list of NFS-specific options. Note that the
	 general option <option>nofail</option> (do not report errors if
	 device does not exist) is automatically set.
	</para>
       </listitem>
      </varlistentry>
     </variablelist>
    </step>
   </procedure>
   <para>
    The NFS &barcl; is the only &barcl; that lets you create different
    proposals, enabling you to be able to mount different NFS shares on
    different nodes. Once you have created an NFS proposal, a special
    <guimenu>Edit</guimenu> is shown in the &barcl; overview screen
    of the &crow; Web interface. Click it to either <guimenu>Edit</guimenu> an
    existing proposal or <guimenu>Create</guimenu> a new one. Creating a new
    proposal requires to give it a unique name.
   </para>
  </sect2>

<!-- FIXME: Check if this is still possible

  <sect2 id="sec.depl.inst.nodes.post.contrnode">
   <title>Providing a Volume or Separate Partition for the &o_img; Image Repository</title>
   <para>
    If you plan to host the &o_img; Image Repository on a separate volume
    (recommended) or partition, you need to prepare the &contrnode; before
    deploying the &o_img; service.
   </para>
   <para>
    Log in to the &contrnode; as &rootuser; via SSH from the &admserv; (see
    <xref linkend="sec.depl.trouble.faq.ostack"/> for detailed
    instructions). Set up the volume or format the partition and mount it to
    <filename>/var/lib/glance/images</filename> (if you do not use &yast;
    for this tasks, you need to create the directory prior to mounting).
   </para>
  </sect2>

-->

  <sect2 id="sec.depl.inst.nodes.post.access">
   <title>Accessing the Nodes</title>
   <para>
    The nodes can only be accessed via SSH from the &admserv;&mdash;it is
    not possible to connect to them from any other host in the network.
   </para>
   <para>
    The &rootuser; account <emphasis>on the nodes</emphasis> has no password
    assigned, therefore logging in to a node as
    &rootuser;@<replaceable>node</replaceable> is only possible via SSH and
    key authentication. By default, only the key of the &rootuser; of the
    &admserv; (root@<replaceable>admin</replaceable>) is enabled to log in
    via SSH.
   </para>
   <para>
    In case you have added additional users to the &admserv; and want to
    give them permission, to log in to the nodes as well, you need to add
    these user's public SSH keys to &rootuser;'s
    <filename>authorized_keys</filename> file on all nodes. Proceed as
    follows:
   </para>
   <procedure>
    <title>Copying SSH Keys to all Nodes</title>
    <step>
     <para>
      If not already existing, generate an SSH key pair for the user that
      should be able to log in to the nodes with
      <command>ssh-keygen</command>. Alternatively copy an existing public
      key with <command>ssh-copy-id</command>. Refer to the respective man
      pages for more information.
     </para>
    </step>
    <step>
     <para>
      Log in to the &crow; Web interface available at port
      <literal>3000</literal> of the &admserv;, for example
      <ulink
      url="http://192.168.124.10:3000/"/> (username and default
      password: <literal>crowbar</literal>).
     </para>
    </step>
    <step>
     <para>
      Open the &barcl; menu by clicking <menuchoice>
      <guimenu>Barclamps</guimenu> <guimenu>All Barclamps</guimenu>
      </menuchoice>. Click the <guimenu>Provisioner</guimenu> &barcl; entry
      and <guimenu>Edit</guimenu> the <guimenu>Default</guimenu> proposal.
     </para>
    </step>
    <step>
     <para>
      Copy and paste the <emphasis>public</emphasis> SSH key of the user
      into the <guimenu>Additional SSH Keys</guimenu> input field. If adding
      keys for multiple users, note that each key needs to be placed on a
      new line.
     </para>
    </step>
    <step>
     <para>
      Click <guimenu>Apply</guimenu> to deploy the keys and save your
      changes to the proposal.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 id="sec.depl.inst.nodes.post.ssl">
   <title>Enabling SSL (optional)</title>
   <para>
    In order to enable SSL to encrypt communication within the cloud (see
    <xref linkend="sec.depl.req.ssl"/> for details), the respective
    certificates need to be available on the nodes running the encrypted
    services. An SSL certificate is at least required on the &contrnode;.
   </para>
   <para>
    To make them available, copy them to the node. Each certificate consists
    of a pair of files the certificate file
    (e.g. <filename>signing_cert.pem</filename>) and the key file
    (e.g. <filename>signing_key.pem</filename>). If you use your own
    certificate authority (CA) for signing, you will also need a certificate
    file for the CA (e.g. ca.pem). It is recommended to copy the files to the
    <filename>/etc</filename> directory using the directory structure outlined
    below. If you use a dedicated certificate for each service, create
    directories named after the services
    (e.g. <filename>/etc/keystone</filename>). If sharing the certificates,
    use a directory such as <filename>/etc/cloud</filename>.
   </para>
   <variablelist>
    <varlistentry>
     <term>SSL Certificate File</term>
     <listitem>
      <para>
       <filename>/etc/cloud/ssl/certs/signing_cert.pem</filename>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>SSL Key File</term>
     <listitem>
      <para>
       <filename>/etc/cloud/private/signing_key.pem</filename>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>CA Certificates File</term>
     <listitem>
      <para>
       <filename>/etc/cloud/ssl/certs/ca.pem</filename>
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 id="sec.depl.inst.nodes.edit">
  <title>Editing Allocated Nodes</title>

  <para>
   All nodes that have been allocated can be decommissioned or re-installed.
   Click a node's name in the <guimenu>Node Dashboard</guimenu> and then
   click <guimenu>Edit</guimenu>. The following options are available:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Forget</guimenu>
    </term>
    <listitem>
     <para>
      Deletes a node from the pool. If you want to re-use this node again,
      it needs to be reallocated and re-installed from scratch.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Deallocate</guimenu>
    </term>
    <listitem>
     <para>
      Temporarily removes the node from the pool of nodes. Once you
      reallocate the node it will take its former role. Useful for adding
      additional machines in times of high load or for decommissioning
      machines in times of low load.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Reinstall</guimenu>
    </term>
    <listitem>
     <para>
      Triggers a reinstallation. The machine stays allocated.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <warning>
   <title>Editing Nodes in a Production System</title>
   <para>
    When deallocating nodes that provide essential services, the complete
    cloud will become unusable. While it is uncritical to disable single
    storage nodes (provided you have not disabled redundancy) or single
    compute nodes, disabling &contrnode;(s) will cause major problems.  It
    will either <quote>kill</quote> certain services (for example
    &o_objstore;) or, at worst (when deallocating the &contrnode; hosting
    &o_netw;) the the complete cloud. You should also not disable nodes
    providing &ceph; monitoring services or the nodes providing swift ring and
    proxy services.
   </para>
  </warning>
 </sect1>
</chapter>
