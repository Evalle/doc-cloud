<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet
href="urn:x-daps:xslt:profiling:novdoc-profile.xsl" 
type="text/xml"
title="Profiling step"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
<!ENTITY % NOVDOC.DEACTIVATE.IDREF "IGNORE">
<!ENTITY % entities SYSTEM "entity-decl.ent">
%entities;
]>
<chapter id="cha.depl.inst.nodes">
 <title>Installing the &ostack; Nodes</title>
 <para>
  The &ostack; nodes represent the actual cloud infrastructure. Node
  installation and service deployment is done automatically from the
  &admserv;. Before deploying the &ostack; services, you need to install
  &sls; on every node. In order to do so, each node needs to be PXE booted
  using the <systemitem class="resource">tftp</systemitem> server from the
  &admserv;. Afterwards you can allocate the nodes and trigger the operating
  system installation. There are three different types of nodes:
 </para>
 <simplelist>
  <member><emphasis role="bold">&contrnode;:</emphasis> The central management node
   interacting with all other nodes.
  </member>
  <member><emphasis role="bold">&compnode;s:</emphasis> The nodes on which the
   &vmguest;s are started.
  </member>
  <member><emphasis role="bold">&stornode;s:</emphasis> Nodes providing object or
   block storage.
  </member>
 </simplelist>
 <sect1 id="sec.depl.inst.nodes.prep">
  <title>Preparations</title>

  <variablelist>
   <varlistentry>
    <term>Meaningful Node names</term>
    <listitem>
     <para>
      Make a note of the MAC address and the purpose of each node (for
      example, controller, storage &ceph;, storage &swift;, compute). This
      will make deploying the &ostack; services a lot easier and less
      error-prone, since it enables you to assign meaningful names (aliases)
      to the nodes, which are otherwise listed with the MAC address by
      default.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>BIOS Boot Settings</term>
    <listitem>
     <para>
      Make sure PXE-booting (booting from the network) is enabled and
      configured as the <emphasis>primary</emphasis> boot-option for each
      node. The nodes will boot twice from the network during the allocation
      and installation phase.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Custom Node Configuration</term>
    <listitem>
     <para>
      All nodes are installed using &ay; with the same configuration located
      at
      <filename>/opt/dell/chef/cookbooks/provisioner/templates/default/autoyast.xml.erb</filename>.
      If this configuration does not match your needs (for example if you
      need special third party drivers) you need to make adjustments to this
      file. An &ay; manual can be found at
      <ulink
      url="http://www.suse.com/documentation/sles11/book_autoyast/data/book_autoyast.html"/>.
      Having change the &ay; config file, you need to re-upload it to
      &chef;, using the following command:
     </para>
<screen>knife cookbook upload -o /opt/dell/chef/cookbooks/ provisioner</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Direct &rootuser; Login</term>
    <listitem>
     <para>
      By default, the &rootuser; account on the nodes has no password
      assigned, so a direct &rootuser; login is not possible. Logging in on
      the nodes as &rootuser; is only possible via SSH public keys (for
      example, from the &admserv;).
     </para>
     <para>
      If you want to allow direct &rootuser; login, you can set a password via
      the &crow; Provisioner &barcl; before deploying the nodes. That password
      will be used for the &rootuser; account on all &ostack; nodes. Using
      this method after the nodes are deployed is not possible. In that case
      you would have to log in to each node via ssh from the
      &admserv; and change the password manually with <command>passwd</command>.
     </para>
<!-- fs 2012-09-20: Stupid NovDoc does not allow procedures in
          variablelists, so using an orderedlist instead ;-(( -->
     <orderedlist>
      <title>Setting a &rootuser; Password for the &ostack; Nodes</title>
      <listitem>
       <para>
        Create an md5-hashed &rootuser;-password, for example by using
        <command>openssl passwd<option>-1</option></command>.
       </para>
      </listitem>
      <listitem>
       <para>
        Open a browser and point it to the &crow; Web interface available at
        port <literal>3000</literal> of the &admserv;, for example
        <ulink
        url="http://192.168.124.10:3000/"/>. Log in as user
        <systemitem
        class="username">crowbar</systemitem>. The
        password defaults to <literal>crowbar</literal>, if you have not
        changed it during the installation.
       </para>
      </listitem>
      <listitem>
       <para>
        Open the &barcl; menu by clicking <menuchoice>
        <guimenu>Barclamps</guimenu> <guimenu>All Barclamps</guimenu>
        </menuchoice>. Click the <guimenu>Provisioner</guimenu> &barcl;
        entry and <guimenu>Edit</guimenu> the <guimenu>Default</guimenu>
        proposal.
       </para>
      </listitem>
      <listitem>
       <para>
        Click <guimenu>Raw</guimenu> to edit the configuration file.
       </para>
      </listitem>
      <listitem>
       <para>
        Add the following line within the <guimenu>Provisioner</guimenu>
        section of the file:
       </para>
<screen>"root_password_hash": "<replaceable>HASHED_PASSWORD</replaceable>"</screen>
       <para>
        replacing "<replaceable>HASHED_PASSWORD</replaceable>" with the
        password you generated in the first step.
       </para>
      </listitem>
     </orderedlist>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.depl.inst.nodes.install">
  <title>Node Installation</title>

  <para>
   To install a node, you need to PXE boot it first. It will be booted with
   an image that enables the &admserv; to discover the node and make it
   available for installation. Once you have allocated the node, it will PXE
   boot again and the automatic installation will start.
  </para>

  <procedure>
   <remark condition="clarity">
    2012-08-16 - fs: Screenshots?
   </remark>
   <step>
    <para>
     PXE-boot all nodes you want to deploy. Although it is possible to
     allocate nodes one-by-one, doing this in bulk-mode is recommended,
     because it is much faster. The nodes will boot into the
     <quote>SLEShammer</quote> image, which performs the initial hardware
     discovery.
    </para>
   </step>
   <step>
    <para>
     Open a browser and point it to the &crow; Web interface available at
     port <literal>3000</literal> of the &admserv;, for example
     <ulink
     url="http://192.168.124.10:3000/"/>. Log in as user
     <systemitem
     class="username">crowbar</systemitem>. The password
     defaults to <literal>crowbar</literal>, if you have not changed it.
    </para>
    <para>
     Click <menuchoice> <guimenu>Nodes</guimenu>
     <guimenu>&dash;</guimenu> </menuchoice> to open the <guimenu>Node
     Dashboard</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Each node that has successfully booted will be listed as being in state
     <literal>Discovered</literal>, indicated by a yellow bullet. The nodes
     will be listed with their MAC address as a name. Wait until all nodes
     are listed as being <literal>Discovered</literal> before proceeding. In
     case a node does not report as being <literal>Discovered</literal>, it
     may need to be rebooted manually.
    </para>
   </step>
   <step>
    <para>
     Although this step is optional, it is recommended to properly group
     your nodes at this stage, since it lets you to clearly arrange all
     nodes. Grouping the nodes by role would be one option, for example
     control, compute, object storage (&swift;), and block storage (&ceph;)
     .
    </para>
    <substeps>
     <step>
      <para>
       Enter the name of a new group into the <guimenu>New Group</guimenu>
       input field and click <guimenu>Add Group</guimenu>.
      </para>
     </step>
     <step>
      <para>
       Drag and drop a node onto the title of the newly created group.
       Repeat this step for each node you would like to put into the group.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     To allocate the nodes click on <menuchoice> <guimenu>Nodes</guimenu>
     <guimenu>Bulk Edit</guimenu></menuchoice>. If you prefer to allocate
     the nodes one-by-one, click a node's name followed by a click on
     <guimenu>Edit</guimenu> instead.
    </para>
   </step>
   <step>
    <para>
     Provide a meaningful <guimenu>Alias</guimenu>, <guimenu>Public
     Name</guimenu> and a <guimenu>Description</guimenu> for each node and
     check the <guimenu>Allocate</guimenu> box. The entries for
     <guimenu>BIOS</guimenu> and <guimenu>RAID</guimenu> are currently not
     used.
    </para>
    <tip>
     <title>Alias Names</title>
     <para>
      Providing an alias name will change the default node names (MAC
      address) to the name you provided, making it easier to identify the
      node. Furthermore, this alias will also be used as a DNS
      <literal>CNAME</literal> for the node in the admin network. As a
      result, you will be able to access the node via this alias when, for
      example, logging in via SSH.
     </para>
    </tip>
    
    <tip>
     <title>Public Names</title>
     <para>
      A node's <guimenu>Alias Name</guimenu> is resolved by the DNS server
      installed on the &admserv; and therefore only available within the cloud
      network. The Nova &dash; or some APIs (<systemitem
      class="service">keystone-server</systemitem>, <systemitem
      class="service">glance-server</systemitem>, <systemitem
      class="service">cinder-controller</systemitem>, <systemitem
      class="service">quantum-server</systemitem>, <systemitem
      class="service">nova-multi-controller</systemitem>, and <systemitem
      class="service">swift-proxy</systemitem>) can be accessed from outside
      the &cloud; network. In order to be able to access them by name, these
      names need to be resolved by a name server placed outside of the &cloud;
      network. If you have created DNS entries for nodes, specify the name in
      the <guimenu>Public Name</guimenu> field.
     </para>
     <para>
      The <guimenu>Public Name</guimenu> is never used within the &cloud;
      network. However, if you create an SSL certificate for a node that has a
      public name, this name must be added as an
      <literal>AlternativeName</literal> to the certificate (see <xref
      linkend="sec.depl.req.ssl"/> for more information)..
     </para>
    </tip>
    
   </step>
   <step>
    <para>
     Once you have filled in the data for all nodes, click
     <guimenu>Save</guimenu>. The nodes will reboot and commence the
     &ay;-based &sls; installation via a second PXE boot. Click <menuchoice>
     <guimenu>Nodes</guimenu> <guimenu>&dash;</guimenu> </menuchoice> to
     return to the <guimenu>Node Dashboard.</guimenu>
    </para>
   </step>
   <step>
    <para>
     Nodes that are being installed are listed with the status
     <literal>Installing</literal> (yellow/green bullet). Once the
     installation of a node has finished, it is listed as being
     <literal>Ready</literal>, indicated by a green bullet. Wait until all
     nodes are listed as being <literal>Ready</literal> before proceeding.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 id="sec.depl.inst.nodes.post">
  <title>Post-Installation Configuration</title>

  <para>
   The following lists some <emphasis>optional</emphasis> configuration
   steps like configuring node access and SSL-enablement. You may entirely
   skip the following steps or perform the steps necessary for accessing the
   nodes or the SSL enablement at any later stage.
  </para>

  <sect2 id="sec.depl.inst.nodes.post.contrnode">
   <title>Providing a Volume or Separate Partition for the &o_img; Image Repository</title>
   <para>
    If you plan to host the &o_img; Image Repository on a separate volume
    (recommended) or partition, you need to prepare the &contrnode; before
    deploying the &o_img; service.
   </para>
   <para>
    Log in to the &contrnode; as &rootuser; via SSH from the &admserv; (see
    <xref linkend="sec.depl.trouble.faq.ostack"/> for detailed
    instructions). Set up the volume or format the partition and mount it to
    <filename>/var/lib/glance/images</filename> (if you do not use &yast;
    for this tasks, you need to create the directory prior to mounting).
   </para>
  </sect2>

  <sect2 id="sec.depl.inst.nodes.post.access">
   <title>Accessing the Nodes</title>
   <para>
    The nodes can only be accessed via SSH from the &admserv;&mdash;it is
    not possible to connect to them from any other host in the network.
   </para>
   <para>
    The &rootuser; account <emphasis>on the nodes</emphasis> has no password
    assigned, therefore logging in to a node as
    &rootuser;@<replaceable>node</replaceable> is only possible via SSH and
    key authentication. By default, only the key of the &rootuser; of the
    &admserv; (root@<replaceable>admin</replaceable>) is enabled to log in
    via SSH.
   </para>
   <para>
    In case you have added additional users to the &admserv; and want to
    give them permission, to log in to the nodes as well, you need to add
    these user's public SSH keys to &rootuser;'s
    <filename>authorized_keys</filename> file on all nodes. Proceed as
    follows:
   </para>
   <procedure>
    <title>Copying SSH Keys to all Nodes</title>
    <step>
     <para>
      If not already existing, generate an SSH key pair for the user that
      should be able to log in to the nodes with
      <command>ssh-keygen</command>. Alternatively copy an existing public
      key with <command>ssh-copy-id</command>. Refer to the respective man
      pages for more information.
     </para>
    </step>
    <step>
     <para>
      Log in to the &crow; Web interface available at port
      <literal>3000</literal> of the &admserv;, for example
      <ulink
      url="http://192.168.124.10:3000/"/> (username and default
      password: <literal>crowbar</literal>).
     </para>
    </step>
    <step>
     <para>
      Open the &barcl; menu by clicking <menuchoice>
      <guimenu>Barclamps</guimenu> <guimenu>All Barclamps</guimenu>
      </menuchoice>. Click the <guimenu>Provisioner</guimenu> &barcl; entry
      and <guimenu>Edit</guimenu> the <guimenu>Default</guimenu> proposal.
     </para>
    </step>
    <step>
     <para>
      Copy and paste the <emphasis>public</emphasis> SSH key of the user
      into the <guimenu>Additional SSH Keys</guimenu> input field. If adding
      keys for multiple users, note that each key needs to be placed on a
      new line.
     </para>
    </step>
    <step>
     <para>
      Click <guimenu>Apply</guimenu> to deploy the keys and save your
      changes to the proposal.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 id="sec.depl.inst.nodes.post.ssl">
   <title>Enabling SSL (optional)</title>
   <para>
    In order to enable SSL to encrypt communication within the cloud (see
    <xref linkend="sec.depl.req.ssl"/> for details), the respective
    certificates need to be available on the nodes running the encrypted
    services. An SSL certificate is at least required on the &contrnode;.
   </para>
   <para>
    To make them available, copy them to the node. Each certificate consists
    of a pair of files the certificate file
    (e.g. <filename>signing_cert.pem</filename>) and the key file
    (e.g. <filename>signing_key.pem</filename>). If you use your own
    certificate authority (CA) for signing, you will also need a certificate
    file for the CA (e.g. ca.pem). It is recommended to copy the files to the
    <filename>/etc</filename> directory using the directory structure outlined
    below. If you use a dedicated certificate for each service, create
    directories named after the services
    (e.g. <filename>/etc/keystone</filename>). If sharing the certificates,
    use a directory such as <filename>/etc/cloud</filename>.
   </para>
   <variablelist>
    <varlistentry>
     <term>SSL Certificate File</term>
     <listitem>
      <para>
       <filename>/etc/cloud/ssl/certs/signing_cert.pem</filename>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>SSL Key File</term>
     <listitem>
      <para>
       <filename>/etc/cloud/private/signing_key.pem</filename>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>CA Certificates File</term>
     <listitem>
      <para>
       <filename>/etc/cloud/ssl/certs/ca.pem</filename>
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 id="sec.depl.inst.nodes.edit">
  <title>Editing Allocated Nodes</title>

  <para>
   All nodes that have been allocated can be decommissioned or re-installed.
   Click a node's name in the <guimenu>Node Dashboard</guimenu> and then
   click <guimenu>Edit</guimenu>. The following options are available:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Forget</guimenu>
    </term>
    <listitem>
     <para>
      Deletes a node from the pool. If you want to re-use this node again,
      it needs to be reallocated and re-installed from scratch.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Deallocate</guimenu>
    </term>
    <listitem>
     <para>
      Temporarily removes the node from the pool of nodes. Once you
      reallocate the node it will take its former role. Useful for adding
      additional machines in times of high load or for decommissioning
      machines in times of low load.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Reinstall</guimenu>
    </term>
    <listitem>
     <para>
      Triggers a reinstallation. The machine stays allocated.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <warning>
   <title>Editing Nodes in a Production System</title>
   <para>
    When deallocating nodes that provide essential services, the complete
    cloud will become unusable. While it is uncritical to disable single
    storage nodes (provided you have not disabled redundancy) or single
    compute nodes, disabling the &contrnode; will <quote>kill</quote> the
    complete cloud. You should also not disable nodes providing &ceph;
    monitoring services or the nodes providing swift ring and proxy
    services.
   </para>
  </warning>
 </sect1>
</chapter>
