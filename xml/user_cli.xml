<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet
 href="urn:x-daps:xslt:profiling:novdoc-profile.xsl" 
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "IGNORE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.user.cli">
 <title>Using &ostack; Command Line Interfaces</title>
 <abstract>
  &cli-descr-short;
 </abstract>
<!--taroth 2012-07-31: implementation of unified command lines tools is WIP:
  http://wiki.openstack.org/UnifiedCLI, 
  http://wiki.openstack.org/UnifiedCLI/Mapping, 
  http://wiki.openstack.org/UnifiedCLI/HumanInterfaceGuidelines,
  glance client  is part of that unifcation effort-->
 <sect1 id="cha.user.cli.oview">
  <title>&ostack; Commands&mdash;Overview</title>
  &openstack-python-tools;
  <!-- <para>
   The &ostack; project currently aims to unify the multiple command line
   tools. For details, refer to
   <ulink url="http://wiki.openstack.org/UnifiedCLI"/> and
   <ulink url="http://wiki.openstack.org/UnifiedCLI/Mapping"/>. Commands for
   tasks that may also may executed from outside the cloud have recently
   been moved to the
   <literal>openstack-</literal>&nbsp;<replaceable>SERVICENAME</replaceable>
   packages.
   <remark>taroth 2012-08-06: DEVs is the following correct?</remark>
   Therefore, divergent variants of the command line tools are currently
   available: the older ones are contained in the packages
   <literal>openstack-</literal>&nbsp;<replaceable>SERVICENAME</replaceable>,
   the newer ones in the packages
   <literal>python-<replaceable>SERVICENAME</replaceable>client</literal>.
   </para> -->

<!--As those packages conflict, you cannot install both variants of a command
   in parallel.-->
 </sect1>
 <sect1 id="sec.user.cli.rc">
  <title>&ostack; RC File</title>
  &rc-file-usage;
 </sect1>
 <sect1 id="sec.user.cli.img">
  <title>Managing Images</title>

   &images;

   &img-manage;

   &img-ownership;

  <para>
   Images can either be uploaded to &productname; with the
   <command>glance</command> command line tool or with the &productname;
   &dash;. As the &dash; comes with some limitations with regards to image
   upload and modification of properties, it is recommended to use the
   <command>glance</command> command line tools for comprehensive image
   management.
  </para>
  
  <para>For detailed information, refer to <xref linkend="sec.adm.cli.img"
   />.</para>
 </sect1>
 
 <!--taroth 2013-09-03: disabling for now as there was no feedback/input on the network sections 
  in the UI chapters, therefore it does not make sense to include a further piece
  of crystal ball writing here...->
  <sect1 id="sec.user.cli.networks">
  <title>Managing Networks</title>
  <para>&wip;</para>
  </sect1>-->
 <sect1 id="sec.user.cli.inst.launch">
  <title>Launching Instances</title>
  &instances;
   <para>
       When starting an instance, you need to specify the following key
       parameters:
  </para>

  <variablelist>
   <varlistentry>
    <term>Flavor</term>
    <listitem>
     &flavors;
     <para>
      For more details and a list of default flavors available, refer to
      <xref linkend="sec.adm.cli.flavors"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Keypair</term>
    <listitem>&keypairs;
   <para>
      For details, refer to <xref linkend="sec.user.cli.inst.access.keys"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Security Group</term>
    <listitem>&security;
     <para>
      For details, refer to
      <xref linkend="sec.user.cli.inst.access.security"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Network</term>
    <listitem>
     <para>
      Instances can belong to one or multiple networks. By default, each
      instance is given a fixed IP address, belonging to the internal
      network.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   If needed, you can assign a floating (public) IP address to a running
   instance and attach a block storage device (<literal>volume</literal>)
   for persistent storage. For details, refer to
   <xref linkend="sec.user.cli.inst.access.ips"/>.
<!--taroth 2012-10-15: FIXME,
   reactivate link after finishing the chapter...-->
<!--and 
  <xref linkend="sec.user.cli.volumes"/>.-->
  </para>

  <para>
   Before you can launch an instance, you need to look up a few parameters,
   for example, which images, flavors, and security groups are available.
   Proceed as follows:
  </para>

  <procedure>
   <title>Launching an Instance</title>
   <step>
    <para>
     On a shell, source the &ostack; RC file. For details, refer to
     <xref linkend="sec.user.cli.rc"/>.
    </para>
   </step>
   <step>
    <para>
     Look up the available flavors:
    </para>
<screen>nova flavor-list</screen>
    <para>
     Memorize the ID of the flavor that you want to use for your instance.
    </para>
   </step>
   <step>
    <para>
     Look up the available images:
    </para>
<screen>nova image-list</screen>
    <para>
     Memorize the ID of the image that you want to boot your instance from.
    </para>
   </step>
   <step>
    <para>
     Look up the available security groups:
    </para>
<screen>quantum security-group-list</screen>
    <para>
     If you have not created any specific security groups, you can only
     assign the instance to the default security group.
    </para>
   </step>
   <step>
    <para>
     Look up your keypair's name (for SSH access) and memorize it:
    </para>
<screen>nova keypair-list</screen>
   </step>
   <step>
    <para>
     Now you have all the parameters at hand for starting an instance. Do so
     with the following command:
    </para>
<screen>nova boot --flavor&nbsp;<replaceable>FLAVOR_ID</replaceable>&nbsp;--image<replaceable>IMAGE_ID</replaceable> --key_name <replaceable>KEY_NAME</replaceable> \
--security_group <replaceable>NAME_OF_SEC_GROUP</replaceable> <replaceable>NAME_FOR_INSTANCE</replaceable>&nbsp;</screen>
    <para>
     The command returns a list of instance properties, including the
     <literal>status</literal> of the instance. The status
     <literal>BUILD</literal> indicates that the instance has started, but
     is not yet online.
    </para>
   </step>
   <step>
    <para>
     Check if the instance is online:
    </para>
<screen>nova list</screen>
    <para>
     This command lists all instances of the project you belong to,
     including their ID, their name, their status, and their private (and if
     assigned, their public) IP addresses. If your instance's status is
     <literal>ACTIVE</literal>, the instance is online.
    </para>
    <para>
     To refine the search, run <command>nova help list</command> to view the
     available options for the command.
    </para>
   </step>
  </procedure>

  <para>
   If you did not provide a keypair on starting and have not touched
   security groups or rules so far, by default the instance can only be
   accessed from inside the cloud via VNC at this point. Even pinging the
   instance is not possible. To change this, proceed with
   <xref linkend="sec.user.cli.inst.access"/>.
  </para>
 </sect1>
 <sect1 id="sec.user.cli.inst.access">
  <title>Configuring Access to the Instances</title>
  &inst-access-params;
  
  <sect2 id="sec.user.cli.inst.access.security">
   <title>Configuring Security Groups and Rules</title>
   &security;
   <sect3 id="sec.user.cli.inst.access.security.groups">
    <title>Security Groups</title>
    &sec-groups;
    
    <para>
     Security groups can now be managed with the 
     <command>quantum&nbsp;security-group*</command> commands, provided by the
     <systemitem>python-quantumclient</systemitem> package. 
     Alternatively, you can still use the known <command>nova  secgroup_*-rule</command> commands,
     provided by the <systemitem>python-novaclient</systemitem> package.
     </para>
    <variablelist>
     <varlistentry>
      <term>Listing Security Groups</term>
      <listitem>
<!--<screen>nova secgroup-list</screen>-->
<screen>quantum security-group-list --tenant_id&nbsp;<replaceable>;PROJECT_ID</replaceable>&nbsp;</screen>       
       <para>
        Lists all security groups for the specified project, including the
        groups' descriptions.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Creating a Security Group</term>
      <listitem>
<!--<screen>nova secgroup-create&nbsp;<replaceable>SEC_GROUP_NAME</replaceable>&nbsp;<replaceable>GROUP_DESCRIPTION</replaceable>&nbsp;</screen>-->
<screen>quantum security-group-create&nbsp;<replaceable>SEC_GROUP_NAME</replaceable>&nbsp;<replaceable>GROUP_DESCRIPTION</replaceable>&nbsp;</screen>
       <para>
        Creates a new security group with the specified name and
        description.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Deleting a Security Group</term>
      <listitem>
<!--<screen>nova secgroup-delete&nbsp;<replaceable>SEC_GROUP_NAME</replaceable>&nbsp;</screen>-->
 <screen>quantum security-group-delete&nbsp;<replaceable>SEC_GROUP_NAME_OR_ID</replaceable>&nbsp;</screen>
       <para>
        Deletes the specified group.
       </para>
      &note-groups-delete;
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 id="sec.user.cli.inst.access.security.rules">
    <title>Security Group Rules</title>
    &sec-rules;
    
    &sec-rules-params;
    
    &security-default;
   
    <!--taroth 2012-08-27: IIRC, I read this in upstream docs, but according to
     bmwiedemann's tests, he could not access arbitrary ports of VMs in the same
     security group, therefore commenting-->
<!--Unless you change the rules for the default group, this means that
     those instances are only accessible from IP addresses belonging to other members 
     of this group.-->
    <procedure>
     <title>Configuring Security Group Rules</title>
     <para> Security group rules can now be modified with the
      <command>quantum&nbsp;security-group-rule*</command> commands, available from the
       <systemitem class="resource">python-quantumclient</systemitem> package. Alternatively, you
      can still use the known <command>nova &nbsp;secgroup_*-rule</command> commands, provided
      by the <systemitem>python-novaclient</systemitem> package. Proceed as follows: </para>
     <step>
      <para> On a shell, source the &ostack; RC file. For details, refer to <xref
        linkend="sec.user.cli.rc"/>. </para>
     </step>
     <step>
      <para> Look up the existing rules for a security group: </para>
      <!--<screen>nova secgroup-list-rules <replaceable>SEC_GROUP_NAME</replaceable>&nbsp;</screen>-->
      <screen>quantum security-group-rule-list&nbsp;<replaceable>SEC_GROUP_NAME</replaceable>&nbsp;</screen>
      <remark>taroth 2013-09-03: strange, I could not find any option such as tenant_id or
       sec_group_name in the options - check!</remark>
     </step>
     <step>
      <para> To enable SSH access to the instances: </para>
      <substeps>
       <step id="step.sec.rule.add">
        <para> Either from <emphasis>all</emphasis> IP addresses (specified as IP subnet in CIDR
         notation as <literal>0.0.0.0/0</literal>): </para>
        <!--<screen>nova secgroup-add-rule <replaceable>SEC_GROUP_NAME</replaceable> tcp 22 22 0.0.0.0/0</screen>-->
        <screen>quantum&nbsp;security-group-rule-create&nbsp;--direction&nbsp;ingress \
  --protocol&nbsp;tcp&nbsp;--port_range_min&nbsp;22&nbsp;--port_range_max&nbsp;22 \
  <replaceable>SEC_GROUP_NAME_OR_ID</replaceable>&nbsp;</screen>
        <!--taroth 2013-07-30: CAVE: nova had 2 commands: secgroup-add-group-rule and secgroup-add-rule (for access by other tenants and access from
         outside, respectively), in quantum there only seems to be one command for both...-->
       </step>
       <step id="step.sec.group.rule.add">
        <para> Alternatively, you can allow only IP addresses from other security groups
          (<literal>source groups</literal>) to access the specified port: </para>
        <!--<screen>nova secgroup-add-group-rule -/-ip_proto tcp -/-from_port 22 \
-/-to_port 22 <replaceable>SEC_GROUP_NAME</replaceable>&nbsp;<replaceable>SOURCE_GROUP_NAME</replaceable>&nbsp;</screen>-->
        <screen>quantum&nbsp;security-group-rule-create&nbsp;--direction&nbsp;ingress \
  --protocol&nbsp;tcp&nbsp;--port_range_min&nbsp;22&nbsp;--port_range_max&nbsp;22&nbsp; \
  --remote-group-id&nbsp;SOURCE_GROUP_NAME_OR_ID&nbsp;<replaceable>SEC_GROUP_NAME_OR_ID</replaceable>&nbsp;</screen>
       </step>
      </substeps>
     </step>
     <step>
      <para> To enable pinging the instances: </para>
      <substeps>
       <step>
        <para> Either from <emphasis>all</emphasis> IP addresses (specified as IP subnet in CIDR
         notation as <literal>0.0.0.0/0</literal>): </para>
        <!--<screen>nova secgroup-add-rule <replaceable>SEC_GROUP_NAME</replaceable> icmp -1 -1 0.0.0.0/0</screen>-->
        <screen>quantum security-group-rule-create --protocol icmp --direction ingress &nbsp;<replaceable>SEC_GROUP_NAME_OR_ID</replaceable>&nbsp; </screen>
        <remark>taroth 2013-09-03: DEVs - I added the command above according to
         http://docs.openstack.org/trunk/openstack-network/admin/content/enabling_ping_and_ssh.html,
         but I'm wondering if the following might be missing?
         "--port_range_min&nbsp;1&nbsp;--port_range_max&nbsp;1"</remark>
        <para> This command allows access to all codes and all types of ICMP traffic, respectively.
        </para>
       </step>
       <step>
        <para> Alternatively, you can allow only members of other security groups (<literal>source
          groups</literal>) to ping instances: </para>
        <!--<screen>nova secgroup-add-group-rule -/-ip_proto icmp -/-from_port -1 \
-/-to_port -1
<replaceable>SEC_GROUP_NAME</replaceable>&nbsp;<replaceable>SOURCE_GROUP_NAME</replaceable>&nbsp;</screen>-->
        <screen>quantum security-group-rule-create --protocol icmp --direction ingress \
  --remote-group-id&nbsp;SOURCE_GROUP_NAME_OR_ID&nbsp;<replaceable>SEC_GROUP_NAME_OR_ID</replaceable>&nbsp; </screen>
       </step>
      </substeps>
     </step>
     <step>
      <para> To allow access via UDP port (for example, for a DNS server running on a VM): </para>
      <substeps>
       <step>
        <para> Either from <emphasis>all</emphasis> IP addresses (specified as IP subnet in CIDR
         notation as <literal>0.0.0.0/0</literal>): </para>
        <screen>nova secgroup-add-rule <replaceable>SEC_GROUP_NAME</replaceable> udp 53 53 0.0.0.0/0</screen>
        <remark>taroth 2013-09-03: unfortunately, couldn't find out what the respective quantum
         command is, therefore left the nova command for now... </remark>
       </step>
       <step>
        <para> Alternatively, you can allow only IP addresses from other security groups
          (<literal>source groups</literal>) to access the specified port: </para>
        <screen>nova secgroup-add-group-rule --ip_proto udp --from_port 53 \
--to_port 53 <replaceable>SEC_GROUP_NAME</replaceable>&nbsp;<replaceable>SOURCE_GROUP_NAME</replaceable>&nbsp;</screen>
        <remark>taroth 2013-09-03: unfortunately, couldn't find out what the respective quantum
         command is, therefore left the nova command for now... </remark>
       </step>
      </substeps>
     </step>
     <step>
      <para> To delete a security group rule:</para>
      <screen>quantum security-group-rule-delete <replaceable>SEC_GROUP_ID</replaceable></screen>
     </step>
     <!--taroth 2013-09-03: it seems the following is no longer needed, see
      http://docs.openstack.org/trunk/openstack-network/admin/content/securitygroup_workflow.html,
     therefore commenting-->
     <!-- <step>
      <para>
      To delete security group rules, you need to specify the same
      arguments that you used to create the rule. For example:
      </para>
      <para>
      To delete the security rule that you created in
      <xref linkend="step.sec.rule.add"/>:
      </para>
      <screen>nova secgroup-delete-rule <replaceable>SEC_GROUP_NAME</replaceable>&nbsp;tcp 22 22 0.0.0.0/0</screen>
      <para>
      To delete the security rule that you created in
      <xref linkend="step.sec.group.rule.add"/>:
      </para>
      <screen>nova secgroup-delete-group-rule -/-ip_proto tcp -/-from_port 22 \
      -/-to_port 22 <replaceable>SEC_GROUP_NAME</replaceable>&nbsp;<replaceable>SOURCE_GROUP_NAME</replaceable>&nbsp;</screen>
      </step>-->
    </procedure>

<!--taroth 2012-10-12: maybe integrate some of the following information:
   
    Every security group rule is a policy which allows you to specify inbound 
    connections that are allowed to access the instance, by source address, 
    destination port and IP protocol,(TCP, UDP or ICMP). 
    
    Currently, ipv6 and other protocols cannot be managed with the security rules, 
    making them permitted by default. To manage such, you can deploy a firewall 
    in front of your OpenStack cloud to control other types of traffic. The command 
    requires the following arguments for both TCP and UDP rules :
    
              
   [Note]	The CIDR notation
             
   That notation allows you to specify a base IP address and a suffix that designates 
   the number of significant bits in the IP address used to identify the network. 
   For example, by specifying a 88.170.60.32/27, you specify 88.170.60.32 
   as the base IP and 27 as the suffix. Since you use an IPV4 format, there are only 
   5 bits available for the host part (32 minus 27). 
   The 0.0.0.0/0 notation means you allow the entire IPV4 range, meaning allowing all addresses.
              
   For example, in order to allow any IP address to access to a web server running on one of your 
   instance inside the default security group:
   $ nova secgroup-add-rule default tcp 80 80 0.0.0.0/0
              
   In order to allow any IP address to ping an instance inside the default security group 
   (Code 0, Type 8 for the ECHO request.):
    $ nova secgroup-add-rule default icmp 0 8 0.0.0.0/0-->
<!--taroth 2012-08-15: mention when non-default sec groups are useful (info
    by mvidner, for CLI chapter mention how to change security groups for running 
    instances, info by cthiel: 
    sure, this is possible, but AFAIK only through the command-line:
    
    usage: nova add-secgroup <server> <secgroup>
    Add a Security Group to a server.
    
    Positional arguments:
    <server>    Name or ID of server.
    <secgroup>  Name of Security Group.
    
    
    usage: nova remove-secgroup <server> <secgroup>
    
    Remove a Security Group from a server.
    
    Positional arguments:
    <server>    Name or ID of server.
    <secgroup>  Name of Security Group.
   
    
    taroth 2012-10-12: todo - find out if these commands are also part of 
    python-novaclient-->
   </sect3>
  </sect2>

  <sect2 id="sec.user.cli.inst.access.keys">
   <title>Creating or Importing Keys</title>
   &keypairs;
   <para>
    In case an image uses a static &rootuser; password or a static key set
    (neither is recommended), you do not need to provide a keypair on
    starting of the instance.
   </para>
   <procedure>
    <title>Creating or Importing Keys</title>
    <para>
     Use the <command>nova keypair-add</command> command to generate a new
     keypair, or to upload an existing public key.
    </para>
    <step>
     <para>
      To generate a new keypair, execute the following commands:
     </para>
     <screen>nova keypair-add <replaceable>KEY_NAME</replaceable> &gt; <replaceable>MY_KEY</replaceable>.pem
      chmod 600  <replaceable>MY_KEY</replaceable>.pem</screen>
     <para>
      The first command generates a new keypair named
      <replaceable>KEY_NAME</replaceable>, writing the private key to the
      file <filename><replaceable>MY_KEY</replaceable>.pem</filename> and
      registering the public key at the &comp; database. The second command
      changes the permissions of the file
      <filename><replaceable>MY_KEY</replaceable>.pem</filename> so that
      only you can read and write to it.
     </para>
    </step>
    <step>
     <para>
      If you already have generated a keypair, with the public key located
      at <filename>~/.ssh/id_rsa.pub</filename>, you can upload the public
      key with the following command:
     </para>
     <screen>nova keypair-add --pub_key ~/.ssh/id_rsa.pub <replaceable>KEY_NAME</replaceable>&nbsp;</screen>
     <para>
      The command registers the public key at the &comp; database and names
      the keypair <replaceable>KEY_NAME</replaceable>.
     </para>
    </step>
    <step>
     <para>
      Check if the uploaded keypair appears in the list of available
      keypairs:
     </para>
     <screen>nova keypair-list</screen>
    </step>
   </procedure>
  </sect2>
  <!--taroth 2013-09-03: updated until here-->
  <sect2 id="sec.user.cli.inst.access.ips">
   <title>Managing IP Addresses</title>
   &ip-addr;
   
   &ip-allocate;
  
    &ip-assign;
    
   <!-- Pools of floating IP addresses are created outside of python-novaclient with the 
    nova-manage floating * commands. Refer to "Configuring Public (Floating) IP Addresses" 
    in the Openstack &comp; Administration Manual for more information.-->
   <para>
    Floating IP addresses can be managed with the <command>nova
    *floating-ip-*</command> commands, provided by the
    <systemitem>python-novaclient</systemitem> package.
   </para>
   <variablelist>
    <varlistentry>
     <term>Listing Pools with Floating IP Addresses </term>
     <listitem>
<screen>nova floating-ip-pool-list</screen>
      <para>
       Lists the name of all pools that provide floating IP addresses.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Allocating a Floating IP Address to the Current Project</term>
     <listitem>
<screen>nova floating-ip-pool-list</screen>
      <para>
       The output of the command shows the freshly allocated IP address. If
       there is more than one pool of IP addresses available, you can also
       specify the pool from which to allocate the IP address (optional):
      </para>
<screen>floating-ip-create&nbsp;<replaceable>POOL_NAME</replaceable>&nbsp;</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Listing Floating IP Addresses Allocated to the Current
     Project</term>
     <listitem>
<screen>nova floating-ip-list</screen>
      <para>
       Lists all floating IP addresses that have been allocated to the
       current project. If an IP is already associated with an instance, the
       output also shows the instance's IP, the instance's fixed IP address
       and the name of the pool that provides the floating IP address.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Releasing a Floating IP Address from the Current Project</term>
     <listitem>
<screen>nova floating-ip-delete&nbsp;<replaceable>FLOATING_IP</replaceable>&nbsp;</screen>
      <para>
       The IP address is put back into the pool of IP addresses that are
       available for all projects. If an IP address is currently assigned to
       a running instance, it will automatically be disassociated from the
       instance.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Assigning a Floating IP Address to an Instance</term>
     <listitem>
<screen>nova add-floating-ip&nbsp;<replaceable>INSTANCE_NAME_OR_ID</replaceable>&nbsp;<replaceable>FLOATING_IP</replaceable>&nbsp;</screen>
      <para>
       To associate an IP address with an instance, one or multiple floating
       IP addresses must have been allocated to the current project. Check
       this with <command>nova floating-ip-list</command>. In addition, you
       need to know the instance's name (or ID). To look up the instances
       that belong to the current project, use the <command>nova
       list</command> command.
      </para>
      <para>
       After assigning the IP with <command>nova add-floating-ip</command>,
       the instance is now publicly available under the respective floating
       IP address (provided you have also configured the security group
       rules for the instance accordingly). For details, refer to
       <xref linkend="sec.user.dash.inst.access.security"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Removing a Floating IP Address from an Instance</term>
     <listitem>
<screen>nova remove-floating-ip&nbsp;<replaceable>INSTANCE_NAME_OR_ID</replaceable>&nbsp;<replaceable>FLOATING_IP</replaceable>&nbsp;</screen>
      <para>
       To remove a floating IP address from an instance, you need to specify
       the same arguments that you used to assign the IP.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
<!--taroth 2012-10-15: not sure if it makes sense to describe this
 in procedures, therefore commenting-->
<!--<procedure>
  <title>Allocating Floating (Public) IPs to a Project</title>
  <step>
   <para>
      On a shell, source the &ostack; RC file. For details, refer to
      <xref linkend="sec.user.cli.rc"/>.
     </para>
  </step>
  <step>
   <para>To check which pools of floating IP addresses are available:</para>
   <screen>nova floating-ip-pool-list</screen>
  </step>
  <step>
   <para>To allocate a floating IP address to the current project:</para>
   <screen>nova floating-ip-create</screen>
   <para>The output of the command shows the freshly allocated IP address. 
     If there is more than one pool of IP addresses available, you can
     also specify a pool from which to allocate the IP address (optional): </para>
   <screen>floating-ip-create&nbsp;<replaceable>POOL_NAME</replaceable>&nbsp;</screen>
  </step>
  <step>
   <para>To list all floating IP addresses which have been allocated
     to the current project:</para>
   <screen>nova floating-ip-list</screen>
  </step>
  <step>
   <para>
      To release a floating IP address from the current project:
     </para>
   <screen>nova floating-ip-delete&nbsp;<replaceable>FLOATING_IP</replaceable>&nbsp;</screen>
   <para>The IP address is put back into the pool of IP addresses that 
     are available for all projects. If an IP address is currently assigned 
     to a running instance, it will automatically be dissociated from the 
     instance.</para>
  </step>
 </procedure>
 <procedure>
  <title>Assigning Floating (Public) IP Addresses to Instances</title>
    &ip-assign;
    <step>
     <para>
      On a shell, source the &ostack; RC file. For details, refer to
      <xref linkend="sec.user.cli.rc"/>.
     </para>
  </step>
  <step>
   <para>List all floating IP addresses which have been allocated to the current
     project:</para>
   <screen>nova floating-ip-list</screen>
  </step>
  <step>
   <para>To associate an IP address to an instance, you need to know the
     instance's name (or ID) and the floating IP address.</para>
   <substeps>
    <step>
     <para>To look up the instances that belong to the current project
       with:</para>
     <screen>nova list</screen>
    </step>
    <step>
     <para>To assign the IP address to an instance:</para>
     <screen>nova add-floating-ip&nbsp;<replaceable>INSTANCE_NAME_OR_ID</replaceable>&nbsp;<replaceable>FLOATING_IP</replaceable>&nbsp;</screen>
     <para>The instance is now publicly available under the respective floating 
      IPs address (provided you have also configured the security group rules 
      for the instance accordingly). For details, refer to 
      <xref linkend="sec.user.dash.inst.access.security"/>.
     </para>
    </step>
   </substeps>
  </step>
  <step>
   <para>To remove the floating IP address from an instance:</para>
   <screen>nova remove-floating-ip&nbsp;<replaceable>INSTANCE_NAME_OR_ID</replaceable>&nbsp;<replaceable>FLOATING_IP</replaceable>&nbsp;</screen>
  </step>
 </procedure>-->
  </sect2>
 </sect1>
<!--<sect1 id="sec.user.cli.inst.manage">
  <title>Managing Instances</title>
  <para>The following are typical tasks for managing instances:</para>
  <itemizedlist>
   <listitem>
    <para>Accessing instances from remote</para>
   </listitem>
   <listitem>
    <para>Viewing logs</para>
   </listitem>
   <listitem>
    <para>Creating instance snapshots to preserve a certain disk state of an
     instance</para>
   </listitem>
   <listitem>
    <para>Using instance snapshots as base for new images</para>
   </listitem>
   <listitem>
    <para>Rebooting or terminating instances</para>
   </listitem>
   <listitem>
    <para>Pausing or suspending instances</para>
   </listitem>
   <listitem>
    <para>Tracking instance usage</para>
   </listitem>
  </itemizedlist>
-->
<!--taroth 2012-10-05: nova console-log myinstance-->
<!--  <sect2 id="sec.user.cli.inst.manage.logs">
   <title>Viewing Instance Logs</title>
   <procedure>
    <step>
     <para>Log in to &cloud; &dash;.</para>
    </step>
    <step>
     <para>If you are a member of multiple projects, select a <guimenu>Project</guimenu>
    from the drop-down list at the top of the tab.</para>
    </step>
    <step>
     <para>Click the <guimenu>Instances &amp; Volumes</guimenu> category.</para>
    </step>
    <step>
     <para>Select the instance and from the <guimenu>Actions</guimenu>
      drop-down list, select <guimenu>View Log</guimenu>.</para>
     <para>Alternatively, click the instance's name and switch to the
      <guimenu>Log</guimenu> tab that opens.</para>
     <para>The &dash; shows the output of the instance's serial console. To make
      use of this feature, the respective image must have set the serial console
      correctly in &grub;.</para>
    </step>
   </procedure>
  </sect2>
-->
<!--<sect2 id="sec.user.cli.inst.manage.remote">
   <title>Accessing Instances from Remote</title>
   <para>The &dash;'s built-in VNC client allows you to access instances
   at any time.</para>
  -->
<!--When you need to get a VNC console directly to a server, you can use the 
   nova get-vnc-console command to connect.-->
<!--<procedure>
    <title>Accessing an Instance via VNC</title>
    <step>
     <para>Log in to &cloud; &dash;.</para>
    </step>
    <step>
     <para>If you are a member of multiple projects, select a <guimenu>Project</guimenu>
    from the drop-down list at the top of the tab.</para>
    </step>
    <step>
     <para>Click the <guimenu>Instances &amp; Volumes</guimenu> category.</para>
    </step>
    <step>
     <para>Select the instance to access and from the <guimenu>Actions</guimenu>
      drop-down list, select <guimenu>VNC Console</guimenu>.</para>
     <para>Alternatively, click the instance's name and switch to the
      <guimenu>VNC</guimenu> tab that opens.</para>
    </step>
    <step>
     <para>To display a larger VNC screen, use the link <guimenu>Click here
    to show only VNC</guimenu>.</para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="cloud_dash_vnc.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="cloud_dash_vnc.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>To leave the large VNC screen, use the back button of the browser.</para>
    </step>
   </procedure>

    &ssh-req;-->
<!--For details, refer to <xref linkend="sec.user.dash.inst.access.security"/>.-->
<!--For details, refer to <xref linkend="sec.user.dash.inst.access.ips"/>.-->
<!-- </sect2>-->
<!--<sect2 id="sec.user.cli.inst.manage.snapshots">
   <title>Using Instance Snapshots</title>
   &inst-snap;
     <remark>taroth 2012-08-21: DEVs, if I got it right, ephemeral disks are 
      generally excluded from the snapshots?</remark>
   <procedure>
    <title>Creating Instance Snapshots</title>
    <step>
     <para>Log in to &cloud; &dash;.</para>
    </step>
    <step>
     <para>If you are a member of multiple projects, select a <guimenu>Project</guimenu>
    from the drop-down list at the top of the tab.</para>
    </step>
    <step>
     <para>Click the <guimenu>Instances &amp; Volumes</guimenu> category.</para>
    </step>
    <step>
     <para>Select the instance of which to create a snapshot. From the
        <guimenu>Actions</guimenu> drop-down list, select
        <guimenu>Snapshot</guimenu>.</para>
    </step>
    <step>
     <para>In the window that opens, enter a name for the snapshot and confirm your 
        changes. The &dash; shows the new <guimenu>Instance Snapshot</guimenu> 
        in the <guimenu>Images &amp; Snapshot</guimenu> category.</para>
    </step>
    <step>
     <para>To launch a new instance from the snapshot, select the snapshot and
      click <guimenu>Launch</guimenu>. Proceed with launching an instance as 
      described in <xref linkend="pro.user.instances.launch"/>.</para>
    </step>
   </procedure>
   <procedure>
    <title>Basing an Image on a Snapshot</title>
    <step>
     <para>Log in to &cloud; &dash;.</para>
    </step>
    <step>
     <para>If you are a member of multiple projects, select a <guimenu>Project</guimenu>
    from the drop-down list at the top of the tab.</para>
    </step>
    <step>
     <para>Click the <guimenu>Images &amp; Snapshots</guimenu> category.</para>
    </step>
    <step>
     <para>Select the snapshot and from the <guimenu>Actions</guimenu> drop-down 
        list, select <guimenu>Edit</guimenu>.
       </para>
    </step>
    <step>
     <para>In the window that opens, enter the image properties.
        For more information, refer to <xref linkend="sec.adm.dash.img"/>.</para>
    </step>
    <step>
     <para>Click <guimenu>Update Image</guimenu>. </para>
     <para>&dash; shows the newly created image in the list of images in the 
        <guimenu>Images &amp; Snapshot</guimenu> category. If you delete the 
        snapshot, upon which the image is based, the image will be deleted as 
        well. If you delete the image, the snapshot upon which it is based, will 
        be deleted as well.</para>
    </step>
   </procedure>
  </sect2>
-->
<!--<sect2 id="sec.user.cli.inst.manage.terminate">
   <title>Pausing, Suspending, Rebooting or Terminating Instances</title>
   &inst-terminate;
   
   &inst-terminate-warn;
  
   <procedure>
    <step>
     <para>Log in to &cloud; &dash;.</para>
    </step>
    <step>
     <para>If you are a member of multiple projects, select a <guimenu>Project</guimenu>
    from the drop-down list at the top of the tab.</para>
    </step>
    <step>
     <para>Click the <guimenu>Instances &amp; Volumes</guimenu> category.</para>
    </step>
    <step>
     <para>Select the instance which to put out of the running state. From the
      <guimenu>Actions</guimenu> drop-down list, select the respective action.</para>
    </step>
   </procedure>-->
<!--  Stop and Start an Instance

There are two methods for stopping and starting an instance:

    nova pause / nova unpause

    nova suspend / nova resume

 Pause and Unpause

nova pause stores the state of the VM in RAM. A paused instance continues to run, 
albeit in a "frozen" state.
 Suspend and Resume

nova suspend initiates a hypervisor-level suspend operation. Suspending an instance 
stores the state of the VM on disk; all memory is written to disk and the virtual machine 
is stopped. Suspending an instance is thus similar to placing a device in hibernation, 
and makes memory and vCPUs available. Administrators may want to suspend an 
instance for system maintenance, or if the instance is not frequently used.-->
<!--When you no longer need an instance, use the nova delete command to terminate it. 
  You can use the instance name or the ID string. You will not receive a notification 
  indicating that the instance has been deleted, but if you run the nova list command, 
  the instance will no longer appear in the list.-->
<!--</sect2>
  <sect2 id="sec.user.cli.inst.manage.usage">
   <title>Tracking Usage</title>
   <para>Use the &dash;'s <guimenu>Overview</guimenu> category to track usage of
   instances per project. It allows you to track costs per month by showing metrics like
   number of VCPUs, disks, RAM and uptime of all your instances.</para>
   <para>If you are a member of multiple projects, select a <guimenu>Project</guimenu>
    from the drop-down list at the top of the tab. Select a month and click 
    <guimenu>Submit</guimenu> to query the instance usage for that month. The
    &dash; also allows to download a CVS summary.</para>
   <figure>
    <title>&cloud; &dash;&mdash;Usage Overview</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="cloud_dash_overview_usage.png" width="100%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="cloud_dash_overview_usage.png" width="80%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>
 </sect1>-->
<!--<sect1 id="sec.user.cli.volumes">
  <title>Managing Volumes</title>
  &volumes;-->
<!--you can use the nova CLI to manage volumes.

volume-attach       Attach a volume to a server.
volume-create       Add a new volume.
volume-delete       Remove a volume.
volume-detach       Detach a volume from a server.
volume-list         List all the volumes.
volume-show         Show details about a volume.
volume-snapshot-create
Add a new snapshot.
volume-snapshot-delete
Remove a snapshot.
volume-snapshot-list
List all the snapshots.
volume-snapshot-show
Show details about a snapshot.
volume-type-create  Create a new volume type.
volume-type-delete  Delete a specific flavor
volume-type-list    Print a list of available 'volume types'.-->
<!--<procedure>
   <title>Creating or Deleting Volumes</title>
   <step>
    <para>Log in to &cloud; &dash;.</para>
   </step>
   <step>
    <para>If you are a member of multiple projects, select a <guimenu>Project</guimenu>
    from the drop-down list at the top of the tab.</para>
   </step>
   <step>
    <para>Click the <guimenu>Instances &amp; Volumes</guimenu> category.</para>
   </step>
   <step>
    <para>To create a volume:</para>
    <substeps>
     <step>
      <para>Click <guimenu>Create Volume</guimenu>.</para>
     </step>
     <step>
      <para>In the window that opens, enter a name to assign to a volume,
      a description (optional), and define the size in GB.</para>
     </step>
     <step>
      <para>Confirm your changes.</para>
      <para>The &dash; shows the volume in the <guimenu>Instances &amp;
       Volumes</guimenu> category.</para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     To delete one or multiple volumes:
    </para>
    <substeps>
     <step>
      <para>
       Activate the check boxes in front of the volumes that you want to
       delete.
      </para>
     </step>
     <step>
      <para>
       Click <guimenu>Delete Volumes</guimenu> and confirm your choice in
       the pop-up that appears.<remark>taroth 2012-08-23: check what happens if
       the volume is attached to an instance! taroth 2012-08-27: according to
       bmwiedemann, deleting attached volumes should be refused</remark>
      </para>
      <para>
       A message on the Web page shows if the action has been successful.
      </para>
     </step>
    </substeps>
   </step>
  </procedure>

  <para>&vol-attach;
  View the <guimenu>Status</guimenu> of a volume in the <guimenu>Instances &amp;
  Volumes</guimenu> category of the &dash;: The volume is either  
  <literal>available</literal> or already <literal>In-Use</literal>.</para>
  <procedure>
   <title>Attaching Volumes to Instances</title>
   <step>
    <para>Log in to &cloud; &dash;.</para>
   </step>
   <step>
    <para>If you are a member of multiple projects, select a <guimenu>Project</guimenu>
    from the drop-down list at the top of the tab.</para>
   </step>
   <step>
    <para>Click the <guimenu>Instances &amp; Volumes</guimenu> category.</para>
   </step>
   <step>
    <para>Select the volume to add to an instance and click 
   <guimenu>Edit Attachments</guimenu>.</para>
   </step>
   <step>
    <para>In the window that opens, select an instance to attach the volume
   to.</para>
   </step>
   <step>
    <para>Enter a <guimenu>Device Name</guimenu> under which the volume should
    be accessible on the virtual machine.</para>
   </step>
   <step>
    <para>Confirm your changes. The &dash; shows the instance to which the volume 
    has been attached and the volume's device name.</para>
   </step>
   <step>
    <para>Now you can log in to the instance, mount the disk, format it and use 
   it.</para>
    <para>If the instance is running the latest &sls; SP2 Kernel (or later), it is not necessary
    to reboot the virtual machine to make the device appear. 
    Otherwise load the <literal>acpiphp</literal> module manually:</para>
    <screen>modprobe acpiphp</screen>
   </step>
   <step>
    <para>To detach a volume from an instance:</para>
    <substeps>
     <step>
      <para>Select the volume and click <guimenu>Edit Attachments</guimenu>.</para>
     </step>
     <step>
      <para>In the window that opens, click <guimenu>Detach Volume</guimenu> and
   confirm your changes.</para>
      <para>
          A message on the Web page shows if the action has been successful.
      </para>
     </step>
    </substeps>
   </step>
  </procedure>

  <procedure>
   <title>Creating Volume Snapshots</title>
   -->
<!--taroth 2012-08-24: todo - add what volume snapshots are good for
    and how to delete them-->
<!--
   <step>
    <para>Log in to &cloud; &dash;.</para>
   </step>
   <step>
    <para>If you are a member of multiple projects, select a <guimenu>Project</guimenu>
    from the drop-down list at the top of the tab.</para>
   </step>
   <step>
    <para>Click the <guimenu>Instances &amp; Volumes</guimenu> category.</para>
   </step>
   <step>
    <para>Select the volume of which to create a snapshot.</para>
   </step>
   <step>
    <para>From the <guimenu>Actions</guimenu> drop-down list, select 
    <guimenu>Create Snapshot</guimenu>.</para>
   </step>
   <step>
    <para>In the window that opens, enter a <guimenu>Snapshot Name</guimenu>
   and a <guimenu>Description</guimenu>.
    </para>
   </step>
   <step>
    <para>Confirm your changes. The &dash; shows the new <guimenu>Volume Snapshot</guimenu> 
        in the <guimenu>Images &amp; Snapshot</guimenu> category.</para>
   </step>
  </procedure>

 </sect1>-->
</chapter>
