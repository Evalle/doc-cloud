<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet
 href="urn:x-daps:xslt:profiling:novdoc-profile.xsl" 
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "IGNORE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.user.cli">
 <title>Using &ostack; Command Line Interfaces</title>
 <abstract>
  &cli-descr-short;
 </abstract>
<!--taroth 2012-07-31: implementation of unified command lines tools is WIP:
  http://wiki.openstack.org/UnifiedCLI, 
  http://wiki.openstack.org/UnifiedCLI/Mapping, 
  http://wiki.openstack.org/UnifiedCLI/HumanInterfaceGuidelines,
  glance client  is part of that unifcation effort-->
 <sect1 id="cha.user.cli.oview">
  <title>&ostack; Commands&mdash;Overview</title>

  <para>
   The following command line tools are available for the respective
   services' APIs:
   <remark>taroth 2012-10-02: todo - list taken from CLI chapter of admin guide 
   - check if also correct in user guide context</remark>
  </para>

  <variablelist>
   <varlistentry>
    <term>
     <command>keystone</command>
    </term>
    <listitem>
     <para>
      For managing users and projects. Provided by the
      <systemitem class="resource">python-keystoneclient</systemitem>
      package.
     </para>
<!--taroth 2012-08-03:check: or <systemitem
     class="resource">openstack-keystone</systemitem> package???-->
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <command>nova</command>
    </term>
    <listitem>
     <para>
      For managing instances and flavors. Provided by the
      <systemitem class="resource">python-novaclient</systemitem> package.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <command>glance</command>
    </term>
    <listitem>
     <para>
      For managing images. Provided by the
      <systemitem class="resource">python-glanceclient</systemitem> package.
     </para>
    </listitem>
   </varlistentry>
<!--<varlistentry>
    <term>
     <command>swift</command>
    </term>
    <listitem>
     <para>For managing the object store. Provided by the 
     <systemitem class="resource">openstack-swift</systemitem> package.</para>
    </listitem>
   </varlistentry>-->
  </variablelist>
  <para>All of them have tab completion.</para>
  <para>
   The &ostack; project currently aims to unify the multiple command line
   tools. For details, refer to
   <ulink url="http://wiki.openstack.org/UnifiedCLI"/> and
   <ulink url="http://wiki.openstack.org/UnifiedCLI/Mapping"/>. Commands for
   tasks that may also may executed from outside the cloud have recently
   been moved to the
   <literal>openstack-</literal>&nbsp;<replaceable>SERVICENAME</replaceable>
   packages.
   <remark>taroth 2012-08-06: DEVs is the following correct?</remark>
   Therefore, divergent variants of the command line tools are currently
   available: the older ones are contained in the packages
   <literal>openstack-</literal>&nbsp;<replaceable>SERVICENAME</replaceable>,
   the newer ones in the packages
   <literal>python_<replaceable>SERVICENAME</replaceable>client</literal>.
   </para>
   <!--As those packages conflict, you cannot install both variants of a command
   in parallel.-->

  <para>
   Help and detailed information about the individual commands, their
   subcommands and arguments are available with
  </para>

  <screen>
   <replaceable>COMMAND</replaceable> help</screen>

  <para>
   or
  </para>

  <screen>
   <replaceable>COMMAND</replaceable> help <replaceable>SUBCOMMAND</replaceable>
  </screen>

  <para>
   For example: <command>glance&nbsp;help</command> or
   <command>glance&nbsp;help&nbsp;image-create</command>
  </para>
 </sect1>
 <sect1 id="sec.user.cli.rc">
  <title>&ostack; RC File</title>
  &rc-file-usage;
 </sect1>

 <sect1 id="sec.user.cli.img">
  <title>Managing Images</title>
   <!--taroth 2012-08-27: on behalf of cthiel, filed bnc#777504 (images can be
   managed by users/admins)-->
   &img-manage;
   </sect1>

 <sect1 id="sec.user.cli.inst.launch">
  <title>Launching Instances</title>
  &instances;
  
  <para>Upon start of an instance, you need to specify the following 
   key parameters:</para>
  <variablelist>
   <varlistentry>
    <term>Flavor</term>
    <listitem>
     &flavors;
     <para>For more details and a list of default flavors available, refer to 
    <xref linkend="sec.adm.cli.flavors"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Keypair</term>
    <listitem>&keypairs;
   <para>For details, refer to <xref linkend="sec.user.cli.inst.access.keys"/>.
   </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Security Group</term>
    <listitem>&security;
     <para>For details, refer to <xref linkend="sec.user.cli.inst.access.security"/>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <para>If needed, you can assign a floating (public) IP address to a running 
  instance and attach a block storage device (<literal>volume</literal>)
  for persistent storage. For details, refer to 
  <xref linkend="sec.user.cli.inst.access.ips"/>.<!--taroth 2012-10-15: FIXME,
   reactivate link after finishing the chapter...-->
   <!--and 
  <xref linkend="sec.user.cli.volumes"/>.--></para>

  <para>Before you can launch an instance, you need to look up a few parameters,
   for example, which images, flavors, and security groups are available. Proceed 
   as follows:</para>
  <procedure>
   <title>Launching an Instance</title>
   <step>
    <para>
      On a shell, source the &ostack; RC file. For details, refer to
      <xref linkend="sec.user.cli.rc"/>.
     </para>
   </step>
   <step>
    <para>Look up the available flavors:</para>
    <screen>nova flavor-list</screen>
    <para>Memorize the ID of the flavor that you want to use for your instance.</para>
   </step>
   <step>
    <para>Look up the available images:</para>
    <screen>nova image-list</screen>
    <para>Memorize the ID of the image that you want to boot your instance from.</para>
   </step>
   <step>
    <para>Look up the available security groups:</para>
    <screen>nova secgroup-list</screen>
    <para>If you did not create any specific security groups, you can only assign
    the instance to the default security group.</para>
   </step>
   <step>
    <para>Look up your keypair's name (for SSH access) and memorize it:</para>
    <screen>nova keypair-list</screen>
   </step>
   <step>
    <para>Now you have all parameters at hand for starting an instance. Do so
    with the following command:</para>
    <screen>nova boot --flavor&nbsp;<replaceable>FLAVOR_ID</replaceable>&nbsp;--image<replaceable>IMAGE_ID</replaceable> --key_name <replaceable>KEY_NAME</replaceable> \
--security_group <replaceable>NAME_OF_SEC_GROUP</replaceable> <replaceable>NAME_FOR_INSTANCE</replaceable>&nbsp;</screen>
    <para>The command returns a list of instance properties, including the 
    <literal>status</literal> of the instance. The status <literal>BUILD</literal>
    indicates that the instance has started, but is not yet online.</para>
   </step>
   <step>
    <para>Check if the instance is online:</para>
    <screen>nova list</screen>
    <para>This command lists all instances of the project you belong to, including
    their ID, their name, their status and their private (and if assigned) 
    their public IP addresses. If your instance's status is <literal>ACTIVE</literal>, 
    the instance is online.</para>
    <para>To refine the search, run <command>nove help list</command> to view
    the available options for the command.</para>
   </step>
  </procedure>

  <para>
   If you did not provide a keypair on start and did not touch security groups
   or rules so far, by default the instance can only be accessed from inside the 
   cloud via VNC at this point. Even pinging the instance is not possible. To 
   change this, proceed with <xref linkend="sec.user.cli.inst.access"/>.</para>
 </sect1>

 <sect1 id="sec.user.cli.inst.access">
  <title>Configuring Access to the Instances</title>
  &inst-access-params;
  
   <sect2 id="sec.user.cli.inst.access.keys">
   <title>Creating or Importing Keys</title>
   &keypairs;
   <para>In case an image uses a static &rootuser; password or a static key set 
   (neither is recommended), you do not need to provide a keypair on start of 
   the instance.</para>

   <procedure>
    <title>Creating or Importing Keys</title>
    <para>Use the <command>nova keypair-add</command> command to generate a new 
   keypair, or to upload an existing public key. </para>
    <step>
     <para>To generate a new keypair, execute the following commands: </para>
     <screen>nova keypair-add <replaceable>KEY_NAME</replaceable> &gt; <replaceable>MY_KEY</replaceable>.pem
chmod 600  <replaceable>MY_KEY</replaceable>.pem</screen>
     <para>The first command generates a new keypair named <replaceable>KEY_NAME</replaceable>,
    writing the private key to the file
    <filename><replaceable>MY_KEY</replaceable>.pem</filename> and registering the
    public key at the Nova database. The second command changes the permissions 
    of the file <filename><replaceable>MY_KEY</replaceable>.pem</filename> so 
    that only you can read and write to it.</para>
    </step>
    <step>
     <para>If you already have generated a keypair, with the public key located at 
    <filename>~/.ssh/id_rsa.pub</filename>, 
    you can upload the public key with the following command:</para>
     <screen>nova keypair-add --pub_key ~/.ssh/id_rsa.pub <replaceable>KEY_NAME</replaceable>
    </screen>
     <para>The command registers the public key at the Nova database and names 
    the keypair <replaceable>KEY_NAME</replaceable>.</para>
    </step>
    <step>
     <para>Check if the uploaded keypair appears in the list of availabe 
   keypairs:</para>
     <screen>nova keypair-list</screen>
    </step>
   </procedure>
   
   <!--Keypairs can also be generated using the following commands.

ssh-keygen
cd .ssh
nova keypair-add -/-pub_key id_rsa.pub mykey

This creates a new keypair called mykey. The private key id_rsa is saved locally in 
~/.ssh which can be used to connect to an instance launched using mykey as the keypair. 
You can see the available keypairs with nova keypair-list command.

nova keypair-list

|  Name |                   Fingerprint                   |
___________________________________________________________+
| mykey  | b0:18:32:fa:4e:d4:3c:1b:c4:6c:dd:cb:53:29:13:82 |
| mykey2 | b0:18:32:fa:4e:d4:3c:1b:c4:6c:dd:cb:53:29:13:82 |
+__________________________________________________________+

Also while executing 'ssh-keygen' you can specify a custom location and custom file names 
for the keypairs that you want to create-->

<!--Create a keypair so you can ssh to the instance:

$ nova keypair-add test > test.pem
$ chmod 600 test.pem-->
<!--Adding a keypair

The Compute service can inject an SSH public key into an account on the instance, 
assuming the virtual machine image being used supports this. 
To add a keypair to the Compute service, use the nova keypair-add command. 
This command can be used to either generate a new keypair, or to upload an 
existing public key. The following example uploads an existing public key, 
located at ~/.ssh/id_rsa.pub, and gives the keypair the name mykey.

$ nova keypair-add -/-pub_key ~/.ssh/id_rsa.pub mykey

List the keypairs by doing:

$ nova keypair-list
+_______+_________________________________________________+
|  Name |                   Fingerprint                   |
| mykey | c3:d2:b5:d3:ec:4a:29:b0:22:32:6e:34:dd:91:f9:cf |
+_______+_________________________________________________+

Confirm that the uploaded keypair matches your local key by checking your key's 
fingerprint with the ssh-keygen command:

$ ssh-keygen -l -f ~/.ssh/id_rsa.pub
2048 c3:d2:b5:d3:ec:4a:29:b0:22:32:6e:34:dd:91:f9:cf
/home/myaccount/.ssh/id_rsa.pub (RSA)-->

  </sect2>

  <sect2 id="sec.user.cli.inst.access.security">
   <title>Configuring Security Groups and Rules</title>
   &security;
   <sect3 id="sec.user.cli.inst.access.security.groups">
    <title>Security Groups</title>
    &sec-groups;
    
    <para>
   Security groups can be managed with the <command>nova secgroup-*</command>
   commands, provided by the <systemitem>python-novaclient</systemitem>
   package.
  </para>
    <variablelist>
     <varlistentry>
      <term>Listing Security Groups</term>
      <listitem>
       <screen>nova secgroup-list</screen>
       <para>
      Lists all security groups for the current project, including the groups' 
      descriptions.
     </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Creating a Security Group</term>
      <listitem>
       <screen>nova secgroup-create&nbsp;<replaceable>SEC_GROUP_NAME</replaceable>&nbsp;<replaceable>GROUP_DESCRIPTION</replaceable>&nbsp;</screen>
       <para>
      Creates a new security group with the specified name and description.
     </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Deleting a Security Group</term>
      <listitem>
       <screen>nova secgroup-delete&nbsp;<replaceable>SEC_GROUP_NAME</replaceable>&nbsp;</screen>
       <para>
      Deletes the specified group.
     </para>
      &note-groups-delete;
      </listitem>
     </varlistentry>
    </variablelist>

   </sect3>
   <sect3 id="sec.user.cli.inst.access.security.rules">
    <title>Security Group Rules</title>
    &sec-rules;
    
    &sec-rules-params;
    
    &security-default;
   
    <!--taroth 2012-08-27: IIRC, I read this in upstream docs, but according to
     bmwiedemann's tests, he could not access arbitrary ports of VMs in the same
     security group, therefore commenting--> 
     <!--Unless you change the rules for the default group, this means that
     those instances are only accessible from IP addresses belonging to other members 
     of this group.-->


    <procedure>
     <title>Configuring Security Group Rules</title>
     <para>Modify security group rules with the 
     <command>nova secgroup-*-rule</command> commands. Proceed as follows:</para>
     <step>
      <para>
      On a shell, source the &ostack; RC file. For details, refer to
      <xref linkend="sec.user.cli.rc"/>.
     </para>
     </step>
     <step>
      <para>Look up the existing rules for a security group:</para>
      <screen>nova secgroup-list-rules <replaceable>SEC_GROUP_NAME</replaceable>&nbsp;</screen>
     </step>
     <step>
      <para>To allow SSH access to the instances:</para>
      <substeps>
       <step id="step.sec.rule.add">
        <para>Either from <emphasis>all</emphasis> IP addresses 
         (specified as IP subnet in CIDR notation as <literal>0.0.0.0/0</literal>):</para>
        <screen>nova secgroup-add-rule <replaceable>SEC_GROUP_NAME</replaceable> tcp 22 22 0.0.0.0/0</screen>
       </step>
       <step id="step.sec.group.rule.add">
        <para>Alternatively, you can allow only IP addresses from other security
        groups (<literal>source groups</literal>) to access the specified port:</para>
        <screen>nova secgroup-add-group-rule --ip_proto tcp --from_port 22 \
--to_port 22 <replaceable>SEC_GROUP_NAME</replaceable>&nbsp;<replaceable>SOURCE_GROUP_NAME</replaceable>&nbsp;</screen>
       </step>
      </substeps>
     </step>
     <step>
      <para>
       To allow pinging the instances:</para>
      <substeps>
       <step>
        <para>Either from <emphasis>all</emphasis> IP addresses 
         (specified as IP subnet in CIDR notation as <literal>0.0.0.0/0</literal>):</para>
        <screen>nova secgroup-add-rule <replaceable>SEC_GROUP_NAME</replaceable> icmp -1 -1 0.0.0.0/0</screen>
        <para>This command allows access to all codes and all types of ICMP 
        traffic, respectively.</para>
       </step>
       <step>
        <para>Alternatively, you can allow only members of other security groups
        (<literal>source groups</literal>) to ping instances:</para>
        <screen>nova secgroup-add-group-rule --ip_proto icmp --from_port -1 \
--to_port -1 <replaceable>SEC_GROUP_NAME</replaceable>&nbsp;<replaceable>SOURCE_GROUP_NAME</replaceable>&nbsp;</screen>
       </step>
      </substeps>
     </step>
     <step>
      <para>To allow access via UDP port (for example, for an DNS server
      running on a VM):</para>
      <substeps>
       <step>
        <para>Either from <emphasis>all</emphasis> IP addresses 
        (specified as IP subnet in CIDR notation as <literal>0.0.0.0/0</literal>):</para>
        <screen>nova secgroup-add-rule <replaceable>SEC_GROUP_NAME</replaceable> udp 53 53 0.0.0.0/0</screen>
       </step>
       <step>
        <para>Alternatively, you can allow only IP addresses from other security
        groups (<literal>source groups</literal>) to access the specified port:</para>
        <screen>nova secgroup-add-group-rule --ip_proto udp --from_port 53 \
--to_port 53 <replaceable>SEC_GROUP_NAME</replaceable>&nbsp;<replaceable>SOURCE_GROUP_NAME</replaceable>&nbsp;</screen>
       </step>
      </substeps>
     </step>
     <step>
      <para>To delete security group rules, you need to specify the same
     arguments that you used to create the rule. For example:</para>
      <para>To delete the security rule that you created in <xref linkend="step.sec.rule.add"/>:</para>
      <screen>nova secgroup-delete-rule <replaceable>SEC_GROUP_NAME</replaceable>&nbsp;tcp 22 22 0.0.0.0/0</screen>
      <para>To delete the security rule that you created in <xref linkend="step.sec.group.rule.add"/>:</para>
      <screen>nova secgroup-delete-group-rule --ip_proto tcp --from_port 22 \
--to_port 22 <replaceable>SEC_GROUP_NAME</replaceable>&nbsp;<replaceable>SOURCE_GROUP_NAME</replaceable>&nbsp;</screen>
     </step>
    </procedure>
   
   <!--taroth 2012-10-12: edited until here -
    todo: the procedures/commands above need a sanity check
   (contentwise and length of screens in PDF/HTML)!!!-->
  
   <!--taroth 2012-10-12: maybe integrate some of the following information:
   
    Every security group rule is a policy which allows you to specify inbound 
    connections that are allowed to access the instance, by source address, 
    destination port and IP protocol,(TCP, UDP or ICMP). 
    
    Currently, ipv6 and other protocols cannot be managed with the security rules, 
    making them permitted by default. To manage such, you can deploy a firewall 
    in front of your OpenStack cloud to control other types of traffic. The command 
    requires the following arguments for both TCP and UDP rules :
    
              
   [Note]	The CIDR notation
             
   That notation allows you to specify a base IP address and a suffix that designates 
   the number of significant bits in the IP address used to identify the network. 
   For example, by specifying a 88.170.60.32/27, you specify 88.170.60.32 
   as the base IP and 27 as the suffix. Since you use an IPV4 format, there are only 
   5 bits available for the host part (32 minus 27). 
   The 0.0.0.0/0 notation means you allow the entire IPV4 range, meaning allowing all addresses.
              
   For example, in order to allow any IP address to access to a web server running on one of your 
   instance inside the default security group:
   $ nova secgroup-add-rule default tcp 80 80 0.0.0.0/0
              
   In order to allow any IP address to ping an instance inside the default security group 
   (Code 0, Type 8 for the ECHO request.):
    $ nova secgroup-add-rule default icmp 0 8 0.0.0.0/0-->
    
    
   <!--taroth 2012-08-15: mention when non-default sec groups are useful (info
    by mvidner, for CLI chapter mention how to change security groups for running 
    instances, info by cthiel: 
    sure, this is possible, but AFAIK only through the command-line:
    
    usage: nova add-secgroup <server> <secgroup>
    Add a Security Group to a server.
    
    Positional arguments:
    <server>    Name or ID of server.
    <secgroup>  Name of Security Group.
    
    
    usage: nova remove-secgroup <server> <secgroup>
    
    Remove a Security Group from a server.
    
    Positional arguments:
    <server>    Name or ID of server.
    <secgroup>  Name of Security Group.
   
    
    taroth 2012-10-12: todo - find out if these commands are also part of 
    python-novaclient-->

   </sect3>
  </sect2>

  <sect2 id="sec.user.cli.inst.access.ips">
   <title>Managing IP Addresses</title>
   &ip-addr;
   
   &ip-allocate;
  
    &ip-assign;
    
   <!-- Pools of floating IP addresses are created outside of python-novaclient with the 
    nova-manage floating * commands. Refer to "Configuring Public (Floating) IP Addresses" 
    in the Openstack Compute Administration Manual for more information.-->

   <para>Floating IP addresses can be managed with the <command>nova *floating-ip-*</command>
   commands, provided by the <systemitem>python-novaclient</systemitem>
   package.</para>

   <variablelist>
    <varlistentry>
     <term>Listing Pools with Floating IP Adresses </term>
     <listitem>
      <screen>nova floating-ip-pool-list</screen>
      <para>
      Lists the name of all pools that provide floating IP addresses.
     </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Allocating a Floating IP Address to the Current Project</term>
     <listitem>
      <screen>nova floating-ip-pool-list</screen>
      <para>The output of the command shows the freshly allocated IP address. 
     If there is more than one pool of IP addresses available, you can
     also specify the pool from which to allocate the IP address (optional):
     </para>
      <screen>floating-ip-create&nbsp;<replaceable>POOL_NAME</replaceable>&nbsp;</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Listing Floating IP Addresses Allocated to the Current
     Project</term>
     <listitem>
      <screen>nova floating-ip-list</screen>
      <para>Lists all floating IP addresses that have been allocated to the current
     project. If an IP is already associated to an instance, the output
     also shows the instance's IP, the instance's fixed IP adress and the name of
     the pool that provides the floating IP address.</para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Releasing a Floating IP Address from the Current Project</term>
     <listitem>
      <screen>nova floating-ip-delete&nbsp;<replaceable>FLOATING_IP</replaceable>&nbsp;</screen>
      <para>The IP address is put back into the pool of IP addresses that 
     are available for all projects. If an IP address is currently assigned 
     to a running instance, it will automatically be dissociated from the 
     instance.</para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Assigning a Floating IP Address to an Instance</term>
     <listitem>
      <screen>nova add-floating-ip&nbsp;<replaceable>INSTANCE_NAME_OR_ID</replaceable>&nbsp;<replaceable>FLOATING_IP</replaceable>&nbsp;</screen>
      <para>To associate an IP address to an instance, one or multiple floating IP
     addresses must have been allocated to the current project. Check this with 
     <command>nova floating-ip-list</command>. In addition, you need to know the
     instance's name (or ID). To look up the instances that belong to the current 
     project, use the <command>nova list</command> command. </para>
      <para>After assigning the IP with <command>nova add-floating-ip</command>, 
      the instance is now publicly available under the respective floating 
      IP address (provided you have also configured the security group rules 
      for the instance accordingly). For details, refer to 
      <xref linkend="sec.user.dash.inst.access.security"/>.
     </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Removing a Floating IP address from an Instance</term>
     <listitem>
      <screen>nova remove-floating-ip&nbsp;<replaceable>INSTANCE_NAME_OR_ID</replaceable>&nbsp;<replaceable>FLOATING_IP</replaceable>&nbsp;</screen>
      <para>To remove a floating IP address from an instance, you need to specify 
       the same arguments that you used to assign the IP.</para>
     </listitem>
    </varlistentry>
   </variablelist>

 <!--taroth 2012-10-15: not sure if it makes sense to describe this
 in procedures, therefore commenting-->
 <!--<procedure>
  <title>Allocating Floating (Public) IPs to a Project</title>
  <step>
   <para>
      On a shell, source the &ostack; RC file. For details, refer to
      <xref linkend="sec.user.cli.rc"/>.
     </para>
  </step>
  <step>
   <para>To check which pools of floating IP addresses are available:</para>
   <screen>nova floating-ip-pool-list</screen>
  </step>
  <step>
   <para>To allocate a floating IP address to the current project:</para>
   <screen>nova floating-ip-create</screen>
   <para>The output of the command shows the freshly allocated IP address. 
     If there is more than one pool of IP addresses available, you can
     also specify a pool from which to allocate the IP address (optional): </para>
   <screen>floating-ip-create&nbsp;<replaceable>POOL_NAME</replaceable>&nbsp;</screen>
  </step>
  <step>
   <para>To list all floating IP addresses which have been allocated
     to the current project:</para>
   <screen>nova floating-ip-list</screen>
  </step>
  <step>
   <para>
      To release a floating IP address from the current project:
     </para>
   <screen>nova floating-ip-delete&nbsp;<replaceable>FLOATING_IP</replaceable>&nbsp;</screen>
   <para>The IP address is put back into the pool of IP addresses that 
     are available for all projects. If an IP address is currently assigned 
     to a running instance, it will automatically be dissociated from the 
     instance.</para>
  </step>
 </procedure>
 <procedure>
  <title>Assigning Floating (Public) IP Addresses to Instances</title>
    &ip-assign;
    <step>
     <para>
      On a shell, source the &ostack; RC file. For details, refer to
      <xref linkend="sec.user.cli.rc"/>.
     </para>
  </step>
  <step>
   <para>List all floating IP addresses which have been allocated to the current
     project:</para>
   <screen>nova floating-ip-list</screen>
  </step>
  <step>
   <para>To associate an IP address to an instance, you need to know the
     instance's name (or ID) and the floating IP address.</para>
   <substeps>
    <step>
     <para>To look up the instances that belong to the current project
       with:</para>
     <screen>nova list</screen>
    </step>
    <step>
     <para>To assign the IP address to an instance:</para>
     <screen>nova add-floating-ip&nbsp;<replaceable>INSTANCE_NAME_OR_ID</replaceable>&nbsp;<replaceable>FLOATING_IP</replaceable>&nbsp;</screen>
     <para>The instance is now publicly available under the respective floating 
      IPs address (provided you have also configured the security group rules 
      for the instance accordingly). For details, refer to 
      <xref linkend="sec.user.dash.inst.access.security"/>.
     </para>
    </step>
   </substeps>
  </step>
  <step>
   <para>To remove the floating IP address from an instance:</para>
   <screen>nova remove-floating-ip&nbsp;<replaceable>INSTANCE_NAME_OR_ID</replaceable>&nbsp;<replaceable>FLOATING_IP</replaceable>&nbsp;</screen>
  </step>
 </procedure>-->
  </sect2>
  </sect1>

 <!--<sect1 id="sec.user.cli.inst.manage">
  <title>Managing Instances</title>
  <para>The following are typical tasks for managing instances:</para>
  <itemizedlist>
   <listitem>
    <para>Accessing instances from remote</para>
   </listitem>
   <listitem>
    <para>Viewing logs</para>
   </listitem>
   <listitem>
    <para>Creating instance snapshots to preserve a certain disk state of an
     instance</para>
   </listitem>
   <listitem>
    <para>Using instance snapshots as base for new images</para>
   </listitem>
   <listitem>
    <para>Rebooting or terminating instances</para>
   </listitem>
   <listitem>
    <para>Pausing or suspending instances</para>
   </listitem>
   <listitem>
    <para>Tracking instance usage</para>
   </listitem>
  </itemizedlist>
-->

  <!--taroth 2012-10-05: nova console-log myinstance-->
<!--  <sect2 id="sec.user.cli.inst.manage.logs">
   <title>Viewing Instance Logs</title>
   <procedure>
    <step>
     <para>Log in to &cloud; &dash;.</para>
    </step>
    <step>
     <para>If you are a member of multiple projects, select a <guimenu>Project</guimenu>
    from the drop-down list at the top of the tab.</para>
    </step>
    <step>
     <para>Click the <guimenu>Instances &amp; Volumes</guimenu> category.</para>
    </step>
    <step>
     <para>Select the instance and from the <guimenu>Actions</guimenu>
      drop-down list, select <guimenu>View Log</guimenu>.</para>
     <para>Alternatively, click the instance's name and switch to the
      <guimenu>Log</guimenu> tab that opens.</para>
     <para>The &dash; shows the output of the instance's serial console. To make
      use of this feature, the respective image must have set the serial console
      correctly in &grub;.</para>
    </step>
   </procedure>
  </sect2>
-->
  <!--<sect2 id="sec.user.cli.inst.manage.remote">
   <title>Accessing Instances from Remote</title>
   <para>The &dash;'s built-in VNC client allows you to access instances
   at any time.</para>
  --> 
   <!--When you need to get a VNC console directly to a server, you can use the 
   nova get-vnc-console command to connect.-->

   <!--<procedure>
    <title>Accessing an Instance via VNC</title>
    <step>
     <para>Log in to &cloud; &dash;.</para>
    </step>
    <step>
     <para>If you are a member of multiple projects, select a <guimenu>Project</guimenu>
    from the drop-down list at the top of the tab.</para>
    </step>
    <step>
     <para>Click the <guimenu>Instances &amp; Volumes</guimenu> category.</para>
    </step>
    <step>
     <para>Select the instance to access and from the <guimenu>Actions</guimenu>
      drop-down list, select <guimenu>VNC Console</guimenu>.</para>
     <para>Alternatively, click the instance's name and switch to the
      <guimenu>VNC</guimenu> tab that opens.</para>
    </step>
    <step>
     <para>To display a larger VNC screen, use the link <guimenu>Click here
    to show only VNC</guimenu>.</para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="cloud_dash_vnc.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="cloud_dash_vnc.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>To leave the large VNC screen, use the back button of the browser.</para>
    </step>
   </procedure>

    &ssh-req;-->
   <!--For details, refer to <xref linkend="sec.user.dash.inst.access.security"/>.-->
   <!--For details, refer to <xref linkend="sec.user.dash.inst.access.ips"/>.-->

 <!-- </sect2>-->

  <!--<sect2 id="sec.user.cli.inst.manage.snapshots">
   <title>Using Instance Snapshots</title>
   &inst-snap;
     <remark>taroth 2012-08-21: DEVs, if I got it right, ephemeral disks are 
      generally excluded from the snapshots?</remark>
   <procedure>
    <title>Creating Instance Snapshots</title>
    <step>
     <para>Log in to &cloud; &dash;.</para>
    </step>
    <step>
     <para>If you are a member of multiple projects, select a <guimenu>Project</guimenu>
    from the drop-down list at the top of the tab.</para>
    </step>
    <step>
     <para>Click the <guimenu>Instances &amp; Volumes</guimenu> category.</para>
    </step>
    <step>
     <para>Select the instance of which to create a snapshot. From the
        <guimenu>Actions</guimenu> drop-down list, select
        <guimenu>Snapshot</guimenu>.</para>
    </step>
    <step>
     <para>In the window that opens, enter a name for the snapshot and confirm your 
        changes. The &dash; shows the new <guimenu>Instance Snapshot</guimenu> 
        in the <guimenu>Images &amp; Snapshot</guimenu> category.</para>
    </step>
    <step>
     <para>To launch a new instance from the snapshot, select the snapshot and
      click <guimenu>Launch</guimenu>. Proceed with launching an instance as 
      described in <xref linkend="pro.user.instances.launch"/>.</para>
    </step>
   </procedure>
   <procedure>
    <title>Basing an Image on a Snapshot</title>
    <step>
     <para>Log in to &cloud; &dash;.</para>
    </step>
    <step>
     <para>If you are a member of multiple projects, select a <guimenu>Project</guimenu>
    from the drop-down list at the top of the tab.</para>
    </step>
    <step>
     <para>Click the <guimenu>Images &amp; Snapshots</guimenu> category.</para>
    </step>
    <step>
     <para>Select the snapshot and from the <guimenu>Actions</guimenu> drop-down 
        list, select <guimenu>Edit</guimenu>.
       </para>
    </step>
    <step>
     <para>In the window that opens, enter the image properties.
        For more information, refer to <xref linkend="sec.adm.dash.img"/>.</para>
    </step>
    <step>
     <para>Click <guimenu>Update Image</guimenu>. </para>
     <para>&dash; shows the newly created image in the list of images in the 
        <guimenu>Images &amp; Snapshot</guimenu> category. If you delete the 
        snapshot, upon which the image is based, the image will be deleted as 
        well. If you delete the image, the snapshot upon which it is based, will 
        be deleted as well.</para>
    </step>
   </procedure>
  </sect2>
-->
  <!--<sect2 id="sec.user.cli.inst.manage.terminate">
   <title>Pausing, Suspending, Rebooting or Terminating Instances</title>
   &inst-terminate;
   
   &inst-terminate-warn;
  
   <procedure>
    <step>
     <para>Log in to &cloud; &dash;.</para>
    </step>
    <step>
     <para>If you are a member of multiple projects, select a <guimenu>Project</guimenu>
    from the drop-down list at the top of the tab.</para>
    </step>
    <step>
     <para>Click the <guimenu>Instances &amp; Volumes</guimenu> category.</para>
    </step>
    <step>
     <para>Select the instance which to put out of the running state. From the
      <guimenu>Actions</guimenu> drop-down list, select the respective action.</para>
    </step>
   </procedure>-->
   
 <!--  Stop and Start an Instance

There are two methods for stopping and starting an instance:

    nova pause / nova unpause

    nova suspend / nova resume

 Pause and Unpause

nova pause stores the state of the VM in RAM. A paused instance continues to run, 
albeit in a "frozen" state.
 Suspend and Resume

nova suspend initiates a hypervisor-level suspend operation. Suspending an instance 
stores the state of the VM on disk; all memory is written to disk and the virtual machine 
is stopped. Suspending an instance is thus similar to placing a device in hibernation, 
and makes memory and vCPUs available. Administrators may want to suspend an 
instance for system maintenance, or if the instance is not frequently used.-->
   
 <!--When you no longer need an instance, use the nova delete command to terminate it. 
  You can use the instance name or the ID string. You will not receive a notification 
  indicating that the instance has been deleted, but if you run the nova list command, 
  the instance will no longer appear in the list.-->

  <!--</sect2>
  <sect2 id="sec.user.cli.inst.manage.usage">
   <title>Tracking Usage</title>
   <para>Use the &dash;'s <guimenu>Overview</guimenu> category to track usage of
   instances per project. It allows you to track costs per month by showing metrics like
   number of VCPUs, disks, RAM and uptime of all your instances.</para>
   <para>If you are a member of multiple projects, select a <guimenu>Project</guimenu>
    from the drop-down list at the top of the tab. Select a month and click 
    <guimenu>Submit</guimenu> to query the instance usage for that month. The
    &dash; also allows to download a CVS summary.</para>
   <figure>
    <title>&cloud; &dash;&mdash;Usage Overview</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="cloud_dash_overview_usage.png" width="100%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="cloud_dash_overview_usage.png" width="80%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>
 </sect1>-->
 <!--<sect1 id="sec.user.cli.volumes">
  <title>Managing Volumes</title>
  &volumes;-->

<!--you can use the nova CLI to manage volumes.

volume-attach       Attach a volume to a server.
volume-create       Add a new volume.
volume-delete       Remove a volume.
volume-detach       Detach a volume from a server.
volume-list         List all the volumes.
volume-show         Show details about a volume.
volume-snapshot-create
Add a new snapshot.
volume-snapshot-delete
Remove a snapshot.
volume-snapshot-list
List all the snapshots.
volume-snapshot-show
Show details about a snapshot.
volume-type-create  Create a new volume type.
volume-type-delete  Delete a specific flavor
volume-type-list    Print a list of available 'volume types'.-->

  <!--<procedure>
   <title>Creating or Deleting Volumes</title>
   <step>
    <para>Log in to &cloud; &dash;.</para>
   </step>
   <step>
    <para>If you are a member of multiple projects, select a <guimenu>Project</guimenu>
    from the drop-down list at the top of the tab.</para>
   </step>
   <step>
    <para>Click the <guimenu>Instances &amp; Volumes</guimenu> category.</para>
   </step>
   <step>
    <para>To create a volume:</para>
    <substeps>
     <step>
      <para>Click <guimenu>Create Volume</guimenu>.</para>
     </step>
     <step>
      <para>In the window that opens, enter a name to assign to a volume,
      a description (optional), and define the size in GB.</para>
     </step>
     <step>
      <para>Confirm your changes.</para>
      <para>The &dash; shows the volume in the <guimenu>Instances &amp;
       Volumes</guimenu> category.</para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     To delete one or multiple volumes:
    </para>
    <substeps>
     <step>
      <para>
       Activate the check boxes in front of the volumes that you want to
       delete.
      </para>
     </step>
     <step>
      <para>
       Click <guimenu>Delete Volumes</guimenu> and confirm your choice in
       the pop-up that appears.<remark>taroth 2012-08-23: check what happens if
       the volume is attached to an instance! taroth 2012-08-27: according to
       bmwiedemann, deleting attached volumes should be refused</remark>
      </para>
      <para>
       A message on the Web page shows if the action has been successful.
      </para>
     </step>
    </substeps>
   </step>
  </procedure>

  <para>&vol-attach;
  View the <guimenu>Status</guimenu> of a volume in the <guimenu>Instances &amp;
  Volumes</guimenu> category of the &dash;: The volume is either  
  <literal>available</literal> or already <literal>In-Use</literal>.</para>
  <procedure>
   <title>Attaching Volumes to Instances</title>
   <step>
    <para>Log in to &cloud; &dash;.</para>
   </step>
   <step>
    <para>If you are a member of multiple projects, select a <guimenu>Project</guimenu>
    from the drop-down list at the top of the tab.</para>
   </step>
   <step>
    <para>Click the <guimenu>Instances &amp; Volumes</guimenu> category.</para>
   </step>
   <step>
    <para>Select the volume to add to an instance and click 
   <guimenu>Edit Attachments</guimenu>.</para>
   </step>
   <step>
    <para>In the window that opens, select an instance to attach the volume
   to.</para>
   </step>
   <step>
    <para>Enter a <guimenu>Device Name</guimenu> under which the volume should
    be accessible on the virtual machine.</para>
   </step>
   <step>
    <para>Confirm your changes. The &dash; shows the instance to which the volume 
    has been attached and the volume's device name.</para>
   </step>
   <step>
    <para>Now you can log in to the instance, mount the disk, format it and use 
   it.</para>
    <para>If the instance is running the latest &sls; SP2 Kernel, it is not necessary
    to reboot the virtual machine to make the device appear. 
    Otherwise load the <literal>acpiphp</literal> module manually:</para>
    <screen>modprobe acpiphp</screen>
   </step>
   <step>
    <para>To detach a volume from an instance:</para>
    <substeps>
     <step>
      <para>Select the volume and click <guimenu>Edit Attachments</guimenu>.</para>
     </step>
     <step>
      <para>In the window that opens, click <guimenu>Detach Volume</guimenu> and
   confirm your changes.</para>
      <para>
          A message on the Web page shows if the action has been successful.
      </para>
     </step>
    </substeps>
   </step>
  </procedure>

  <procedure>
   <title>Creating Volume Snapshots</title>
   --><!--taroth 2012-08-24: todo - add what volume snapshots are good for
    and how to delete them--><!--
   <step>
    <para>Log in to &cloud; &dash;.</para>
   </step>
   <step>
    <para>If you are a member of multiple projects, select a <guimenu>Project</guimenu>
    from the drop-down list at the top of the tab.</para>
   </step>
   <step>
    <para>Click the <guimenu>Instances &amp; Volumes</guimenu> category.</para>
   </step>
   <step>
    <para>Select the volume of which to create a snapshot.</para>
   </step>
   <step>
    <para>From the <guimenu>Actions</guimenu> drop-down list, select 
    <guimenu>Create Snapshot</guimenu>.</para>
   </step>
   <step>
    <para>In the window that opens, enter a <guimenu>Snapshot Name</guimenu>
   and a <guimenu>Description</guimenu>.
    </para>
   </step>
   <step>
    <para>Confirm your changes. The &dash; shows the new <guimenu>Volume Snapshot</guimenu> 
        in the <guimenu>Images &amp; Snapshot</guimenu> category.</para>
   </step>
  </procedure>

 </sect1>-->
</chapter>
